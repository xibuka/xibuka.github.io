[{"content":"Background When deploying Kong Gateway, there is some data that we do not want to store in plain text, such as connection information to the database. The Kong Secret Manager was developed to solve this problem, which can be solved using a 3rd party service such as AWS Secrets Manager. However, this functionality is unavailable when the environment does not allow connection to external security services. In this case, we can use a encryption tool SOPS, developed by Mozilla to do the encryption and decryption. And also we can make the deploy process into the CI/CD workflow (e.g. Github Action) to make the decrypted configuration file only existing in a temproray workflow.\nPreparation First, let\u0026rsquo;s try to do the encryption and decryption in a local environment. Install the following necessary tools in your local environment.\n age sops  SOPS is a handy and popular encryption/decryption tool, supporting PGP, age, Google cloud\u0026rsquo;s KMS, Azure\u0026rsquo;s key vault, Hashicorp Vault, etc. We chose age because we intend to use other cloud services as little as possible. Once you have installed the tools, try age-kengen --help.\n1 2 3 4 5 6 7 8  $ age-keygen --help Usage: age-keygen [-o OUTPUT] age-keygen -y [-o OUTPUT] [INPUT] Options: -o, --output OUTPUT Write the result to the file at path OUTPUT. -y Convert an identity file to a recipients file.   Since it looks OK, let\u0026rsquo;s generate the key.\ngenerate key 1 2 3 4 5 6  $ age-keygen -o sops-key.txt Public key: age1hhpnj4ylj5z7qwaek0ncrq882ygmvnv4unup78u3djgmlezrmgkqshyatc $ cat sops-key.txt # created: 2023-03-08T06:07:28Z # public key: age1hhpnj4ylj5z7qwaek0ncrq882ygmvnv4unup78u3djgmlezrmgkqshyatc AGE-SECRET-KEY-1UHJNXS6RKYWA72RVED0ERGZVQ98N6MDFV6Y3CSPPN9JKLKSRH9GSQRLFAE`   Two keys have been generated in the sops-key.txt file: The public key for encryption and the Private key for decryption.\nEncryption of configuration file To use SOPS more conveniently, let\u0026rsquo;s create a single configuration file, .sops.yaml. You no longer need to specify the key from the command line; enter the Public key you just generated after age, and the encrypted_regex part is where you set which sections of the contents to encrypt.\n1 2 3  creation_rules:- encrypted_regex:\u0026#39;^(env|admin|proxy|enterprise|manager|portal|portalapi|postgresql)$\u0026#39;age:age1hhpnj4ylj5z7qwaek0ncrq882ygmvnv4unup78u3djgmlezrmgkqshyatc  The following command will encrypt the files in the Kong GW deployment with the above configuration. Parameter -e means encrypt, and -i means save the changes to the orignal file.\n1 2 3 4 5 6 7 8 9 10 11 12  $ sops -e -i values.yaml $ head values.yaml image: repository: kong/kong-gateway tag: \u0026#34;3.1\u0026#34; env: prefix: ENC[AES256_GCM,data:2AFF90x0Q3Ej9cMDiw==,iv:o/jBUcZIypUEioKk0Fd4uheBrCOlUOL4RQYExOW696E=,tag:LoIjejClr7lh37Rq9YeKDw==,type:str] log_level: ENC[AES256_GCM,data:ZhYxI6A=,iv:oZJ8E/MocmOonUPD2FY6BLaXPuj4TBl//0fqTmOY0Xg=,tag:46p/kxlNSctmOGFupQSnOQ==,type:str] database: ENC[AES256_GCM,data:3L8H1aqImEc=,iv:9iF+73VeFWbsmHqW1yKBCgwMpO3us8pTWwSWmNaCl80=,tag:icNvo+EvoYnxiAGdjxzPqw==,type:str] proxy_url: ENC[AES256_GCM,data:89WLCtCUglsyZQdsns1Lj6O/CI6YtCc8wXdX9EIJCGNwzmjB8v8=,iv:HUFPH8bgG62UvAeQATh/0GprR8zOgLBGmvcYbON4B00=,tag:Q1l7E8l5xClp52KSEz+MNQ==,type:str] admin_gui_url: ENC[AES256_GCM,data:Xun4V7eliBRlmfn8v3CVFwxMjRumh+REwmPgCDbWwrPhjSQMXkn6qQ==,iv:8zH3GjO35ycpAsuOgDB+UKNAc19zSee72z2UlrdZ+Js=,tag:U4IEK5Nef6h59vbHyM0aSA==,type:str] admin_api_uri: ENC[AES256_GCM,data:52j0JgFNldx5Qytsqav9nSLLLEUzR7+KsNo8aTsjbS2IyTM1R00=,iv:0RkBMi8k/XhuEzGSRpIQ9VQGbcUOTcb+o/KUVJ5LSYk=,tag:m5o89fI+GCKxD7TcLf5Nqg==,type:str]   The image section is not in the encrypted_regex filed, so it is left in plain text.\nDecrypt the configuration file First, we need to copy the Private Key to .config/sops/age/keys.txt. This is the default path of the private key, so you will not need to specify the Private Key when executing the sops command. Once the Private Key is set, you can compound the file encrypted above with the following command. Similar to the above, parameter -d means dencryption.\n1 2 3 4 5 6 7 8 9 10 11 12  $ sops -d -i values.yaml $ head values.yaml image: repository: kong/kong-gateway tag: \u0026#34;3.1\u0026#34; env: prefix: /kong_prefix/ log_level: debug database: postgres proxy_url: http://www.kongtest.net:8000 admin_gui_url: http://www.kongtest.net:8002 admin_api_uri: http://www.kongtest.net:8001   After decrypt the config file, we can use is via helm to install to a Kubernetes Cluster.\nCI/CD workflow Next, let\u0026rsquo;s combian above steps and setup it up into the CI/CD workflow. The configuration file can be deployed without disclosing the plain text configuration file to the user. The decrypted configuration file also exists only in the CI/CD workflow and will be deleted when the CI/CD is finished.\nThe workflow is as follows.  creation of Public/Private encrypt the configuration file Commit configuration file GitHub Action  install tools Getting Private key Decrypt the Configuration File Deploy Kong Gateway with helm using the configuration file. (Optional) Deploy the license key via Kong Admin API    All steps 1-3 can be done in the local environment. The point is 4-2, Private Key acquisition. If you register the Private Key in advance using the Secrets function of Github Action, you can refer to it directly in the CI/CD workflow. That means you do not have to worry about it being displayed in plain text. Also, please remember that if the Public/Private you use is not a pair, you will get an error when decrypting.\nThe CI/CD workflow you were writing can be found in [main.yml](https://raw.githubusercontent.com/robincher/kong-mozilla-sops-demo/master/.github/workflows/main. yaml). After committing the encrypted configuration file, you can successfully install Kong Gateway, so please give it a try.\nSummary With CI/CD, after deploying Kong Gateway, there are still many possibilities you can do. For example, create services and routes, or restore the configuration from a backup. Likewise, if there is sensitive information in the configuration that you want to keep encrpied, you can use the same method described above to encrypt it. It\u0026rsquo;s very simple so please have a try.\n","permalink":"https://wenhan.blog/post/20230308_using-sops-and-age-deploy-konggw_en/","summary":"Background When deploying Kong Gateway, there is some data that we do not want to store in plain text, such as connection information to the database. The Kong Secret Manager was developed to solve this problem, which can be solved using a 3rd party service such as AWS Secrets Manager. However, this functionality is unavailable when the environment does not allow connection to external security services. In this case, we can use a encryption tool SOPS, developed by Mozilla to do the encryption and decryption.","title":"Secure deployment Kong Gateway using Mozila SOPS, age and Github Action"},{"content":"背景 Kong Gatewayをデプロイする時に、データベースへの接続情報など平文で保存したくないデータが存在しています。これを解決するためにKong Secret Managerが開発され、AWS Secrets Managerなどの3rdパーティのサービスを利用すれば解決できます。しかし、環境によって外部のセキュリティサービスに接続できない時にこの機能は利用できません。ここで、Mozilaが開発したSOPSという暗号化ツールとGithub ActionのCI/CD workflowを利用し、普段暗号化されている設定ファイルをデプロイ時だけ復号し、Kong GWをインストールする実現することができました。\n事前準備 CI/CD workflowを構築する前に、まずはローカル環境で暗号と復号を試してみましょう。以下の必要なツールをローカル環境にインストールします。\n age sops  SOPSはとても便利な暗号化・復号化のツールで人気があります。PGP, age, Google cloud\u0026rsquo;s KMS, Azure\u0026rsquo;s key valut, Hashicorp Vaultなどをサポートしています。今回は他のクラウドサービスを極力利用しない方針のため、ageを選択しました。\nツールのインストールができましたら、age-kengen --helpを試してみましょう。\n1 2 3 4 5 6 7 8  $ age-keygen --help Usage: age-keygen [-o OUTPUT] age-keygen -y [-o OUTPUT] [INPUT] Options: -o, --output OUTPUT Write the result to the file at path OUTPUT. -y Convert an identity file to a recipients file.   見た感じ大丈夫そうなので、早速Keyを生成しましょう。\nkeyの生成 1 2 3 4 5 6 7  $ age-keygen -o sops-key.txt Public key: age1hhpnj4ylj5z7qwaek0ncrq882ygmvnv4unup78u3djgmlezrmgkqshyatc $ cat sops-key.txt # created: 2023-03-08T06:07:28Z # public key: age1hhpnj4ylj5z7qwaek0ncrq882ygmvnv4unup78u3djgmlezrmgkqshyatc AGE-SECRET-KEY-1UHJNXS6RKYWA72RVED0ERGZVQ98N6MDFV6Y3CSPPN9JKLKSRH9GSQRLFAE   sops-key.txtファイルに２種類のkeyが生成されました。Public keyは暗号化用、Private Keyは復号化用です。\n設定ファイルの暗号化 SOPSをもっと便利に使うために、一つの設定ファイル .sops.yamlを作りましょう。これでコマンドラインからkeyを指定する必要がなくなります。ageの後ろに先ほど生成したPublic keyを記入しまして、encrypted_regexの部分は、どのセクションの内容を暗号化するかを設定するところです。\n1 2 3  creation_rules:- encrypted_regex:\u0026#39;^(env|admin|proxy|enterprise|manager|portal|portalapi|postgresql)$\u0026#39;age:age1hhpnj4ylj5z7qwaek0ncrq882ygmvnv4unup78u3djgmlezrmgkqshyatc  上記の設定で、以下のコマンドで、Kong GWデプロイのファイルを暗号化することができます。\n1 2 3 4 5 6 7 8 9 10 11 12  $ sops -e -i values.yaml $ head values.yaml image: repository: kong/kong-gateway tag: \u0026#34;3.1\u0026#34; env: prefix: ENC[AES256_GCM,data:2AFF90x0Q3Ej9cMDiw==,iv:o/jBUcZIypUEioKk0Fd4uheBrCOlUOL4RQYExOW696E=,tag:LoIjejClr7lh37Rq9YeKDw==,type:str] log_level: ENC[AES256_GCM,data:ZhYxI6A=,iv:oZJ8E/MocmOonUPD2FY6BLaXPuj4TBl//0fqTmOY0Xg=,tag:46p/kxlNSctmOGFupQSnOQ==,type:str] database: ENC[AES256_GCM,data:3L8H1aqImEc=,iv:9iF+73VeFWbsmHqW1yKBCgwMpO3us8pTWwSWmNaCl80=,tag:icNvo+EvoYnxiAGdjxzPqw==,type:str] proxy_url: ENC[AES256_GCM,data:89WLCtCUglsyZQdsns1Lj6O/CI6YtCc8wXdX9EIJCGNwzmjB8v8=,iv:HUFPH8bgG62UvAeQATh/0GprR8zOgLBGmvcYbON4B00=,tag:Q1l7E8l5xClp52KSEz+MNQ==,type:str] admin_gui_url: ENC[AES256_GCM,data:Xun4V7eliBRlmfn8v3CVFwxMjRumh+REwmPgCDbWwrPhjSQMXkn6qQ==,iv:8zH3GjO35ycpAsuOgDB+UKNAc19zSee72z2UlrdZ+Js=,tag:U4IEK5Nef6h59vbHyM0aSA==,type:str] admin_api_uri: ENC[AES256_GCM,data:52j0JgFNldx5Qytsqav9nSLLLEUzR7+KsNo8aTsjbS2IyTM1R00=,iv:0RkBMi8k/XhuEzGSRpIQ9VQGbcUOTcb+o/KUVJ5LSYk=,tag:m5o89fI+GCKxD7TcLf5Nqg==,type:str]   imageのセクションは暗号化対象encrypted_regexにないので、平文のままになっております。\n設定ファイルの復号化 複合するために、まずはPrivate Keyを.config/sops/age/keys.txtにコピーします。このパスはデフォルトのパスなので、 sopsコマンドを実行する時にはPrivate Keyを指定する必要がなくなります。 Private Keyの設定したら、以下のコマンドで上で暗号化したファイルを複合化することができます。\n1 2 3 4 5 6 7 8 9 10 11 12  $ sops -d -i values.yaml $ head values.yaml image: repository: kong/kong-gateway tag: \u0026#34;3.1\u0026#34; env: prefix: /kong_prefix/ log_level: debug database: postgres proxy_url: http://www.kongtest.net:8000 admin_gui_url: http://www.kongtest.net:8002 admin_api_uri: http://www.kongtest.net:8001   CI/CD workflow 次に、実際にKong GatewayをデプロイCI/CDの中に上記の内容を追加すると、ユーザからは平文の設定ファイルを開示せずにデプロイできます。復号した設定ファイルもCI/CD workflow内にしか存在しないため、CI/CDが終了したら削除されます。\n作業の流れは以下の感じです。\n Public/Privateの生成 設定ファイルの暗号化 設定ファイルをコミット GitHub Action 4-1. ツールのインストール 4-2. Private keyの取得 4-3. 設定ファイルの復号化 4-4. helmで設定ファイルを使ってKong Gatewayをデプロイ  1から3は全てローカル環境で実施できます。気をつけたいのは、4-2 Private Key取得のところです。Github ActionのSecrets機能でPrivate Keyを事前に登録したら、CI/CD workflowで直接参照できますので、平文で表示される心配もなくなります。また、利用するPublic/Privateはペアじゃなかったら復号する時にエラーになりますのでご注意ください。\n書いていたCI/CD workflowは、main.yml から参照できます。暗号化した設定ファイルをコミットしたら、無事にKong Gatewayのインストールができました。\nまとめ CI/CDを使えば、Kong Gatewayをデプロイした後、serviceやRouteを作成したり、バックアップしたに設定をリストアしたり、可能性はまだたくさんあります。同じくもし設定の中に伏せたい機密情報があったら、上記と同じ手法で暗号化にすることができます。ぜひみなさん試してみてください。\n","permalink":"https://wenhan.blog/post/20230308_using-sops-and-age-deploy-konggw/","summary":"背景 Kong Gatewayをデプロイする時に、データベースへの接続情報など平文で保存したくないデータが存在しています。これを解決するためにKong Secret Managerが開発され、AWS Secrets Managerなどの3rdパーティのサービスを利用すれば解決できます。しかし、環境によって外部のセキュリティサービスに接続できない時にこの機能は利用できません。ここで、Mozilaが開発したSOPSという暗号化ツールとGithub ActionのCI/CD workflowを利用し、普段暗号化されている設定ファイルをデプロイ時だけ復号し、Kong GWをインストールする実現することができました。\n事前準備 CI/CD workflowを構築する前に、まずはローカル環境で暗号と復号を試してみましょう。以下の必要なツールをローカル環境にインストールします。\n age sops  SOPSはとても便利な暗号化・復号化のツールで人気があります。PGP, age, Google cloud\u0026rsquo;s KMS, Azure\u0026rsquo;s key valut, Hashicorp Vaultなどをサポートしています。今回は他のクラウドサービスを極力利用しない方針のため、ageを選択しました。\nツールのインストールができましたら、age-kengen --helpを試してみましょう。\n1 2 3 4 5 6 7 8  $ age-keygen --help Usage: age-keygen [-o OUTPUT] age-keygen -y [-o OUTPUT] [INPUT] Options: -o, --output OUTPUT Write the result to the file at path OUTPUT. -y Convert an identity file to a recipients file.","title":"Mozila SOPS+ageで実現するKong Gatewayセキュアデプロイメント"},{"content":"背景 Kong Gateway 3.0 から、Secrets ManagementがGAとなりました。Kong Gateway は、データベースのパスワードからプラグインで使用される API キーまで、多くのSecretに依存して動作します。以前ではRBACを使って、Admin APIとKong Managerから機密情報へのアクセスを制限できましたが、Secretを平文で表示されずに管理できたら嬉しいですよね。これを実現できるのはSecrets Managementです。\nサポートするVault 現時点で、サポートしているVaultは以下の４種類です。\n AWS Secrets Manager GCP Secrets Manager HashiCorp Vault Environment Variable  Kong は、上記の各システムを抽象化して、利用するときにVaultのキーワード(hcv、aws、gcpまたは env)を変更するだけで利用できます。たとえば、HashiCorp Vaultの Postgres Secretのpassword フィールドにアクセスするには、次のフォーマットで参照できます。\n{vault://hcv/postgres/password}  AWS Secrets Managerの場合\n{vault://aws/postgres/password}  環境変数の場合\nexport POSTGRES='{\u0026quot;username\u0026quot;:\u0026quot;user\u0026quot;, \u0026quot;password\u0026quot;:\u0026quot;pass\u0026quot;}' {vault://env/postgres/password}  デモ では実際にSecrets Managementを使ってVaultのSecretsを参照し、Kongのデプロイを試してみよう\nVault環境を用意 ここで、TOKEN_IDをkongにします。この値は後の認証するときに使用されます。\n1 2 3 4 5 6  docker run -d --name vault.idp \\ --network=kong-net \\ -e \u0026#34;VAULT_DEV_ROOT_TOKEN_ID=kong\u0026#34; \\ -p 8200:8200 \\ --cap-add=IPC_LOCK \\ vault:latest   Secretを作成 コンテナーに入って、Secretを作成しましょう\ndocker exec -it vault.idp sh / # export VAULT_ADDR='http://127.0.0.1:8200' / # export VAULT_TOKEN=\u0026quot;kong\u0026quot; / # vault kv put -mount secret kong pg_password=kongpass  VAULT_ADDRとVAULT_TOKENを設定しないと、Get \u0026quot;https://127.0.0.1:8200/v1/sys/internal/ui/mounts/secret\u0026quot;: http: server gave HTTP response to HTTPS clientのエラーが出ます。\nSecretを利用しKong gatewayをデプロイ Vaultが無事起動している状態でしたら、Kong GatewayのDocker コンテナーも起動しに行きます。DBへの接続用のパラメータKONG_PG_PASSWORDのところに、HashiCorp VaultからSecretを参照するように{vault://hcv/kong/pg_password}と書き換えています。Vaultへの接続情報はKONG_VAULT_HCV_*のパラメータで設定しています。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29  docker run -d --name kong-gateway-sm \\ --network=kong-net \\ -e \u0026#34;KONG_DATABASE=postgres\u0026#34; \\ -e \u0026#34;KONG_PG_HOST=kong-database\u0026#34; \\ -e \u0026#34;KONG_PG_USER=kong\u0026#34; \\ -e \u0026#34;KONG_PG_PASSWORD={vault://hcv/kong/pg_password}\u0026#34; \\ -e \u0026#34;KONG_PROXY_ACCESS_LOG=/dev/stdout\u0026#34; \\ -e \u0026#34;KONG_ADMIN_ACCESS_LOG=/dev/stdout\u0026#34; \\ -e \u0026#34;KONG_PROXY_ERROR_LOG=/dev/stderr\u0026#34; \\ -e \u0026#34;KONG_ADMIN_ERROR_LOG=/dev/stderr\u0026#34; \\ -e \u0026#34;KONG_ADMIN_LISTEN=0.0.0.0:8001\u0026#34; \\ -e \u0026#34;KONG_ADMIN_GUI_URL=http://localhost:8002\u0026#34; \\ -e \u0026#34;KONG_VAULT_HCV_PROTOCOL=http\u0026#34; \\ -e \u0026#34;KONG_VAULT_HCV_HOST=vault.idp\u0026#34; \\ -e \u0026#34;KONG_VAULT_HCV_PORT=8200\u0026#34; \\ -e \u0026#34;KONG_VAULT_HCV_MOUNT=secret\u0026#34; \\ -e \u0026#34;KONG_VAULT_HCV_KV=v2\u0026#34; \\ -e \u0026#34;KONG_VAULT_HCV_AUTH_METHOD=token\u0026#34; \\ -e \u0026#34;KONG_VAULT_HCV_TOKEN=kong\u0026#34; \\ -e KONG_LICENSE_DATA \\ -p 8000:8000 \\ -p 8443:8443 \\ -p 8001:8001 \\ -p 8444:8444 \\ -p 8002:8002 \\ -p 8445:8445 \\ -p 8003:8003 \\ -p 8004:8004 \\ kong/kong-gateway:3.1.1.3   コンテナーが無事起動しているであれば、ちゃんとVaultからSecretが参照できましたね！\nSecretを利用しプラグインをデプロイ Vaultへのアクセス情報を設定しているため、PluginをデプロイするときにもSecretを参照することができます。以下の例では、Proxy Caching Advancedプラグインをデプロイするときに、config.redis.passwordの設定はVaultからの参照にしています。\n1 2 3 4 5 6 7 8 9 10  curl localhost:8001/services/example-service/plugins \\ --data \u0026#34;name=proxy-cache-advanced\u0026#34; \\ --data \u0026#34;config.response_code=200\u0026#34; \\ --data \u0026#34;config.request_method=GET\u0026#34; \\ --data \u0026#34;config.content_type=application/json; charset=utf-8\u0026#34; \\ --data \u0026#34;config.cache_control=false\u0026#34; \\ --data \u0026#34;config.strategy=redis\u0026#34; \\ --data \u0026#34;config.redis.host=host.docker.internal\u0026#34; \\ --data \u0026#34;config.redis.port=6379\u0026#34; \\ --data \u0026#34;config.redis.password={vault://hcv/redis/password}\u0026#34;   以上！Kong GWで機密情報の管理や参照の方法をご紹介しました。現時点では、環境変数、HashiCorp Vault、 AWS Secrets Manager、およびGCP Secrets Managerのドライバはbuilt-inで入っているのですぐ利用できます。Azure Key Vaultファンの方には朗報です。近いうちにそちらもサポートするようになるのでもう少しお待ちください！\n","permalink":"https://wenhan.blog/post/20230223_vault_konggwdeploy_via_sm/","summary":"背景 Kong Gateway 3.0 から、Secrets ManagementがGAとなりました。Kong Gateway は、データベースのパスワードからプラグインで使用される API キーまで、多くのSecretに依存して動作します。以前ではRBACを使って、Admin APIとKong Managerから機密情報へのアクセスを制限できましたが、Secretを平文で表示されずに管理できたら嬉しいですよね。これを実現できるのはSecrets Managementです。\nサポートするVault 現時点で、サポートしているVaultは以下の４種類です。\n AWS Secrets Manager GCP Secrets Manager HashiCorp Vault Environment Variable  Kong は、上記の各システムを抽象化して、利用するときにVaultのキーワード(hcv、aws、gcpまたは env)を変更するだけで利用できます。たとえば、HashiCorp Vaultの Postgres Secretのpassword フィールドにアクセスするには、次のフォーマットで参照できます。\n{vault://hcv/postgres/password}  AWS Secrets Managerの場合\n{vault://aws/postgres/password}  環境変数の場合\nexport POSTGRES='{\u0026quot;username\u0026quot;:\u0026quot;user\u0026quot;, \u0026quot;password\u0026quot;:\u0026quot;pass\u0026quot;}' {vault://env/postgres/password}  デモ では実際にSecrets Managementを使ってVaultのSecretsを参照し、Kongのデプロイを試してみよう\nVault環境を用意 ここで、TOKEN_IDをkongにします。この値は後の認証するときに使用されます。\n1 2 3 4 5 6  docker run -d --name vault.idp \\ --network=kong-net \\ -e \u0026#34;VAULT_DEV_ROOT_TOKEN_ID=kong\u0026#34; \\ -p 8200:8200 \\ --cap-add=IPC_LOCK \\ vault:latest   Secretを作成 コンテナーに入って、Secretを作成しましょう","title":"HashiCorp Vaultを参照しKongGatewayをデプロイ"},{"content":"背景 Kong Gatewayを使ってAPIにアクセスする時には、もしリクエストが失敗したら通知して欲しいですね。通常のやり方はLog系のプラグインでLogを保存してから、3rd partyの製品（ELK）でAlertを設定して通知することです。でも他の製品を使うとデプロイも面倒だし、ライセンスや設定内容も必要だし、できればKong内部で実現したいな。。という要望があると思います。 今回は、Exit Transformer プラグインを使って、リクエストがエラーだったらLuaスクリプトでSlack Webhookを叩くことをメモします。このプラグインは、Lua 関数を使用して、Kong 応答終了メッセージを変換およびカスタマイズすることができます。メッセージ、ステータスコード、ヘッダーの変更から、Kong 応答の構造の完全な変換まで、さまざまな機能があります。\nプラグインを試す まずはページ上にある例を試してみよう。\nサービスとルートを作成 1 2  $ http :8001/services name=example.com host=mockbin.org $ http -f :8001/services/example.com/routes hosts=example.com   失敗させるため key auth プラグインを実装 1  $ http :8001/services/example.com/plugins name=key-auth   Luaスクリプトを作成 以下のコードでは、x-some-headerのヘッダーを追加し、メッセージの最後に, arrを追加した。 この内容をtransform.luaとして保存する。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15  -- transform.lua return function(status, body, headers) if not body or not body.message then return status, body, headers end headers = { [\u0026#34;x-some-header\u0026#34;] = \u0026#34;some value\u0026#34; } local new_body = { error = true, status = status, message = body.message .. \u0026#34;, arr!\u0026#34;, } return status, new_body, headers end   スクリプトを使ってプラグインをデプロイ 1 2 3  http -f :8001/services/example.com/plugins \\ name=exit-transformer \\ config.functions=@transform.lua   動作確認 ここまで設定したらアクセスしてみましょう。ヘッダーと最後のメッセージarrがちゃんとレスポンスに追加されました。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16  ❯ http :8000 Host:example.com HTTP/1.1 401 Unauthorized Connection: keep-alive Content-Length: 73 Content-Type: application/json; charset=utf-8 Date: Fri, 10 Feb 2023 07:36:15 GMT Server: kong/3.1.1.3-enterprise-edition WWW-Authenticate: Key realm=\u0026#34;kong\u0026#34; X-Kong-Response-Latency: 2 x-some-header: some value { \u0026#34;error\u0026#34;: true, \u0026#34;message\u0026#34;: \u0026#34;No API key found in request, arr!\u0026#34;, \u0026#34;status\u0026#34;: 401 }   Webhookをスクリプトに実装 今回はSlackのWebhookを利用します。メッセージに内容を追加した後にWebhookをキックする実装にします。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18  ...localhttpc=require(\u0026#34;resty.http\u0026#34;).new()-- Single-shot requests use the `request_uri` interface. localres,err=httpc:request_uri(\u0026#34;https://hooks.slack.com/services/xxxxxx/xxxxxx/xxxxxxxx\u0026#34;,{method=\u0026#34;POST\u0026#34;,body=\u0026#39;{\u0026#34;text\u0026#34;: \u0026#34;Hello, world.\u0026#34;}\u0026#39;,headers={[\u0026#34;Content-Type\u0026#34;]=\u0026#34;application/json\u0026#34;,},})ifnotresthenngx.log(ngx.ERR,\u0026#34;request failed: \u0026#34;,err)returnendreturnstatus,new_body,headers...  リクエストを送るために、lua-resty-httpを利用しています。一般的にはrequireを利用してリクエストを出しますが、nginx イベント ループでうまく機能するとは思わない心配があります。つまり、hooks.slack.com からの応答を待機している間、nginx が他のリクエストを処理できなくなる可能性があります。\nもう一つ注意すべきところは、外部のライブラリを呼び出しているため、untrusted_luaパラメータをOnにする必要があります。これをしないと以下のようなエラーが出ます。\n1  2023/02/10 02:19:13 [error] 2175#0: *15812 lua entry thread aborted: runtime error: /usr/local/share/lua/5.1/kong/tools/sandbox.lua:88: require \u0026#39;ssl.https\u0026#39; not allowed within sandbox   もう一度動作確認 さてさっきのAPIにもう一度アクセスしてSlackのメッセージを確認しましょう！\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16  ❯ http :8000 Host:example.com HTTP/1.1 401 Unauthorized Connection: keep-alive Content-Length: 73 Content-Type: application/json; charset=utf-8 Date: Mon, 13 Feb 2023 13:28:42 GMT Server: kong/3.1.1.3-enterprise-edition WWW-Authenticate: Key realm=\u0026#34;kong\u0026#34; X-Kong-Response-Latency: 0 x-some-header: some value { \u0026#34;error\u0026#34;: true, \u0026#34;message\u0026#34;: \u0026#34;No API key found in request, arr!\u0026#34;, \u0026#34;status\u0026#34;: 401 }   きたーーーーーーーーーーーーー！\n","permalink":"https://wenhan.blog/post/20230220_exit_slack/","summary":"背景 Kong Gatewayを使ってAPIにアクセスする時には、もしリクエストが失敗したら通知して欲しいですね。通常のやり方はLog系のプラグインでLogを保存してから、3rd partyの製品（ELK）でAlertを設定して通知することです。でも他の製品を使うとデプロイも面倒だし、ライセンスや設定内容も必要だし、できればKong内部で実現したいな。。という要望があると思います。 今回は、Exit Transformer プラグインを使って、リクエストがエラーだったらLuaスクリプトでSlack Webhookを叩くことをメモします。このプラグインは、Lua 関数を使用して、Kong 応答終了メッセージを変換およびカスタマイズすることができます。メッセージ、ステータスコード、ヘッダーの変更から、Kong 応答の構造の完全な変換まで、さまざまな機能があります。\nプラグインを試す まずはページ上にある例を試してみよう。\nサービスとルートを作成 1 2  $ http :8001/services name=example.com host=mockbin.org $ http -f :8001/services/example.com/routes hosts=example.com   失敗させるため key auth プラグインを実装 1  $ http :8001/services/example.com/plugins name=key-auth   Luaスクリプトを作成 以下のコードでは、x-some-headerのヘッダーを追加し、メッセージの最後に, arrを追加した。 この内容をtransform.luaとして保存する。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15  -- transform.lua return function(status, body, headers) if not body or not body.","title":"Kong Gatewayで実装！リクエストが失敗したらSlackWebhookで通知"},{"content":"Kong GatewayのPoCをやっている時に、ある問題に遭いました。 PoCの要件は以下になる\n Key-authプラグインでGateway全体を保護 Request-transformerプラグインで必要なAPI KeyをHeaderに付与し認証を突破  二つのプラグインを設定した後、API keyをrequestに追加されても認証されない\n1 2 3 4 5 6 7 8 9 10 11  # key-auth is enabled ❯ http --header localhost:8000/demo | head -1 HTTP/1.1 401 Unauthorized # Got 401 after creating request-transformer-adv plugin. ❯ curl -X POST http://localhost:8001/services/testsvc/plugins \\ --data \u0026#34;name=request-transformer-advanced\u0026#34; \\ --data \u0026#34;config.add.headers=apikey:wenhandemo\u0026#34; ... ❯ http --header localhost:8000/demo | head -1 HTTP/1.1 401 Unauthorized   理由を調べたら、どうやらPluginのデフォルトの実行順番があるらしいです。\nhttps://docs.konghq.com/gateway/latest/plugin-development/custom-logic/#plugins-execution-order\nkey-authのプライオリティは1250、request-transformer-advの801より高いです。よってkey-authが実行するときに、Headerに必要なAPI Keyがまだ付与されていない状態になります。\n解決方法として、Kong Gateway 3.0に新しく追加された機能、plugin orderingでプラグインの実行順番を明示すればOKです。\nhttps://docs.konghq.com/gateway/3.0.x/kong-enterprise/plugin-ordering/get-started/\nよしでは実装してみよう。request-transformer-advancedのプラグインを設定するときに、key-authの前にすると宣言します。\n1 2 3 4 5 6 7  ❯ curl -X POST http://localhost:8001/services/testsvc/plugins \\ --data \u0026#34;name=request-transformer-advanced\u0026#34; \\ --data \u0026#34;config.add.headers=apikey:wenhandemo\u0026#34; \\ --data \u0026#34;ordering.before.access=key-auth\u0026#34; ... ❯ http --header localhost:8000/demo | head -1 HTTP/1.1 200 OK   ジャジャン♪\n","permalink":"https://wenhan.blog/post/20230127_plugin_ordering/","summary":"Kong GatewayのPoCをやっている時に、ある問題に遭いました。 PoCの要件は以下になる\n Key-authプラグインでGateway全体を保護 Request-transformerプラグインで必要なAPI KeyをHeaderに付与し認証を突破  二つのプラグインを設定した後、API keyをrequestに追加されても認証されない\n1 2 3 4 5 6 7 8 9 10 11  # key-auth is enabled ❯ http --header localhost:8000/demo | head -1 HTTP/1.1 401 Unauthorized # Got 401 after creating request-transformer-adv plugin. ❯ curl -X POST http://localhost:8001/services/testsvc/plugins \\ --data \u0026#34;name=request-transformer-advanced\u0026#34; \\ --data \u0026#34;config.add.headers=apikey:wenhandemo\u0026#34; ... ❯ http --header localhost:8000/demo | head -1 HTTP/1.1 401 Unauthorized   理由を調べたら、どうやらPluginのデフォルトの実行順番があるらしいです。\nhttps://docs.konghq.com/gateway/latest/plugin-development/custom-logic/#plugins-execution-order\nkey-authのプライオリティは1250、request-transformer-advの801より高いです。よってkey-authが実行するときに、Headerに必要なAPI Keyがまだ付与されていない状態になります。","title":"Kong GatewayのPlugin Ordering機能を試す"},{"content":"Event Hooks機能を利用することで、Kong Gatewayで特定のイベントが発生したときに通知を受け取ることができます。新しい管理者やサービスを作成したり、プラグインによる制限が有効になったりすることを監視したい場合に役に立ちます。\nどのような機能? Event Hooksには以下の４種類があります。\n webhook: 定義されたフォーマットのPOSTリクエストを送信 webhook-custom: カスタムしたHTTPリクエストを送信 log: Eventの内容をKong Gatewayのエラーログに記録 lambda: 事前に用意したLua関数を起動  ここでは、Webhook-customとlogを試していきます。\n利用可能なEventを確認 Kong Gateway は adminAPIの/event-hooks/sources エンドポイントを提供します。ここから利用できるソース、Eventとパラメータを確認することができます。データが大量にあるので、一部抜粋で説明します。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20  ... \u0026#34;rate-limiting-advanced\u0026#34;: { \u0026#34;rate-limit-exceeded\u0026#34;: { \u0026#34;fields\u0026#34;: [ \u0026#34;consumer\u0026#34;, \u0026#34;ip\u0026#34;, \u0026#34;service\u0026#34;, \u0026#34;rate\u0026#34;, \u0026#34;limit\u0026#34;, \u0026#34;window\u0026#34; ], \u0026#34;unique\u0026#34;: [ \u0026#34;consumer\u0026#34;, \u0026#34;ip\u0026#34;, \u0026#34;service\u0026#34; ], \u0026#34;description\u0026#34;: \u0026#34;Run an event when a rate limit has been exceeded\u0026#34; } }, ...   Event Hookの登録と検証 上記の例では、ソースがrate-limiting-advanced、Eventはrate-limit-exceeded、パラメータにconsumerやipなどが利用できます。こちらの情報を使って、まずはwebhook-customのEvent Hookを作成します。\n1 2 3 4 5 6 7 8 9  curl -k -X POST \u0026#39;http://localhost:31001/event-hooks\u0026#39; \\ -H \u0026#39;Kong-Admin-Token: kong\u0026#39; \\ --data \u0026#34;source=rate-limiting-advanced\u0026#34; \\ --data \u0026#34;event=rate-limit-exceeded\u0026#34; \\ --data \u0026#34;handler=webhook-custom\u0026#34; \\ --data \u0026#34;config.method=POST\u0026#34; \\ --data \u0026#34;config.headers.content-type=application/json\u0026#34; \\ --data \u0026#34;config.payload.text=Rate limit exceeded by username \u0026#39;{{ consumer.username }}\u0026#39; on service \u0026#39; {{ service.name }}\u0026#39; (on server owned by $USER) \u0026#34; \\ --data \u0026#34;config.url=https://hooks.slack.com/services/TTTTTTTTT/YYYYYYYYYYY/xxxxxxxxxxxxxxxxxx\u0026#34;   これでRate LimitのEventが発生するときに、以下のようなメッセージがSlackチャンネルに送信されます。\n1  Rate limit exceeded by username \u0026#39;Joe\u0026#39; on service \u0026#39;test\u0026#39; (on server owned by ubuntu)   logタイプのEvent Hooksの登録も簡単で、handlerの部分をlogに変更するだけです。\n1 2 3 4 5  curl -k -X POST \u0026#39;http://localhost:31001/event-hooks\u0026#39; \\ -H \u0026#39;Kong-Admin-Token: kong\u0026#39; \\ --data \u0026#34;source=rate-limiting-advanced\u0026#34; \\ --data \u0026#34;event=rate-limit-exceeded\u0026#34; \\ --data \u0026#34;handler=log\u0026#34;   これによって、イベントが発生したら、以下みたいなログが出力されます。 ログレベルをDEBUGに変更する必要があります。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25  2022/11/21 04:27:36 [debug] 2120#0: *1338 [kong] event_hooks.lua:?:452 [core]--------------------------------------------------------------------------------------+ 2022/11/21 04:27:36 [debug] 2120#0: *1338 |\u0026#34;log callback: \u0026#34; { \u0026#34;rate-limit-exceeded\u0026#34;, \u0026#34;rate-limiting-advanced\u0026#34;, { | 2022/11/21 04:27:36 [debug] 2120#0: *1338 | consumer = {}, | 2022/11/21 04:27:36 [debug] 2120#0: *1338 | ip = \u0026#34;10.42.0.1\u0026#34;, | 2022/11/21 04:27:36 [debug] 2120#0: *1338 | limit = 3, | 2022/11/21 04:27:36 [debug] 2120#0: *1338 | rate = 4, | 2022/11/21 04:27:36 [debug] 2120#0: *1338 | service = { | 2022/11/21 04:27:36 [debug] 2120#0: *1338 | connect_timeout = 60000, | 2022/11/21 04:27:36 [debug] 2120#0: *1338 | created_at = 1668754239, | 2022/11/21 04:27:36 [debug] 2120#0: *1338 | enabled = true, | 2022/11/21 04:27:36 [debug] 2120#0: *1338 | host = \u0026#34;httpbin.org\u0026#34;, | 2022/11/21 04:27:36 [debug] 2120#0: *1338 | id = \u0026#34;9d0bee90-04b9-4af3-993f-b10db881b144\u0026#34;, | 2022/11/21 04:27:36 [debug] 2120#0: *1338 | name = \u0026#34;demo\u0026#34;, | 2022/11/21 04:27:36 [debug] 2120#0: *1338 | path = \u0026#34;/anything\u0026#34;, | 2022/11/21 04:27:36 [debug] 2120#0: *1338 | port = 80, | 2022/11/21 04:27:36 [debug] 2120#0: *1338 | protocol = \u0026#34;http\u0026#34;, | 2022/11/21 04:27:36 [debug] 2120#0: *1338 | read_timeout = 60000, | 2022/11/21 04:27:36 [debug] 2120#0: *1338 | retries = 5, | 2022/11/21 04:27:36 [debug] 2120#0: *1338 | updated_at = 1668754239, | 2022/11/21 04:27:36 [debug] 2120#0: *1338 | write_timeout = 60000, | 2022/11/21 04:27:36 [debug] 2120#0: *1338 | ws_id = \u0026#34;d06c2d4d-3fb1-49ae-a4c8-5806328c591d\u0026#34; | 2022/11/21 04:27:36 [debug] 2120#0: *1338 | }, | 2022/11/21 04:27:36 [debug] 2120#0: *1338 | window = \u0026#34;minute\u0026#34; | 2022/11/21 04:27:36 [debug] 2120#0: *1338 | }, 2120 } | 2022/11/21 04:27:36 [debug] 2120#0: *1338 +------------------------------------------------------------------------------------------------------------------------+   Event Hooksで実現できることはまだまだたくさんありますが、今日はここまでにしましょう。興味を持つ人はぜひチャレンジしてみてください。\n","permalink":"https://wenhan.blog/post/20221122_event-hooks/","summary":"Event Hooks機能を利用することで、Kong Gatewayで特定のイベントが発生したときに通知を受け取ることができます。新しい管理者やサービスを作成したり、プラグインによる制限が有効になったりすることを監視したい場合に役に立ちます。\nどのような機能? Event Hooksには以下の４種類があります。\n webhook: 定義されたフォーマットのPOSTリクエストを送信 webhook-custom: カスタムしたHTTPリクエストを送信 log: Eventの内容をKong Gatewayのエラーログに記録 lambda: 事前に用意したLua関数を起動  ここでは、Webhook-customとlogを試していきます。\n利用可能なEventを確認 Kong Gateway は adminAPIの/event-hooks/sources エンドポイントを提供します。ここから利用できるソース、Eventとパラメータを確認することができます。データが大量にあるので、一部抜粋で説明します。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20  ... \u0026#34;rate-limiting-advanced\u0026#34;: { \u0026#34;rate-limit-exceeded\u0026#34;: { \u0026#34;fields\u0026#34;: [ \u0026#34;consumer\u0026#34;, \u0026#34;ip\u0026#34;, \u0026#34;service\u0026#34;, \u0026#34;rate\u0026#34;, \u0026#34;limit\u0026#34;, \u0026#34;window\u0026#34; ], \u0026#34;unique\u0026#34;: [ \u0026#34;consumer\u0026#34;, \u0026#34;ip\u0026#34;, \u0026#34;service\u0026#34; ], \u0026#34;description\u0026#34;: \u0026#34;Run an event when a rate limit has been exceeded\u0026#34; } }, .","title":"Kong GatewayのWebhook/event hookを試した"},{"content":" NOTE: https://svenwal.de/blog/20210316_kong_manager_install/ より翻訳\n TL;DR: Kong Managerが動作しない場合は、KONG_ADMIN_API_URIとKONG_ADMIN_GUI_URLの設定を確認してください。\nKong Managerの紹介 2021年2月より、Kong Manager（長年Kong Enterpriseの機能でした）はKongの無料版の一部となりました。\n私は長い間Kong Managerを使用しており、多くのユーザーが正しくセットアップするのを助けてきましたので、幾つの典型的な設定の問題を共有したいと思います。\nKong Managerは利用しやすい Kong Managerはout of the boxでありすぐに使えます。ローカル マシンに Kong (Free または Enterprise) をインストールし、有効にすると、 http://localhost:8002でKong Managerをすぐアクセスできます。\n壊れ方 (直し方) 実際のインストールでは、Kong Managerを適当なローカルマシンに置くのではなく、サーバーにインストールし、適切なDNSエントリを使ってアクセスする必要があります。そして、そうしている間、8002のようなポートを持つのではなく、異なるホストネームを使用したいのです。\nそれでは、KongはLoadBalancer/Ingress/\u0026hellip;の後ろにインストールされ、Kong Managerをhttps://kong-manager.my-company.example.comのような素敵なホスト名でを公開していると仮定しましょう。これを開くと、Kongマネージャが表示されますが、デフォルトのワークスペースは消えていて、新しいワークスペースを作成するボタンもありません。では、何が起こったのでしょうか？\nKongのすべてがAPIであり、Kong Managerのユーザーインターフェイス全体が、ローカルのブラウザで動作するブラウザベースのアプリケーションであリます。そして、設定を変更していない場合、デフォルトのAdmin-APIアドレス（http://localhost:8001）への呼び出しが開始されます。\nKongはAdmin-APIの外部URLを知ることができないため、設定でそれを指定する必要があります。例えば、8001-Portをhttps://kong-admin.my-company.example.com のようにマッピングしたとすると、次のように設定する必要があります。\n1  admin_api_uri = https://kong-admin.my-company.example.com  または（環境変数を使用している場合）\n1  kong_admin_api_uri = https://kong-admin.my-company.example.com  これで、ブラウザはAdmin APIがどこにあるかを知り、呼び出しを開始するようになりました。しかし、実際に試してみると、やはりうまくアクセスできません。では、何が足りないのでしょうか？\n私たちはKong ManagerとAdmin APIに異なるホスト名を作成しましたが、これはブラウザのCORS保護をトリガーします。https://kong-manager.my-company.example.com 上の JavaScript は https://kong-admin.my-company.example.com への呼び出しを試み、ブラウザはそれを拒否します（クロスオリジンのため）。そこで、第二段階として、Admin-API が正しい Allow-Origin-Header を送信しているかどうかを確認する必要があります。そのためには、Kong ManagerのURLがどのようなものであるかをKongに伝える必要があります。\n1  admin_gui_url = https://kong-manager.my-company.example.com  または（環境変数を使用している場合）\n1  KONG_ADMIN_GUI_URL = https://kong-manager.my-company.example.com  これで無事Workspaceにアクセスすることができました。\nRBACとロギングを有効に Kong Enterpriseを使用する場合、通常、RBACを使用してAdmin-APIとKong Managerを保護する必要があります。Basic-authに対しすべてをセットアップし、Kong-Admin-Token headerがadmin APIでうまく機能すると仮定してみましょう。しかし、ブラウザを開いてログインすると（ヒント：起動時にパスワードを設定した場合、標準のユーザ名はkong_adminです）、うまくいきません。\n上記の例では、ログイン自体はうまくいきますが、作成されたセッションcookieが両方のドメインで有効ではありません（https://kong-manager.my-company.example.com でログインすると、cookieは UI でのみ有効で、Admin-API では有効ではありません）。\nこのcookieをブラウザがDNS名で受け入れるようにするためには、両方のDNSエントリで有効になるように設定する必要があります。\n1  admin_gui_session_conf = {\u0026#34;cookie_domain\u0026#34;: \u0026#34;my-company.example.com\u0026#34;, \u0026#34;secret\u0026#34;: \u0026#34;your-random-secret\u0026#34;, \u0026#34;cookie_secure\u0026#34;:false}   または (環境変数を使っている場合)\n1  KONG_ADMIN_GUI_SESSION_CONF = {\u0026#34;cookie_domain\u0026#34;: \u0026#34;my-company.example.com\u0026#34;, \u0026#34;secret\u0026#34;: \u0026#34;your-random-secret\u0026#34;, \u0026#34;cookie_secure\u0026#34;: false}.  ここで重要なのは cookie_domain です。両方の URL が共通に持つサブドメインを設定する必要があります。この例では my-company.example.com が両方の URL で共有されています。\nヒント: この例では、Cookie_secureも追加しています。httpのみでKongを公開している場合に備えて、ここに追加しておきたいと思っただけです。\n NOTE: 特定のドメイン、特にクラウドプロバイダーからのドメイン（例えば.amazonaws.com）は、ブラウザ内でcookieがブロックされています。cookieに問題がある場合は、StackOverflowのこの投稿を参照してください。\n Developer Portalも API ベースのユーザーインターフェースの主要な原則について多くを学んだので、Developer Portal(開発者ポータル)についても同じ原則（ウェブベースのユーザーインターフェースと API）を共有しています。そのため、これを動作させるために同様のことをしなければならないです。\n1 2 3 4  portal_gui_protocol = httpsportal_gui_host = kong-portal.my-company.example.comportal_api_url = https://kong-portal-api.my-company.example.comportal_session_conf = {\u0026#34;cookie_name\u0026#34;:\u0026#34;portal_session\u0026#34;,\u0026#34;secret\u0026#34;:\u0026#34;another-random-secret\u0026#34;,\u0026#34;cookie_secure\u0026#34;:false,\u0026#34;cookie_domain\u0026#34;:\u0026#34;my-company.example.com\u0026#34;}   または (環境変数を使っている場合)\n1 2 3 4  KONG_PORTAL_GUI_PROTOCOL = httpsKONG_PORTAL_GUI_HOST = kong-portal.my-company.example.comKONG_PORTAL_API_URL = https://kong-portal-api.my-company.example.comKONG_PORTAL_SESSION_CONF = {\u0026#34;cookie_name\u0026#34;:\u0026#34;portal_session\u0026#34;,\u0026#34;secret\u0026#34;:\u0026#34;another-random-secret\u0026#34;,\u0026#34;cookie_secure\u0026#34;:false,\u0026#34;cookie_domain\u0026#34;:\u0026#34;my-company.example.com\u0026#34;}   OpenID Connectを使用する場合、この設定は必要ありません。代わりに、OIDCプラグインのconfig.session_cookie_domainを確認してください（この例では、config.session_cookie_domain=my-company.example.comとなります）。\n NOTE: 特定のドメイン、特にクラウドプロバイダーからのドメイン（例えば.amazonaws.com）は、ブラウザ内でcookieがブロックされています。cookieに問題がある場合は、StackOverflowのこの投稿を参照してください。\n ","permalink":"https://wenhan.blog/post/20220906_gettingkongmanagertowork/","summary":"NOTE: https://svenwal.de/blog/20210316_kong_manager_install/ より翻訳\n TL;DR: Kong Managerが動作しない場合は、KONG_ADMIN_API_URIとKONG_ADMIN_GUI_URLの設定を確認してください。\nKong Managerの紹介 2021年2月より、Kong Manager（長年Kong Enterpriseの機能でした）はKongの無料版の一部となりました。\n私は長い間Kong Managerを使用しており、多くのユーザーが正しくセットアップするのを助けてきましたので、幾つの典型的な設定の問題を共有したいと思います。\nKong Managerは利用しやすい Kong Managerはout of the boxでありすぐに使えます。ローカル マシンに Kong (Free または Enterprise) をインストールし、有効にすると、 http://localhost:8002でKong Managerをすぐアクセスできます。\n壊れ方 (直し方) 実際のインストールでは、Kong Managerを適当なローカルマシンに置くのではなく、サーバーにインストールし、適切なDNSエントリを使ってアクセスする必要があります。そして、そうしている間、8002のようなポートを持つのではなく、異なるホストネームを使用したいのです。\nそれでは、KongはLoadBalancer/Ingress/\u0026hellip;の後ろにインストールされ、Kong Managerをhttps://kong-manager.my-company.example.comのような素敵なホスト名でを公開していると仮定しましょう。これを開くと、Kongマネージャが表示されますが、デフォルトのワークスペースは消えていて、新しいワークスペースを作成するボタンもありません。では、何が起こったのでしょうか？\nKongのすべてがAPIであり、Kong Managerのユーザーインターフェイス全体が、ローカルのブラウザで動作するブラウザベースのアプリケーションであリます。そして、設定を変更していない場合、デフォルトのAdmin-APIアドレス（http://localhost:8001）への呼び出しが開始されます。\nKongはAdmin-APIの外部URLを知ることができないため、設定でそれを指定する必要があります。例えば、8001-Portをhttps://kong-admin.my-company.example.com のようにマッピングしたとすると、次のように設定する必要があります。\n1  admin_api_uri = https://kong-admin.my-company.example.com  または（環境変数を使用している場合）\n1  kong_admin_api_uri = https://kong-admin.my-company.example.com  これで、ブラウザはAdmin APIがどこにあるかを知り、呼び出しを開始するようになりました。しかし、実際に試してみると、やはりうまくアクセスできません。では、何が足りないのでしょうか？\n私たちはKong ManagerとAdmin APIに異なるホスト名を作成しましたが、これはブラウザのCORS保護をトリガーします。https://kong-manager.my-company.example.com 上の JavaScript は https://kong-admin.my-company.example.com への呼び出しを試み、ブラウザはそれを拒否します（クロスオリジンのため）。そこで、第二段階として、Admin-API が正しい Allow-Origin-Header を送信しているかどうかを確認する必要があります。そのためには、Kong ManagerのURLがどのようなものであるかをKongに伝える必要があります。\n1  admin_gui_url = https://kong-manager.","title":"Kong Managerをホストネームでデプロイための設定"},{"content":"紹介 Kong GatewayではRouteでサブパスを定義し、どのServiceにアクセスするかを決めています。例えば、以下のURLの後ろのサブパスがpathAの場合は、トラフィックがserviceAにルーティングされ、最終的に後ろにあるendpointAにアクセスされます。\n1  http://mykong.org:8000/pathA =\u0026gt; serviceA =\u0026gt; endpointA   しかし、サブパスではなくて、もっと動的にリクエストの転送先を決めたい場合があります。このプラグインは、事前に定義されたヘッダーと転送先をルールに、リクエストヘッダ情報を見て転送先を決めています。ルールに二つのパラメータがあり、conditionは定義したヘッダーと値、upstream_nameは転送先のUpstreamオブジェクトです。\n1  {\u0026#34;rules\u0026#34;:[{\u0026#34;condition\u0026#34;: {\u0026#34;location\u0026#34;:\u0026#34;us-east\u0026#34;}, \u0026#34;upstream_name\u0026#34;: \u0026#34;east.doamin.com\u0026#34;}]}   この記事では、このプラグインの実装についてメモします。 注：このプラグインを操作できるのはAdmin APIを通す方法のみ。Kong Managerでの操作はできない。\nhttps://docs.konghq.com/hub/kong-inc/route-by-header/\nデモ 実際にこのプラグインを使ってみましょう。まずは作業用のserviceとrouteを作成する。serviceのendpointはhttp://httpbin.org/anything となりますが、このプラグインのルールに当てはまらない時にアクセスされます。\n1 2 3 4 5 6 7 8 9 10  curl -i -X POST http://localhost:8001/services \\  --data protocol=http \\  --data host=httpbin.org \\  --date path=/anything --data name=demo curl -i -X POST http://localhost:8001/routes \\  --data \u0026#34;paths[]=/\u0026#34; \\  --data service.id=\u0026lt;The id of service created above\u0026gt; --data name=demo   次に、転送先のUpstreamを二つ用意し、myipとdateをそれぞれアクセスするとIPアドレスと時間が表示されます。\n1 2 3 4 5  curl -i -X POST http://localhost:8001/upstreams -d name=myip curl -i -X POST http://localhost:8001/upstreams/myip/targets --data target=\u0026#34;ip.jsontest.com:80\u0026#34; curl -i -X POST http://localhost:8001/upstreams -d name=date curl -i -X POST http://localhost:8001/upstreams/date/targets --data target=\u0026#34;date.jsontest.com:80\u0026#34;   最後はrouteに対してこのプラグインを作ります。\n1 2 3  curl -i -X POST http://localhost:8001/routes/demo/plugins \\  -H \u0026#39;Content-Type: application/json\u0026#39; \\  --data \u0026#39;{\u0026#34;name\u0026#34;: \u0026#34;route-by-header\u0026#34;, \u0026#34;config\u0026#34;: {\u0026#34;rules\u0026#34;:[{\u0026#34;condition\u0026#34;: {\u0026#34;info\u0026#34;:\u0026#34;myip\u0026#34;}, \u0026#34;upstream_name\u0026#34;: \u0026#34;myip\u0026#34;}, {\u0026#34;condition\u0026#34;: {\u0026#34;info\u0026#34;:\u0026#34;date\u0026#34;}, \u0026#34;upstream_name\u0026#34;: \u0026#34;date\u0026#34;}]}}\u0026#39;   --dataの部分はJsonで記載した振り分けのルールです。headerにinfo:myipが存在したら、myipのupstreamにふりわけされ、info:dateが存在したら、dateのupstreamに振り分けられます。\n検証 リクエストのヘッダーにinfo:myipを設定したら、myipのUpstreamに転送されました。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14  $ curl -i -X GET http://localhost:8000/ -H \u0026#34;info:myip\u0026#34; HTTP/1.1 200 OK Content-Type: application/json Content-Length: 24 Connection: keep-alive Access-Control-Allow-Origin: * X-Cloud-Trace-Context: d2c85f05ccd0e7d1273aee2476504cac Date: Wed, 03 Aug 2022 04:23:29 GMT Server: Google Frontend X-Kong-Upstream-Latency: 212 X-Kong-Proxy-Latency: 0 Via: kong/2.8.1.2-enterprise-edition {\u0026#34;ip\u0026#34;: \u0026#34;18.181.83.240\u0026#34;}   リクエストのヘッダーにinfo:dateを設定したら、dateのUpstreamに転送されました。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18  $ curl -i -X GET http://localhost:8000/ -H \u0026#34;info:date\u0026#34; HTTP/1.1 200 OK Content-Type: application/json Content-Length: 100 Connection: keep-alive Access-Control-Allow-Origin: * X-Cloud-Trace-Context: b03d154c04e8d788ae3c0d870ab9c2df Date: Wed, 03 Aug 2022 04:23:33 GMT Server: Google Frontend X-Kong-Upstream-Latency: 214 X-Kong-Proxy-Latency: 0 Via: kong/2.8.1.2-enterprise-edition { \u0026#34;date\u0026#34;: \u0026#34;08-03-2022\u0026#34;, \u0026#34;milliseconds_since_epoch\u0026#34;: 1659500613301, \u0026#34;time\u0026#34;: \u0026#34;04:23:33 AM\u0026#34; }   リクエストのヘッダーを設定せず、つまりどのルールにも当てはまらない時は、serviceの元のendpoint、http://httpbin.org/anything に転送されました。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31  [ec2-user@ip-10-0-18-228 ~]$ curl -i -X GET http://localhost:8000/ HTTP/1.1 200 OK Content-Type: application/json Content-Length: 419 Connection: keep-alive Date: Wed, 03 Aug 2022 04:26:05 GMT Server: gunicorn/19.9.0 Access-Control-Allow-Origin: * Access-Control-Allow-Credentials: true X-Kong-Upstream-Latency: 335 X-Kong-Proxy-Latency: 3 Via: kong/2.8.1.2-enterprise-edition { \u0026#34;args\u0026#34;: {}, \u0026#34;data\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;files\u0026#34;: {}, \u0026#34;form\u0026#34;: {}, \u0026#34;headers\u0026#34;: { \u0026#34;Accept\u0026#34;: \u0026#34;*/*\u0026#34;, \u0026#34;Host\u0026#34;: \u0026#34;httpbin.org\u0026#34;, \u0026#34;User-Agent\u0026#34;: \u0026#34;curl/7.61.1\u0026#34;, \u0026#34;X-Amzn-Trace-Id\u0026#34;: \u0026#34;Root=1-62e9f8dd-7e352f095824a8b136de808a\u0026#34;, \u0026#34;X-Forwarded-Host\u0026#34;: \u0026#34;localhost\u0026#34;, \u0026#34;X-Forwarded-Path\u0026#34;: \u0026#34;/\u0026#34; }, \u0026#34;json\u0026#34;: null, \u0026#34;method\u0026#34;: \u0026#34;GET\u0026#34;, \u0026#34;origin\u0026#34;: \u0026#34;127.0.0.1, 18.181.83.240\u0026#34;, \u0026#34;url\u0026#34;: \u0026#34;http://localhost/anything\u0026#34; }   ","permalink":"https://wenhan.blog/post/20220802_using-kong-gw-route-by-header-plugin/","summary":"紹介 Kong GatewayではRouteでサブパスを定義し、どのServiceにアクセスするかを決めています。例えば、以下のURLの後ろのサブパスがpathAの場合は、トラフィックがserviceAにルーティングされ、最終的に後ろにあるendpointAにアクセスされます。\n1  http://mykong.org:8000/pathA =\u0026gt; serviceA =\u0026gt; endpointA   しかし、サブパスではなくて、もっと動的にリクエストの転送先を決めたい場合があります。このプラグインは、事前に定義されたヘッダーと転送先をルールに、リクエストヘッダ情報を見て転送先を決めています。ルールに二つのパラメータがあり、conditionは定義したヘッダーと値、upstream_nameは転送先のUpstreamオブジェクトです。\n1  {\u0026#34;rules\u0026#34;:[{\u0026#34;condition\u0026#34;: {\u0026#34;location\u0026#34;:\u0026#34;us-east\u0026#34;}, \u0026#34;upstream_name\u0026#34;: \u0026#34;east.doamin.com\u0026#34;}]}   この記事では、このプラグインの実装についてメモします。 注：このプラグインを操作できるのはAdmin APIを通す方法のみ。Kong Managerでの操作はできない。\nhttps://docs.konghq.com/hub/kong-inc/route-by-header/\nデモ 実際にこのプラグインを使ってみましょう。まずは作業用のserviceとrouteを作成する。serviceのendpointはhttp://httpbin.org/anything となりますが、このプラグインのルールに当てはまらない時にアクセスされます。\n1 2 3 4 5 6 7 8 9 10  curl -i -X POST http://localhost:8001/services \\  --data protocol=http \\  --data host=httpbin.org \\  --date path=/anything --data name=demo curl -i -X POST http://localhost:8001/routes \\  --data \u0026#34;paths[]=/\u0026#34; \\  --data service.","title":"Kong Gatewayの Route By Header Pluginを使ってみる"},{"content":"紹介 Kong Gatewayのプラグインを利用すれば、カナリアリリースが簡単に設定することができます。しかもカナリアリリースのモードは単なるパーセンテージではなく、徐々に利用拡大や、Writelist\u0026amp;blacklistの設定もできます。\nこの記事では、カナリアリリースのやり方についてメモします。\nhttps://docs.konghq.com/hub/kong-inc/canary/\nServiceとRouteの作成 デモの例として、現在のバージョンをhttp://httpbin.org/xmlに、新規リリースのバージョンをhttp://httpbin.org/jsonにします。なので、現在のバージョンの場合は、xmlのレスポンス、新規リリースのバージョンの場合は、jsonのレスポンスが帰ってくるはず。\n1 2 3 4 5 6 7  ❯ http POST localhost:8001/services \\ name=canary-api-service \\ url=http://httpbin.org/xml ❯ http -f POST localhost:8001/services/canary-api-service/routes \\ name=canary-api-route \\ paths=/api/canary   動作確認、xmlのレスポンスが帰ってきた。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37  ❯ http GET localhost:8000/api/canary HTTP/1.1 200 OK Access-Control-Allow-Credentials: true Access-Control-Allow-Origin: * Connection: keep-alive Content-Length: 522 Content-Type: application/xml Date: Wed, 25 May 2022 05:28:07 GMT Server: gunicorn/19.9.0 Via: kong/2.8.1.0-enterprise-edition X-Kong-Proxy-Latency: 0 X-Kong-Upstream-Latency: 295 \u0026lt;?xml version=\u0026#39;1.0\u0026#39; encoding=\u0026#39;us-ascii\u0026#39;?\u0026gt; \u0026lt;!-- A SAMPLE set of slides --\u0026gt; \u0026lt;slideshow title=\u0026#34;Sample Slide Show\u0026#34; date=\u0026#34;Date of publication\u0026#34; author=\u0026#34;Yours Truly\u0026#34; \u0026gt; \u0026lt;!-- TITLE SLIDE --\u0026gt; \u0026lt;slide type=\u0026#34;all\u0026#34;\u0026gt; \u0026lt;title\u0026gt;Wake up to WonderWidgets!\u0026lt;/title\u0026gt; \u0026lt;/slide\u0026gt; \u0026lt;!-- OVERVIEW --\u0026gt; \u0026lt;slide type=\u0026#34;all\u0026#34;\u0026gt; \u0026lt;title\u0026gt;Overview\u0026lt;/title\u0026gt; \u0026lt;item\u0026gt;Why \u0026lt;em\u0026gt;WonderWidgets\u0026lt;/em\u0026gt; are great\u0026lt;/item\u0026gt; \u0026lt;item/\u0026gt; \u0026lt;item\u0026gt;Who \u0026lt;em\u0026gt;buys\u0026lt;/em\u0026gt; WonderWidgets\u0026lt;/item\u0026gt; \u0026lt;/slide\u0026gt; \u0026lt;/slideshow\u0026gt;   カナリアリリースモード 一定期間(Set a Period) 翻訳が合っているかどうかが分からないが、リリースの開始時刻と移行期間を決めるモードです。リリース期間の初期段階はほとんど現バージョンで、そして徐々に新バージョンの出現確率が高くなり、最終的に新バージョンのみになるモードです。\n以下のコマンドの設定から、リリースの開始が10秒後、移行期間が120秒となります。\n1 2 3 4 5 6 7 8 9  ❯ current_time=`expr $(date \u0026#34;+%s\u0026#34;) + 10` \u0026amp;\u0026amp; http -f POST http://localhost:8001/routes/canary-api-route/plugins \\ name=canary \\ config.start=$current_time \\ config.duration=120 \\ config.upstream_host=httpbin.org \\ config.upstream_port=80 \\ config.upstream_uri=/json \\ config.hash=none   この設定で以下のコマンドを実行すると、最初はxmlのレスポンスになるが、徐々にJsonの方が増えていて、最終的に全部Jsonのレスポンスになります。\n1 2 3 4 5  ❯ for num in {1..70}; do echo \u0026#34;Calling API #$num\u0026#34; http http://localhost:8000/api/canary sleep 0.5 done   1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34  ❯ http GET localhost:8000/api/canary HTTP/1.1 200 OK Access-Control-Allow-Credentials: true Access-Control-Allow-Origin: * Connection: keep-alive Content-Length: 429 Content-Type: application/json Date: Wed, 25 May 2022 07:27:08 GMT Server: gunicorn/19.9.0 Via: kong/2.8.1.0-enterprise-edition X-Kong-Proxy-Latency: 4 X-Kong-Upstream-Latency: 284 { \u0026#34;slideshow\u0026#34;: { \u0026#34;author\u0026#34;: \u0026#34;Yours Truly\u0026#34;, \u0026#34;date\u0026#34;: \u0026#34;date of publication\u0026#34;, \u0026#34;slides\u0026#34;: [ { \u0026#34;title\u0026#34;: \u0026#34;Wake up to WonderWidgets!\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;all\u0026#34; }, { \u0026#34;items\u0026#34;: [ \u0026#34;Why \u0026lt;em\u0026gt;WonderWidgets\u0026lt;/em\u0026gt; are great\u0026#34;, \u0026#34;Who \u0026lt;em\u0026gt;buys\u0026lt;/em\u0026gt; WonderWidgets\u0026#34; ], \u0026#34;title\u0026#34;: \u0026#34;Overview\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;all\u0026#34; } ], \u0026#34;title\u0026#34;: \u0026#34;Sample Slide Show\u0026#34; } }   これで動作確認が終わったので、次のリリース方式をテストするために、カナリアリリースプラグインを削除する\n1  ❯ plugin_id=$(http -f http://localhost:8001/routes/canary-api-route/plugins | jq -r \u0026#39;.data[].id\u0026#39;) \u0026amp;\u0026amp; http DELETE http://localhost:8001/plugins/$plugin_id   一定割合(Set a Percentage) トラフィックが現バージョンと新バージョンにアクセスする確率を一定にするモードです。設定した割合はパーセンテージであり、新バージョンにアクセスする割合となります。\n以下のコマンドの設定、config.percentageから、現バージョンと新バージョンの割合が５割５割となっています。\n1 2 3 4 5 6 7  ❯ http -f POST http://localhost:8001/routes/canary-api-route/plugins \\ name=canary \\ config.percentage=50 \\ config.upstream_host=httpbin.org \\ config.upstream_port=80 \\ config.upstream_uri=/json \\ config.hash=none   以下のコマンドで確認すると、大体ですけど半分ずつとなっているはずです。\n1 2 3 4 5  ❯ for num in {1..10}; do echo \u0026#34;Calling API #$num\u0026#34; http http://localhost:8000/api/canary sleep 0.5 done   もしパーセンテージのテストを段階的に行いたい場合、以下のコマンドで割合を変更することもできます。\n1 2 3 4 5 6 7 8  $ plugin_id=$(http -f http://localhost:8001/routes/canary-api-route/plugins | jq -r \u0026#39;.data[].id\u0026#39;) $ http -f PUT http://localhost:8001/routes/canary-api-route/plugins/$plugin_id \\ name=canary \\ config.percentage=90 \\ config.upstream_host=httpbin.org \\ config.upstream_port=80 \\ config.upstream_uri=/json \\ config.hash=none   これで動作確認が終わったので、次のリリース方式をテストするために、カナリアリリースプラグインを削除する\n1  ❯ plugin_id=$(http -f http://localhost:8001/routes/canary-api-route/plugins | jq -r \u0026#39;.data[].id\u0026#39;) \u0026amp;\u0026amp; http DELETE http://localhost:8001/plugins/$plugin_id   Whitelist and Blacklist こちらのモードでは、リクエストに含まれたAPI KeyでユーザConsumerを特定し、Consumerが所属するグループに基づいてどのバージョンにアクセスしにいくのかを設定できます。 Consumerの特定とグループの登録は、key-authとaclのプラグインを利用します。\nまずはkey-authのプラグインを登録し、これでAPI keyなしだとアクセスができなくなります。\n1  ❯ http http://localhost:8001/routes/canary-api-route/plugins name=key-auth   次に、今回は２種類のConsumerを準備し、それぞれ別のグループに登録します。\n1 2 3  ❯ http :8001/consumers username=vip-consumer \u0026amp;\u0026amp; http :8001/consumers/vip-consumer/key-auth key=vip-api \u0026amp;\u0026amp; http :8001/consumers/vip-consumer/acls group=vip-acl ❯ http :8001/consumers username=general-consumer \u0026amp;\u0026amp; http :8001/consumers/general-consumer/key-auth key=general-api \u0026amp;\u0026amp; http :8001/consumers/general-consumer/acls group=general-acl   上の設定が終わったら、グループ情報を用いてカナリアリリースプラグインを作成します。 ここで注意してほしいのは、config.hashです。whitelistかblacklistを設定する必要がありまして、whitelistに設定したグループは新バージョンにアクセスしにいきます。\n1 2 3 4 5 6 7  ❯ http -f POST http://localhost:8001/routes/canary-api-route/plugins \\ name=canary \\ config.hash=whitelist \\ config.groups=vip-acl \\ config.upstream_host=httpbin.org \\ config.upstream_port=80 \\ config.upstream_uri=/json   vip-apiを持ってアクセスするクライアント、つまりConsumerのグループはvip-aclであり、whitelistに登録されているので新バージョンにアクセスしにいきます。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34  ❯ http http://localhost:8000/api/canary apiKey:vip-api HTTP/1.1 200 OK Access-Control-Allow-Credentials: true Access-Control-Allow-Origin: * Connection: keep-alive Content-Length: 429 Content-Type: application/json Date: Wed, 25 May 2022 07:33:45 GMT Server: gunicorn/19.9.0 Via: kong/2.8.1.0-enterprise-edition X-Kong-Proxy-Latency: 118 X-Kong-Upstream-Latency: 290 { \u0026#34;slideshow\u0026#34;: { \u0026#34;author\u0026#34;: \u0026#34;Yours Truly\u0026#34;, \u0026#34;date\u0026#34;: \u0026#34;date of publication\u0026#34;, \u0026#34;slides\u0026#34;: [ { \u0026#34;title\u0026#34;: \u0026#34;Wake up to WonderWidgets!\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;all\u0026#34; }, { \u0026#34;items\u0026#34;: [ \u0026#34;Why \u0026lt;em\u0026gt;WonderWidgets\u0026lt;/em\u0026gt; are great\u0026#34;, \u0026#34;Who \u0026lt;em\u0026gt;buys\u0026lt;/em\u0026gt; WonderWidgets\u0026#34; ], \u0026#34;title\u0026#34;: \u0026#34;Overview\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;all\u0026#34; } ], \u0026#34;title\u0026#34;: \u0026#34;Sample Slide Show\u0026#34; } }   一方、general-apiを持ってアクセスするクライアント、つまりConsumerのグループはgeneral-aclのため、現バージョンにアクセスしにいきます。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37  ❯ http http://localhost:8000/api/canary apiKey:general-api HTTP/1.1 200 OK Access-Control-Allow-Credentials: true Access-Control-Allow-Origin: * Connection: keep-alive Content-Length: 522 Content-Type: application/xml Date: Wed, 25 May 2022 07:33:55 GMT Server: gunicorn/19.9.0 Via: kong/2.8.1.0-enterprise-edition X-Kong-Proxy-Latency: 5 X-Kong-Upstream-Latency: 289 \u0026lt;?xml version=\u0026#39;1.0\u0026#39; encoding=\u0026#39;us-ascii\u0026#39;?\u0026gt; \u0026lt;!-- A SAMPLE set of slides --\u0026gt; \u0026lt;slideshow title=\u0026#34;Sample Slide Show\u0026#34; date=\u0026#34;Date of publication\u0026#34; author=\u0026#34;Yours Truly\u0026#34; \u0026gt; \u0026lt;!-- TITLE SLIDE --\u0026gt; \u0026lt;slide type=\u0026#34;all\u0026#34;\u0026gt; \u0026lt;title\u0026gt;Wake up to WonderWidgets!\u0026lt;/title\u0026gt; \u0026lt;/slide\u0026gt; \u0026lt;!-- OVERVIEW --\u0026gt; \u0026lt;slide type=\u0026#34;all\u0026#34;\u0026gt; \u0026lt;title\u0026gt;Overview\u0026lt;/title\u0026gt; \u0026lt;item\u0026gt;Why \u0026lt;em\u0026gt;WonderWidgets\u0026lt;/em\u0026gt; are great\u0026lt;/item\u0026gt; \u0026lt;item/\u0026gt; \u0026lt;item\u0026gt;Who \u0026lt;em\u0026gt;buys\u0026lt;/em\u0026gt; WonderWidgets\u0026lt;/item\u0026gt; \u0026lt;/slide\u0026gt; \u0026lt;/slideshow\u0026gt;   アップグレードを完了させる カナリアリリースのテストが終わったら、特に問題がない場合、全てのトラフィックをプラグインなしで新バージョンにrouteしていきたい。\nそこで実行すべきのは、serviceのUpstream Endpointを更新し、カナリアリリースプラグインを削除することです。\n1 2 3 4 5 6  # update service ❯ http -f PUT :8001/services/canary-api-service url=http://httpbin.org/json # delete all plugins ❯ http http://localhost:8001/routes/canary-api-route/plugins | jq -r -c \u0026#39;.data[].id\u0026#39; | while read id; do http --ignore-stdin DELETE http://localhost:8001/plugins/$id done   これでカナリアリリースで新バージョンのサービスのアップグレードが完了した！\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34  ❯ http http://localhost:8000/api/canary HTTP/1.1 200 OK Access-Control-Allow-Credentials: true Access-Control-Allow-Origin: * Connection: keep-alive Content-Length: 429 Content-Type: application/json Date: Wed, 25 May 2022 07:35:55 GMT Server: gunicorn/19.9.0 Via: kong/2.8.1.0-enterprise-edition X-Kong-Proxy-Latency: 4 X-Kong-Upstream-Latency: 288 { \u0026#34;slideshow\u0026#34;: { \u0026#34;author\u0026#34;: \u0026#34;Yours Truly\u0026#34;, \u0026#34;date\u0026#34;: \u0026#34;date of publication\u0026#34;, \u0026#34;slides\u0026#34;: [ { \u0026#34;title\u0026#34;: \u0026#34;Wake up to WonderWidgets!\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;all\u0026#34; }, { \u0026#34;items\u0026#34;: [ \u0026#34;Why \u0026lt;em\u0026gt;WonderWidgets\u0026lt;/em\u0026gt; are great\u0026#34;, \u0026#34;Who \u0026lt;em\u0026gt;buys\u0026lt;/em\u0026gt; WonderWidgets\u0026#34; ], \u0026#34;title\u0026#34;: \u0026#34;Overview\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;all\u0026#34; } ], \u0026#34;title\u0026#34;: \u0026#34;Sample Slide Show\u0026#34; } }   ","permalink":"https://wenhan.blog/post/20220531_using-kong-canary-release-plugin/","summary":"紹介 Kong Gatewayのプラグインを利用すれば、カナリアリリースが簡単に設定することができます。しかもカナリアリリースのモードは単なるパーセンテージではなく、徐々に利用拡大や、Writelist\u0026amp;blacklistの設定もできます。\nこの記事では、カナリアリリースのやり方についてメモします。\nhttps://docs.konghq.com/hub/kong-inc/canary/\nServiceとRouteの作成 デモの例として、現在のバージョンをhttp://httpbin.org/xmlに、新規リリースのバージョンをhttp://httpbin.org/jsonにします。なので、現在のバージョンの場合は、xmlのレスポンス、新規リリースのバージョンの場合は、jsonのレスポンスが帰ってくるはず。\n1 2 3 4 5 6 7  ❯ http POST localhost:8001/services \\ name=canary-api-service \\ url=http://httpbin.org/xml ❯ http -f POST localhost:8001/services/canary-api-service/routes \\ name=canary-api-route \\ paths=/api/canary   動作確認、xmlのレスポンスが帰ってきた。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37  ❯ http GET localhost:8000/api/canary HTTP/1.","title":"KongのCanary Release pluginを使ってみる"},{"content":"紹介 Mockingプラグインは、開発中のAPIに対するテストを行うために、模擬のエンドポイントを提供するものです。Open API（OAS）仕様に基づいて、APIに送信されたリクエストに模擬応答をレスポンスします。また、テストの柔軟性のため、Mockingプラグインを簡単に有効または無効にすることができます。 https://docs.konghq.com/hub/kong-inc/mocking/\n注意点として、このプラグインは、200、201と204の応答をレスポンスすることができます。正常動作をテストするために開発されたもので、200番台以外のコードをを返すことができません。\n今回は、db-less環境のKong-gatewayをDocker containerで構築し、その上に Mocking Pluginを試してみます。\nKong-gatewayをデプロイする前に db-lessの環境のため、Admin APIやGUIで設定を変更することはできません（保存先はないから）。そのため、起動時にDeclarative Configurationという名の設定ファイルを事前にコンテナーに食わせる必要があります。\nhttps://docs.konghq.com/gateway/2.8.x/reference/db-less-and-declarative-config/#main\n今回の目標は Mockingを試したいので、 Mockingの設定内容も事前にこのファイルに記述していきます。Mocking Pluginの紹介ページで書いたように、設定自体はとてもシンプルです。ただ、肝心なのはapi_specificationとapi_specification_filenameの二つです。この二つのパラメータは、模擬のエンドポイントのスペック内容を定義するために使われますが、違いは以下の通りです。\n api_specification_filename: スペック内容が保存されたファイル名を設定する。 api_specification: スペック内容をそのままパラメータに設定する。  api_specification_filenameは、設定するファイルを事前にDev Portalにアップロードしてから、パスなしで設定してください。設定の例はここに書いています。しかし、今回はdb-lessの環境を構築するため、そもそもdev protalが利用できないし、api_specification_filenameを利用できません。\nそのため、api_specificationにサンプルのAPIのスペック内容をそのまま設定し、コンテナーを立ち上げます。スペックのサンプルは、Mocking Pluginの紹介ページにあります。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20  _format_version:\u0026#34;1.1\u0026#34;_transform:trueservices:- host:mockbin.orgname:example_serviceport:80protocol:httproutes:- name:example_routepaths:- /mockstrip_path:trueplugins:- name:mockingconfig:api_specification:\u0026#39;{\u0026#34;swagger\u0026#34;:\u0026#34;2.0\u0026#34;,\u0026#34;info\u0026#34;:{\u0026#34;title\u0026#34;:\u0026#34;Stock API\u0026#34;,\u0026#34;description\u0026#34;:\u0026#34;Stock Information Service\u0026#34;,\u0026#34;version\u0026#34;:\u0026#34;0.1\u0026#34;},\u0026#34;host\u0026#34;:\u0026#34;127.0.0.1:8000\u0026#34;,\u0026#34;basePath\u0026#34;:\u0026#34;/\u0026#34;,\u0026#34;schemes\u0026#34;:[\u0026#34;http\u0026#34;,\u0026#34;https\u0026#34;],\u0026#34;consumes\u0026#34;:[\u0026#34;application/json\u0026#34;],\u0026#34;produces\u0026#34;:[\u0026#34;application/json\u0026#34;],\u0026#34;paths\u0026#34;:{\u0026#34;/stock/historical\u0026#34;:{\u0026#34;get\u0026#34;:{\u0026#34;description\u0026#34;:\u0026#34;\u0026#34;,\u0026#34;operationId\u0026#34;:\u0026#34;GET /stock/historical\u0026#34;,\u0026#34;produces\u0026#34;:[\u0026#34;application/json\u0026#34;],\u0026#34;tags\u0026#34;:[\u0026#34;Production\u0026#34;],\u0026#34;parameters\u0026#34;:[{\u0026#34;required\u0026#34;:true,\u0026#34;in\u0026#34;:\u0026#34;query\u0026#34;,\u0026#34;name\u0026#34;:\u0026#34;tickers\u0026#34;,\u0026#34;type\u0026#34;:\u0026#34;string\u0026#34;}],\u0026#34;responses\u0026#34;:{\u0026#34;200\u0026#34;:{\u0026#34;description\u0026#34;:\u0026#34;Status 200\u0026#34;,\u0026#34;examples\u0026#34;:{\u0026#34;application/json\u0026#34;:{\u0026#34;meta_data\u0026#34;:{\u0026#34;api_name\u0026#34;:\u0026#34;historical_stock_price_v2\u0026#34;,\u0026#34;num_total_data_points\u0026#34;:1,\u0026#34;credit_cost\u0026#34;:10,\u0026#34;start_date\u0026#34;:\u0026#34;yesterday\u0026#34;,\u0026#34;end_date\u0026#34;:\u0026#34;yesterday\u0026#34;},\u0026#34;result_data\u0026#34;:{\u0026#34;AAPL\u0026#34;:[{\u0026#34;date\u0026#34;:\u0026#34;2000-04-23\u0026#34;,\u0026#34;volume\u0026#34;:33,\u0026#34;high\u0026#34;:100.75,\u0026#34;low\u0026#34;:100.87,\u0026#34;adj_close\u0026#34;:275.03,\u0026#34;close\u0026#34;:100.03,\u0026#34;open\u0026#34;:100.87}]}}}}}}},\u0026#34;/stock/closing\u0026#34;:{\u0026#34;get\u0026#34;:{\u0026#34;description\u0026#34;:\u0026#34;\u0026#34;,\u0026#34;operationId\u0026#34;:\u0026#34;GET /stock/closing\u0026#34;,\u0026#34;produces\u0026#34;:[\u0026#34;application/json\u0026#34;],\u0026#34;tags\u0026#34;:[\u0026#34;Beta\u0026#34;],\u0026#34;parameters\u0026#34;:[{\u0026#34;required\u0026#34;:true,\u0026#34;in\u0026#34;:\u0026#34;query\u0026#34;,\u0026#34;name\u0026#34;:\u0026#34;tickers\u0026#34;,\u0026#34;type\u0026#34;:\u0026#34;string\u0026#34;}],\u0026#34;responses\u0026#34;:{\u0026#34;200\u0026#34;:{\u0026#34;description\u0026#34;:\u0026#34;Status 200\u0026#34;,\u0026#34;examples\u0026#34;:{\u0026#34;application/json\u0026#34;:{\u0026#34;meta_data\u0026#34;:{\u0026#34;api_name\u0026#34;:\u0026#34;closing_stock_price_v1\u0026#34;},\u0026#34;result_data\u0026#34;:{\u0026#34;AAPL\u0026#34;:[{\u0026#34;date\u0026#34;:\u0026#34;2000-06-23\u0026#34;,\u0026#34;volume\u0026#34;:33,\u0026#34;high\u0026#34;:100.75,\u0026#34;low\u0026#34;:100.87,\u0026#34;adj_close\u0026#34;:275.03,\u0026#34;close\u0026#34;:100.03,\u0026#34;open\u0026#34;:100.87}]}}}}}}}}}\u0026#39;max_delay_time:1min_delay_time:0.001random_delay:false  db-lessのKong-gateway環境を構築 ライセンスを環境変数に設定した後、以下のコマンドでコンテナーを立ち上げます。KONG_DATABASEがoffになっているところと、KONG_DECLARATIVE_CONFIGに設定ファイルのパスが書かれているところが重要です。設定ファイルは、-vでローカルからMountされています。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25  $ docker run -d --name kong-dbless \\  --network=kong-net \\  -v \u0026#34;$(pwd):/kong/declarative/\u0026#34; \\  -e \u0026#34;KONG_DATABASE=off\u0026#34; \\  -e \u0026#34;KONG_DECLARATIVE_CONFIG=/kong/declarative/kong.yaml\u0026#34; \\  -e \u0026#34;KONG_PROXY_ACCESS_LOG=/dev/stdout\u0026#34; \\  -e \u0026#34;KONG_ADMIN_ACCESS_LOG=/dev/stdout\u0026#34; \\  -e \u0026#34;KONG_PROXY_ERROR_LOG=/dev/stderr\u0026#34; \\  -e \u0026#34;KONG_ADMIN_ERROR_LOG=/dev/stderr\u0026#34; \\  -e \u0026#34;KONG_ADMIN_LISTEN=0.0.0.0:8001\u0026#34; \\  -e \u0026#34;KONG_ADMIN_GUI_URL=http://35.74.69.178:8002\u0026#34; \\  -e KONG_LICENSE_DATA \\  -p 8000:8000 \\  -p 8443:8443 \\  -p 8001:8001 \\  -p 8444:8444 \\  -p 8002:8002 \\  -p 8445:8445 \\  -p 8003:8003 \\  -p 8004:8004 \\  kong/kong-gateway:2.8.1.0-alpine $ docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 28917c9510f8 kong/kong-gateway:2.8.1.0-alpine \u0026#34;/docker-entrypoint.…\u0026#34; 18 seconds ago Up 17 seconds (healthy) 0.0.0.0:8000-8004-\u0026gt;8000-8004/tcp, :::8000-8004-\u0026gt;8000-8004/tcp, 0.0.0.0:8443-8445-\u0026gt;8443-8445/tcp, :::8443-8445-\u0026gt;8443-8445/tcp, 8446-8447/tcp kong-dbless   検証 Mocking Pluginのスペックに書いたパスを指定すれば、backendのサービスではなく、このスペックに定義した内容がResponseされていました。これでMocking Pluginが有効に働いていることが検証できました。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62  $ http :8000/mock/stock/historical HTTP/1.1 200 OK Connection: keep-alive Content-Length: 279 Content-Type: application/json; charset=utf-8 Date: Mon, 23 May 2022 05:18:16 GMT Server: kong/2.8.1.0-enterprise-edition X-Kong-Mocking-Plugin: true X-Kong-Response-Latency: 1 { \u0026#34;meta_data\u0026#34;: { \u0026#34;api_name\u0026#34;: \u0026#34;historical_stock_price_v2\u0026#34;, \u0026#34;credit_cost\u0026#34;: 10, \u0026#34;end_date\u0026#34;: \u0026#34;yesterday\u0026#34;, \u0026#34;num_total_data_points\u0026#34;: 1, \u0026#34;start_date\u0026#34;: \u0026#34;yesterday\u0026#34; }, \u0026#34;result_data\u0026#34;: { \u0026#34;AAPL\u0026#34;: [ { \u0026#34;adj_close\u0026#34;: 275.03, \u0026#34;close\u0026#34;: 100.03, \u0026#34;date\u0026#34;: \u0026#34;2000-04-23\u0026#34;, \u0026#34;high\u0026#34;: 100.75, \u0026#34;low\u0026#34;: 100.87, \u0026#34;open\u0026#34;: 100.87, \u0026#34;volume\u0026#34;: 33 } ] } } $ http :8000/mock/stock/closing HTTP/1.1 200 OK Connection: keep-alive Content-Length: 185 Content-Type: application/json; charset=utf-8 Date: Mon, 23 May 2022 05:20:52 GMT Server: kong/2.8.1.0-enterprise-edition X-Kong-Mocking-Plugin: true X-Kong-Response-Latency: 0 { \u0026#34;meta_data\u0026#34;: { \u0026#34;api_name\u0026#34;: \u0026#34;closing_stock_price_v1\u0026#34; }, \u0026#34;result_data\u0026#34;: { \u0026#34;AAPL\u0026#34;: [ { \u0026#34;adj_close\u0026#34;: 275.03, \u0026#34;close\u0026#34;: 100.03, \u0026#34;date\u0026#34;: \u0026#34;2000-06-23\u0026#34;, \u0026#34;high\u0026#34;: 100.75, \u0026#34;low\u0026#34;: 100.87, \u0026#34;open\u0026#34;: 100.87, \u0026#34;volume\u0026#34;: 33 } ] } }   ","permalink":"https://wenhan.blog/post/20220517_using-kong-mocking-plugin/","summary":"紹介 Mockingプラグインは、開発中のAPIに対するテストを行うために、模擬のエンドポイントを提供するものです。Open API（OAS）仕様に基づいて、APIに送信されたリクエストに模擬応答をレスポンスします。また、テストの柔軟性のため、Mockingプラグインを簡単に有効または無効にすることができます。 https://docs.konghq.com/hub/kong-inc/mocking/\n注意点として、このプラグインは、200、201と204の応答をレスポンスすることができます。正常動作をテストするために開発されたもので、200番台以外のコードをを返すことができません。\n今回は、db-less環境のKong-gatewayをDocker containerで構築し、その上に Mocking Pluginを試してみます。\nKong-gatewayをデプロイする前に db-lessの環境のため、Admin APIやGUIで設定を変更することはできません（保存先はないから）。そのため、起動時にDeclarative Configurationという名の設定ファイルを事前にコンテナーに食わせる必要があります。\nhttps://docs.konghq.com/gateway/2.8.x/reference/db-less-and-declarative-config/#main\n今回の目標は Mockingを試したいので、 Mockingの設定内容も事前にこのファイルに記述していきます。Mocking Pluginの紹介ページで書いたように、設定自体はとてもシンプルです。ただ、肝心なのはapi_specificationとapi_specification_filenameの二つです。この二つのパラメータは、模擬のエンドポイントのスペック内容を定義するために使われますが、違いは以下の通りです。\n api_specification_filename: スペック内容が保存されたファイル名を設定する。 api_specification: スペック内容をそのままパラメータに設定する。  api_specification_filenameは、設定するファイルを事前にDev Portalにアップロードしてから、パスなしで設定してください。設定の例はここに書いています。しかし、今回はdb-lessの環境を構築するため、そもそもdev protalが利用できないし、api_specification_filenameを利用できません。\nそのため、api_specificationにサンプルのAPIのスペック内容をそのまま設定し、コンテナーを立ち上げます。スペックのサンプルは、Mocking Pluginの紹介ページにあります。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20  _format_version:\u0026#34;1.1\u0026#34;_transform:trueservices:- host:mockbin.orgname:example_serviceport:80protocol:httproutes:- name:example_routepaths:- /mockstrip_path:trueplugins:- name:mockingconfig:api_specification:\u0026#39;{\u0026#34;swagger\u0026#34;:\u0026#34;2.0\u0026#34;,\u0026#34;info\u0026#34;:{\u0026#34;title\u0026#34;:\u0026#34;Stock API\u0026#34;,\u0026#34;description\u0026#34;:\u0026#34;Stock Information Service\u0026#34;,\u0026#34;version\u0026#34;:\u0026#34;0.1\u0026#34;},\u0026#34;host\u0026#34;:\u0026#34;127.0.0.1:8000\u0026#34;,\u0026#34;basePath\u0026#34;:\u0026#34;/\u0026#34;,\u0026#34;schemes\u0026#34;:[\u0026#34;http\u0026#34;,\u0026#34;https\u0026#34;],\u0026#34;consumes\u0026#34;:[\u0026#34;application/json\u0026#34;],\u0026#34;produces\u0026#34;:[\u0026#34;application/json\u0026#34;],\u0026#34;paths\u0026#34;:{\u0026#34;/stock/historical\u0026#34;:{\u0026#34;get\u0026#34;:{\u0026#34;description\u0026#34;:\u0026#34;\u0026#34;,\u0026#34;operationId\u0026#34;:\u0026#34;GET /stock/historical\u0026#34;,\u0026#34;produces\u0026#34;:[\u0026#34;application/json\u0026#34;],\u0026#34;tags\u0026#34;:[\u0026#34;Production\u0026#34;],\u0026#34;parameters\u0026#34;:[{\u0026#34;required\u0026#34;:true,\u0026#34;in\u0026#34;:\u0026#34;query\u0026#34;,\u0026#34;name\u0026#34;:\u0026#34;tickers\u0026#34;,\u0026#34;type\u0026#34;:\u0026#34;string\u0026#34;}],\u0026#34;responses\u0026#34;:{\u0026#34;200\u0026#34;:{\u0026#34;description\u0026#34;:\u0026#34;Status 200\u0026#34;,\u0026#34;examples\u0026#34;:{\u0026#34;application/json\u0026#34;:{\u0026#34;meta_data\u0026#34;:{\u0026#34;api_name\u0026#34;:\u0026#34;historical_stock_price_v2\u0026#34;,\u0026#34;num_total_data_points\u0026#34;:1,\u0026#34;credit_cost\u0026#34;:10,\u0026#34;start_date\u0026#34;:\u0026#34;yesterday\u0026#34;,\u0026#34;end_date\u0026#34;:\u0026#34;yesterday\u0026#34;},\u0026#34;result_data\u0026#34;:{\u0026#34;AAPL\u0026#34;:[{\u0026#34;date\u0026#34;:\u0026#34;2000-04-23\u0026#34;,\u0026#34;volume\u0026#34;:33,\u0026#34;high\u0026#34;:100.75,\u0026#34;low\u0026#34;:100.87,\u0026#34;adj_close\u0026#34;:275.03,\u0026#34;close\u0026#34;:100.03,\u0026#34;open\u0026#34;:100.87}]}}}}}}},\u0026#34;/stock/closing\u0026#34;:{\u0026#34;get\u0026#34;:{\u0026#34;description\u0026#34;:\u0026#34;\u0026#34;,\u0026#34;operationId\u0026#34;:\u0026#34;GET /stock/closing\u0026#34;,\u0026#34;produces\u0026#34;:[\u0026#34;application/json\u0026#34;],\u0026#34;tags\u0026#34;:[\u0026#34;Beta\u0026#34;],\u0026#34;parameters\u0026#34;:[{\u0026#34;required\u0026#34;:true,\u0026#34;in\u0026#34;:\u0026#34;query\u0026#34;,\u0026#34;name\u0026#34;:\u0026#34;tickers\u0026#34;,\u0026#34;type\u0026#34;:\u0026#34;string\u0026#34;}],\u0026#34;responses\u0026#34;:{\u0026#34;200\u0026#34;:{\u0026#34;description\u0026#34;:\u0026#34;Status 200\u0026#34;,\u0026#34;examples\u0026#34;:{\u0026#34;application/json\u0026#34;:{\u0026#34;meta_data\u0026#34;:{\u0026#34;api_name\u0026#34;:\u0026#34;closing_stock_price_v1\u0026#34;},\u0026#34;result_data\u0026#34;:{\u0026#34;AAPL\u0026#34;:[{\u0026#34;date\u0026#34;:\u0026#34;2000-06-23\u0026#34;,\u0026#34;volume\u0026#34;:33,\u0026#34;high\u0026#34;:100.75,\u0026#34;low\u0026#34;:100.87,\u0026#34;adj_close\u0026#34;:275.03,\u0026#34;close\u0026#34;:100.03,\u0026#34;open\u0026#34;:100.87}]}}}}}}}}}\u0026#39;max_delay_time:1min_delay_time:0.001random_delay:false  db-lessのKong-gateway環境を構築 ライセンスを環境変数に設定した後、以下のコマンドでコンテナーを立ち上げます。KONG_DATABASEがoffになっているところと、KONG_DECLARATIVE_CONFIGに設定ファイルのパスが書かれているところが重要です。設定ファイルは、-vでローカルからMountされています。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25  $ docker run -d --name kong-dbless \\  --network=kong-net \\  -v \u0026#34;$(pwd):/kong/declarative/\u0026#34; \\  -e \u0026#34;KONG_DATABASE=off\u0026#34; \\  -e \u0026#34;KONG_DECLARATIVE_CONFIG=/kong/declarative/kong.","title":"Kongの Mocking Plugingを使ってみる"},{"content":"Access k3s cluster from outside When you have created a k3s cluster with the default settings, it only can be access inside the node where you\u0026rsquo;re using. Bring the kubeconfig file /etc/rancher/k3s/k3s.yaml outside the node and import it to another host, you will get below issue when trying to access.\n1 2  ❯ kubectl get node Unable to connect to the server: x509: certificate is valid for 10.0.140.68, 10.43.0.1, 127.0.0.1, not xxx.xxx.xxx.xxx   To make it possible for access k3s cluster outside the node, you can use the below parameter when create the cluster.\n1  --tls-san value (listener) Add additional hostname or IP as a Subject Alternative Name in the TLS cert   for details of this parameter refers to https://rancher.com/docs/k3s/latest/en/installation/install-options/#registration-options-for-the-k3s-server\nSo the install command should like this.\n1 2  curl -sfL https://get.k3s.io | INSTALL_K3S_EXEC=\u0026#34;--tls-san \u0026lt;your node public ip address\u0026gt;\u0026#34; sh -   Then you can copy the contents from /etc/rancher/k3s/k3s.yaml and save it in your local machine. Modifying the server\u0026rsquo;s IP address from 127.0.0.1 to the real address you want to access with.\nTry to fetch the cluster\u0026rsquo;s info and all went well this time.\n1 2 3  ❯ kubectl get node NAME STATUS ROLES AGE VERSION wenhan-dev Ready control-plane,master 61s v1.22.7+k3s1   ","permalink":"https://wenhan.blog/post/access-k3s-from-outside/","summary":"Access k3s cluster from outside When you have created a k3s cluster with the default settings, it only can be access inside the node where you\u0026rsquo;re using. Bring the kubeconfig file /etc/rancher/k3s/k3s.yaml outside the node and import it to another host, you will get below issue when trying to access.\n1 2  ❯ kubectl get node Unable to connect to the server: x509: certificate is valid for 10.0.140.68, 10.43.0.1, 127.","title":"Access K3s From Outside"},{"content":"rkeクラスタのcidrを弄ったら、以下のようなエラーが出てクラスタの作成が失敗した。\n1  {\u0026#34;log\u0026#34;:\u0026#34;F1203 01:36:22.168496 1 node_ipam_controller.go:115] Controller: Invalid --cluster-cidr, mask size of cluster CIDR must be less than or equal to --node-cidr-mask-size configured for CIDR family\\n\u0026#34;,\u0026#34;stream\u0026#34;:\u0026#34;stderr\u0026#34;,\u0026#34;time\u0026#34;:\u0026#34;2021-12-03T01:36:22.16859524Z\u0026#34;}   cluster.yamlの関連部分は以下のようになっています。\n1 2 3 4 5 6  services:kube-controller:cluster_cidr:10.42.0.0/25service_cluster_ip_range:10.43.0.0/25kube-api:service_cluster_ip_range:10.43.0.0/25  問題になる場所はCIDRのマスクサイズです。このマスクサイズは--node-cidr-mask-sizeで設定され、クラスタのCIDRのマスクサイズより大きく（IP範囲が少なく）なる必要があります。デフォルトは24のため、上記の設定の25より小さい（IP範囲が溢れた）のでエラーとなりました。\nこの--node-cidr-mask-sizeは、以下のようにextra_argsで変更することができます。\n1 2 3 4 5 6  services:kube-controller:cluster_cidr:10.42.0.0/25service_cluster_ip_range:10.43.0.0/25extra_args:node-cidr-mask-size:25  25, 26のような数字に設定したら、クラスタの作成ができるようになります。\nこのマスクサイズとノードあたりで利用できるPod数の関係として、Pod の追加 / 削除などを考慮し、確保したIPアドレスの数の半分ぐらいが実際に作成できるPodの数となります。\n例えば、cluster_cidrのマスクが25なので、クラスタレベルのIPアドレス数は128個です。\n1ノードの場合、--node-cidr-mask-sizeを同じく25を設定したら128個のIPアドレスが確保され、33～64のPodが作成できます。\n3ノードの場合、ノードあたりのIP数は42個までしか使えなくて、--node-cidr-mask-sizeを27に設定しなければいけません。そうすると、Pod数は 9～16になります。 システムレベルのPod（corednsなど）も10個程度あるので、28にしたら業務Pod用のIPアドレスが足りなくなります。\nまた、上記にあわせてmax-podsの設定も必要です。\n1 2 3  kubelet:extra_args:max-pods:40  参考まで https://cloud.google.com/kubernetes-engine/docs/how-to/flexible-pod-cidr\n","permalink":"https://wenhan.blog/post/20220126_--node-cidr-mask-size_error/","summary":"rkeクラスタのcidrを弄ったら、以下のようなエラーが出てクラスタの作成が失敗した。\n1  {\u0026#34;log\u0026#34;:\u0026#34;F1203 01:36:22.168496 1 node_ipam_controller.go:115] Controller: Invalid --cluster-cidr, mask size of cluster CIDR must be less than or equal to --node-cidr-mask-size configured for CIDR family\\n\u0026#34;,\u0026#34;stream\u0026#34;:\u0026#34;stderr\u0026#34;,\u0026#34;time\u0026#34;:\u0026#34;2021-12-03T01:36:22.16859524Z\u0026#34;}   cluster.yamlの関連部分は以下のようになっています。\n1 2 3 4 5 6  services:kube-controller:cluster_cidr:10.42.0.0/25service_cluster_ip_range:10.43.0.0/25kube-api:service_cluster_ip_range:10.43.0.0/25  問題になる場所はCIDRのマスクサイズです。このマスクサイズは--node-cidr-mask-sizeで設定され、クラスタのCIDRのマスクサイズより大きく（IP範囲が少なく）なる必要があります。デフォルトは24のため、上記の設定の25より小さい（IP範囲が溢れた）のでエラーとなりました。\nこの--node-cidr-mask-sizeは、以下のようにextra_argsで変更することができます。\n1 2 3 4 5 6  services:kube-controller:cluster_cidr:10.42.0.0/25service_cluster_ip_range:10.43.0.0/25extra_args:node-cidr-mask-size:25  25, 26のような数字に設定したら、クラスタの作成ができるようになります。\nこのマスクサイズとノードあたりで利用できるPod数の関係として、Pod の追加 / 削除などを考慮し、確保したIPアドレスの数の半分ぐらいが実際に作成できるPodの数となります。\n例えば、cluster_cidrのマスクが25なので、クラスタレベルのIPアドレス数は128個です。\n1ノードの場合、--node-cidr-mask-sizeを同じく25を設定したら128個のIPアドレスが確保され、33～64のPodが作成できます。\n3ノードの場合、ノードあたりのIP数は42個までしか使えなくて、--node-cidr-mask-sizeを27に設定しなければいけません。そうすると、Pod数は 9～16になります。 システムレベルのPod（corednsなど）も10個程度あるので、28にしたら業務Pod用のIPアドレスが足りなくなります。\nまた、上記にあわせてmax-podsの設定も必要です。\n1 2 3  kubelet:extra_args:max-pods:40  参考まで https://cloud.google.com/kubernetes-engine/docs/how-to/flexible-pod-cidr","title":"--node-cidr-mask-sizeエラーの原因と修正"},{"content":"この記事では、Single nodeのRancher server と二つのk3sクラスタ環境を構築し、 そRancherのContinuous Delivery機能で、二つのk3sクラスタをGitOpsで操作します。\nStep 1: Deploying a Rancher Server まずはrancherノードで、以下のdocker コマンドを実行しSingle nodeのRancher serverを構築します。\n1 2 3 4  sudo docker run -d --restart=unless-stopped \\ -p 80:80 -p 443:443 \\ --privileged \\ rancher/rancher:v2.5.10   Rancher serverが1分程度で立ち上がりますので、rancherノードのIPアドレスをブラウザで開いてRancher UIをアクセスしてください。\n今回の場合、Rancherは自己署名証明書を使用しているため、ブラウザに証明書の警告が表示されます。この警告はスキップしても問題ありません。 また、一部のブラウザでは、スキップボタンが表示されない場合があります。 この場合は、エラーページの任意の場所をクリックして、thisisunsafeと入力します。 これにより、ブラウザは警告をバイパスして証明書を受け入れるようになります。\n初回アクセスの時パスワードの初期設定が必要です。画面のガイドに従って設定してください。\nStep 2: Deploy k3s Kubernetes Cluster 次はk3sクラスタをデプロイします。手順はとても簡単で、以下のコマンドをk3s-1とk3s-2ノードで実行するだけです。\n後でアップグレードもやる予定なので、ここではあえてちょっと古いバージョンを指定しk3sのクラスタをデプロイします。\nk3s-1 1  sudo curl -sfL https://get.k3s.io | INSTALL_K3S_VERSION=v1.19.15+k3s2 sh -   k3s-2 1  sudo curl -sfL https://get.k3s.io | INSTALL_K3S_VERSION=v1.19.15+k3s2 sh -   デプロイが完了したら、以下のコマンドでクラスタを確認してください。\n1  sudo kubectl get node   Note: K3sはrootユーザの操作を想定しているのでsudo 権限が必要です。\nStep 3: Add k3s cluster to Rancher 次はk3sクラスタをRancher にImportします。 以下の手順で、k3s-1を登録してください。\n RancherのGlobal画面で、右側にあるAdd Cluster をクリックし、Other Clusterを選択してください。 Cluster Nameにクラスタの名前　k3s-1　を入力しCreateをクリックします。 curl...から始まっているコマンドをクリックし、k3s-1のノードで実行してください。  同じ手順で、k3s-2を登録してください。\nStep 4: Cluster Group ここから、Continuous Deliveryの操作に入ります。 Global画面のTools-\u0026gt;Continous Deliveryを選択してください。\nまずはCluster Groupを作成します。左側のCluster Groupメニューに入り、画面右側にあるCreateをクリックし、Cluster Group作成メニューに入ります。 今回はk3sを使ってデモするので、Nameのところをk3s-demoに入力します。\n次は重要なところです。Cluster Groupの所属は設定されたラベルを持つクラスタを選択するので、ここではAdd Ruleをクリックしてラベルを設定します。今回のDemoでは、K3sを使っているので、Cluster Selectorsのところに k3s=true のラベルを設定します。\n上記の設定が終わったらCreateをクリックしCluster Groupの作成が終了します。\n 次はimportされた2機のk3sクラスタをこのCluster Groupにアサインします。左側のClustersメニューに入り、まずはk3s-1のクラスタの右側にある︙をクリックし、Assign toを選択します。 Add/Set Labelをクリックし、さっき設定したラベルk3s=trueを設定し、Applyをクリックします。これでこのクラスタがさっき作成したクラスタグループに配属されました。 同じ手順でk3s-2のクラスタもCluster Groupに配属しましょう。Cluster Groups画面からClusters Readyが２になっていることが分かります。 Step 5: Git Repos このステップでは、参照するGit Repoの設定を行います。左側のGit Reposに入り、画面右側にあるCreateをクリックし、Git Repos作成画面に入ります。 名前を入力した後、各自のGithubのアカウントで https://github.com/rancher/fleet-examples をfolkし、Repository URL(e.g. https://github.com/xibuka/fleet-examples.git )に設定してください。 また、Pathsに今回利用するguest-bookの定義ファイルが置いてあった /single-cluster/manifests を設定して ください。 次はこのGit Reposをどこにデプロイするのかを設定します。Deploy Toのメニューから、さっき作成したCluster Groupを選択し、Createをクリックしてください。\nこれで、k3s-1とk3s-2にguest-bookのリソースがデプロイされます。  次に、deployment以下に新しいファイルnginx.yamlを追加しましょう。以下の内容で追加したら、Continuous Deliveryがこのファイルを検知し、Cluster Groupにデプロイします。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34  apiVersion:apps/v1kind:Deploymentmetadata:name:nginx-deploymentlabels:app:nginxspec:replicas:1selector:matchLabels:app:nginxtemplate:metadata:labels:app:nginxspec:containers:- name:nginximage:nginx:1.21ports:- containerPort:80---apiVersion:v1kind:Servicemetadata:name:nginx-nodeportspec:type:NodePortselector:app:nginxports:- port:80targetPort:80nodePort:32000  デプロイが完了したら、クラスタ側でPodやServiceなどを確認しましょう。 同じく、監視先のファイルに変更がある場合も、その修正内容は随時cluster groupにデプロイされます。 試しに、上記のYAMLの以下の２行を変更してみてください。\n1 2 3 4  ...image:nginx:1.20...nodePort:31000  修正内容が反映されました。\nStep 6: Deploy Application このステップでは、Git Repoの登録からRancherのアップリケーションをデプロイします。\nまずはRepository URLに、https://github.com/xibuka/core-bundles.gitを設定し、Add Pathで以下の二つのパスを追加します。\n /longhorn /longhorn-crd  Createをクリックしたら定義したアップリケーションがデプロイされます。\nStep 7: Upgrade このステップでは、Git Repoの登録から二つのk3sクラスタをアップグレードします。アップグレードを実現するために、以下のAutomated Upgrades機能を利用します。\nhttps://rancher.com/docs/k3s/latest/en/upgrades/automated/\nまずはこの機能に必要なリソースを準備します。以下のコマンドを一回ずつクリックし各クラスタで実行させてください。\n1 2  sudo kubectl apply -f https://github.com/rancher/system-upgrade-controller/releases/download/v0.6.2/system-upgrade-controller.yaml sudo kubectl apply -f https://github.com/rancher/system-upgrade-controller/releases/download/v0.6.2/system-upgrade-controller.yaml   次は https://github.com/xibuka/k3s-upgrade-plan をfolkし、Git reposを設定してください。Branch NameをMainにして、Createをクリックしてください。 / 以下が対象なのでpathの追加が必要ないです。 現在のk3sのバージョンは　1.21.5+k3s2　であることを確認します。 Git ReposにあるPlanのファイルを変更し、versionを1.22.2+k3s1に変更しcommitしてください。少し時間が立つと二つのk3sクラスタのバージョンは1.22.2にアップグレードされるはずです。\nFleetのDemoはここまでですが、まだまだ使える機能はたくさんありますので探してみてください。\n","permalink":"https://wenhan.blog/post/20211111_fleet-demo/","summary":"この記事では、Single nodeのRancher server と二つのk3sクラスタ環境を構築し、 そRancherのContinuous Delivery機能で、二つのk3sクラスタをGitOpsで操作します。\nStep 1: Deploying a Rancher Server まずはrancherノードで、以下のdocker コマンドを実行しSingle nodeのRancher serverを構築します。\n1 2 3 4  sudo docker run -d --restart=unless-stopped \\ -p 80:80 -p 443:443 \\ --privileged \\ rancher/rancher:v2.5.10   Rancher serverが1分程度で立ち上がりますので、rancherノードのIPアドレスをブラウザで開いてRancher UIをアクセスしてください。\n今回の場合、Rancherは自己署名証明書を使用しているため、ブラウザに証明書の警告が表示されます。この警告はスキップしても問題ありません。 また、一部のブラウザでは、スキップボタンが表示されない場合があります。 この場合は、エラーページの任意の場所をクリックして、thisisunsafeと入力します。 これにより、ブラウザは警告をバイパスして証明書を受け入れるようになります。\n初回アクセスの時パスワードの初期設定が必要です。画面のガイドに従って設定してください。\nStep 2: Deploy k3s Kubernetes Cluster 次はk3sクラスタをデプロイします。手順はとても簡単で、以下のコマンドをk3s-1とk3s-2ノードで実行するだけです。\n後でアップグレードもやる予定なので、ここではあえてちょっと古いバージョンを指定しk3sのクラスタをデプロイします。\nk3s-1 1  sudo curl -sfL https://get.k3s.io | INSTALL_K3S_VERSION=v1.19.15+k3s2 sh -   k3s-2 1  sudo curl -sfL https://get.","title":"RancherのContinuous Delivery機能で簡単GitOpsを実現できる"},{"content":"仕事の関係で、期限切れのkubernetesクラスタの証明書更新の手順を検証することになった。 今まで特にやったことないので記録しておきます。\nkubeadmのソースを変更しビルド  ビルド環境にgoとgitをインストールし、goの実行パスをPATHに追加\n  kubernetesのソースコードをダウンロード、今回はv1.18.18を利用\ngit clone -b v1.18.18 https://github.com/kubernetes/kubernetes\n  以下の通りでファイルを修正するし、証明書の有効期限を10分にする\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38  diff --git a/cmd/kubeadm/app/constants/constants.go b/cmd/kubeadm/app/constants/constants.go index b56891ca908..eed934280e7 100644 --- a/cmd/kubeadm/app/constants/constants.go +++ b/cmd/kubeadm/app/constants/constants.go @@ -46,7 +46,8 @@ const ( TempDirForKubeadm = \u0026#34;tmp\u0026#34; // CertificateValidity defines the validity for all the signed certificates generated by kubeadm - CertificateValidity = time.Hour * 24 * 365 + //CertificateValidity = time.Hour * 24 * 365 + CertificateValidity = time.Second * 600 // CACertAndKeyBaseName defines certificate authority base name CACertAndKeyBaseName = \u0026#34;ca\u0026#34; diff --git a/staging/src/k8s.io/client-go/util/cert/cert.go b/staging/src/k8s.io/client-go/util/cert/cert.go index 9fd097af5e3..64e1dd90f43 100644 --- a/staging/src/k8s.io/client-go/util/cert/cert.go +++ b/staging/src/k8s.io/client-go/util/cert/cert.go @@ -35,7 +35,8 @@ import ( \u0026#34;k8s.io/client-go/util/keyutil\u0026#34; ) -const duration365d = time.Hour * 24 * 365 +//const duration365d = time.Hour * 24 * 365 +const duration365d = time.Second * 600 // Config contains the basic fields required for creating a certificate type Config struct { @@ -93,7 +94,8 @@ func GenerateSelfSignedCertKey(host string, alternateIPs []net.IP, alternateDNS // Certs/keys not existing in that directory are created. func GenerateSelfSignedCertKeyWithFixtures(host string, alternateIPs []net.IP, alternateDNS []string, fixtureDirectory string) ([]byte, []byte, error) { validFrom := time.Now().Add(-time.Hour) // valid an hour earlier to avoid flakes due to clock skew - maxAge := time.Hour * 24 * 365 // one year self-signed certs + //maxAge := time.Hour * 24 * 365 // one year self-signed certs + maxAge := time.Second * 600 // one year self-signed certs baseName := fmt.Sprintf(\u0026#34;%s_%s_%s\u0026#34;, host, strings.Join(ipsToStrings(alternateIPs), \u0026#34;-\u0026#34;), strings.Join(alternateDNS, \u0026#34;-\u0026#34;)) certFixturePath := path.Join(fixtureDirectory, baseName+\u0026#34;.crt\u0026#34;)     以下のコマンドでkubeadmだけをビルドします。\n make WHAT=cmd/kubeadm GOFLAGS=-v\nビルドが完了したら、_outputフォルダが生成され、さらにそのbinの下にkubeadmのバイナリが格納されます。 このバイナリを利用しクラスタをデプロイすれば、有効期間が１０分になります。\n証明書の確認＆更新   3 nodeのクラスタを構築し、証明書の有効期間が残り４分になっている\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26  root@wenhan-adm-cp:~# ./kubeadm alpha certs check-expiration [check-expiration] Reading configuration from the cluster... [check-expiration] FYI: You can look at this config file with \u0026#39;kubectl -n kube-system get cm kubeadm-config -oyaml\u0026#39; CERTIFICATE EXPIRES RESIDUAL TIME CERTIFICATE AUTHORITY EXTERNALLY MANAGED admin.conf Jun 24, 2021 06:01 UTC 4m no apiserver Jun 24, 2021 06:01 UTC 4m ca no apiserver-etcd-client Jun 24, 2021 06:01 UTC 4m etcd-ca no apiserver-kubelet-client Jun 24, 2021 06:01 UTC 4m ca no controller-manager.conf Jun 24, 2021 06:01 UTC 4m no etcd-healthcheck-client Jun 24, 2021 06:01 UTC 4m etcd-ca no etcd-peer Jun 24, 2021 06:01 UTC 4m etcd-ca no etcd-server Jun 24, 2021 06:01 UTC 4m etcd-ca no front-proxy-client Jun 24, 2021 06:01 UTC 4m front-proxy-ca no scheduler.conf Jun 24, 2021 06:01 UTC 4m no CERTIFICATE AUTHORITY EXPIRES RESIDUAL TIME EXTERNALLY MANAGED ca Jun 24, 2021 07:31 UTC 1h no etcd-ca Jun 24, 2021 07:31 UTC 1h no front-proxy-ca Jun 24, 2021 07:31 UTC 1h no root@wenhan-adm-cp:~# kubectl get node NAME STATUS ROLES AGE VERSION wenhan-adm-cp Ready master 4m35s v1.18.18 wenhan-adm-wk1 Ready \u0026lt;none\u0026gt; 3m32s v1.18.18 wenhan-adm-wk2 Ready \u0026lt;none\u0026gt; 3m28s v1.18.18     期限が切れた後、kubectl get nodeが失敗になりました。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27  期限が切れた後、kubectl get nodeが失敗になりました。 root@wenhan-adm-cp:~# ./kubeadm alpha certs check-expiration [check-expiration] Reading configuration from the cluster... [check-expiration] FYI: You can look at this config file with \u0026#39;kubectl -n kube-system get cm kubeadm-config -oyaml\u0026#39; [check-expiration] Error reading configuration from the Cluster. Falling back to default configuration W0624 06:01:22.231290 13746 configset.go:202] WARNING: kubeadm cannot validate component configs for API groups [kubelet.config.k8s.io kubeproxy.config.k8s.io] CERTIFICATE EXPIRES RESIDUAL TIME CERTIFICATE AUTHORITY EXTERNALLY MANAGED admin.conf Jun 24, 2021 06:01 UTC \u0026lt;invalid\u0026gt; no apiserver Jun 24, 2021 06:01 UTC \u0026lt;invalid\u0026gt; ca no apiserver-etcd-client Jun 24, 2021 06:01 UTC \u0026lt;invalid\u0026gt; etcd-ca no apiserver-kubelet-client Jun 24, 2021 06:01 UTC \u0026lt;invalid\u0026gt; ca no controller-manager.conf Jun 24, 2021 06:01 UTC \u0026lt;invalid\u0026gt; no etcd-healthcheck-client Jun 24, 2021 06:01 UTC \u0026lt;invalid\u0026gt; etcd-ca no etcd-peer Jun 24, 2021 06:01 UTC \u0026lt;invalid\u0026gt; etcd-ca no etcd-server Jun 24, 2021 06:01 UTC \u0026lt;invalid\u0026gt; etcd-ca no front-proxy-client Jun 24, 2021 06:01 UTC \u0026lt;invalid\u0026gt; front-proxy-ca no scheduler.conf Jun 24, 2021 06:01 UTC \u0026lt;invalid\u0026gt; no CERTIFICATE AUTHORITY EXPIRES RESIDUAL TIME EXTERNALLY MANAGED ca Jun 24, 2021 07:31 UTC 1h no etcd-ca Jun 24, 2021 07:31 UTC 1h no front-proxy-ca Jun 24, 2021 07:31 UTC 1h no root@wenhan-adm-cp:~# kubectl get node Unable to connect to the server: x509: certificate has expired or is not yet valid     証明書を更新し、新しい有効期限を確認\n1 2 3 4 5 6 7 8 9 10 11 12  root@wenhan-adm-cp:~# ./kubeadm alpha certs renew all --config=kubeadm.yaml W0624 06:04:00.447612 2860 configset.go:202] WARNING: kubeadm cannot validate component configs for API groups [kubelet.config.k8s.io kubeproxy.config.k8s.io] certificate embedded in the kubeconfig file for the admin to use and for kubeadm itself renewed certificate for serving the Kubernetes API renewed certificate the apiserver uses to access etcd renewed certificate for the API server to connect to kubelet renewed certificate embedded in the kubeconfig file for the controller manager to use renewed certificate for liveness probes to healthcheck etcd renewed certificate for etcd nodes to communicate with each other renewed certificate for serving etcd renewed certificate for the front proxy client renewed certificate embedded in the kubeconfig file for the scheduler manager to use renewed   証明書の更新が終わったら、各証明書の期限が更新された\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22  root@wenhan-adm-cp:~# ./kubeadm alpha certs check-expiration [check-expiration] Reading configuration from the cluster... [check-expiration] FYI: You can look at this config file with \u0026#39;kubectl -n kube-system get cm kubeadm-config -oyaml\u0026#39; [check-expiration] Error reading configuration from the Cluster. Falling back to default configuration W0624 06:04:21.176808 3230 configset.go:202] WARNING: kubeadm cannot validate component configs for API groups [kubelet.config.k8s.io kubeproxy.config.k8s.io] CERTIFICATE EXPIRES RESIDUAL TIME CERTIFICATE AUTHORITY EXTERNALLY MANAGED admin.conf Jun 24, 2021 06:14 UTC 9m no apiserver Jun 24, 2021 06:14 UTC 9m ca no apiserver-etcd-client Jun 24, 2021 06:14 UTC 9m etcd-ca no apiserver-kubelet-client Jun 24, 2021 06:14 UTC 9m ca no controller-manager.conf Jun 24, 2021 06:14 UTC 9m no etcd-healthcheck-client Jun 24, 2021 06:14 UTC 9m etcd-ca no etcd-peer Jun 24, 2021 06:14 UTC 9m etcd-ca no etcd-server Jun 24, 2021 06:14 UTC 9m etcd-ca no front-proxy-client Jun 24, 2021 06:14 UTC 9m front-proxy-ca no scheduler.conf Jun 24, 2021 06:14 UTC 9m no CERTIFICATE AUTHORITY EXPIRES RESIDUAL TIME EXTERNALLY MANAGED ca Jun 24, 2021 07:31 UTC 1h no etcd-ca Jun 24, 2021 07:31 UTC 1h no front-proxy-ca Jun 24, 2021 07:31 UTC 1h no     証明書が更新されたら、Control planeの再起動を行う\n1  root@wenhan-adm-cp:~# reboot   これで証明書の更新が完了しました。\n再起動後、新しい認証ファイルでクラスタにアクセスすることができました。\n1 2 3 4 5 6 7 8  root@wenhan-adm-cp:~# cp -i /etc/kubernetes/admin.conf $HOME/.kube/config cp: overwrite \u0026#39;/root/.kube/config\u0026#39;? yes root@wenhan-adm-cp:~# kubectl get node NAME STATUS ROLES AGE VERSION wenhan-adm-cp Ready master 14m v1.18.18 wenhan-adm-wk1 Ready \u0026lt;none\u0026gt; 13m v1.18.18 wenhan-adm-wk2 Ready \u0026lt;none\u0026gt; 13m v1.18.18     ","permalink":"https://wenhan.blog/post/modify-kubeadm-cert-expired-period-test/","summary":"仕事の関係で、期限切れのkubernetesクラスタの証明書更新の手順を検証することになった。 今まで特にやったことないので記録しておきます。\nkubeadmのソースを変更しビルド  ビルド環境にgoとgitをインストールし、goの実行パスをPATHに追加\n  kubernetesのソースコードをダウンロード、今回はv1.18.18を利用\ngit clone -b v1.18.18 https://github.com/kubernetes/kubernetes\n  以下の通りでファイルを修正するし、証明書の有効期限を10分にする\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38  diff --git a/cmd/kubeadm/app/constants/constants.go b/cmd/kubeadm/app/constants/constants.go index b56891ca908..eed934280e7 100644 --- a/cmd/kubeadm/app/constants/constants.go +++ b/cmd/kubeadm/app/constants/constants.go @@ -46,7 +46,8 @@ const ( TempDirForKubeadm = \u0026#34;tmp\u0026#34; // CertificateValidity defines the validity for all the signed certificates generated by kubeadm - CertificateValidity = time.","title":"kubernetes証明書期限切れのテスト"},{"content":"紹介 Rancherはrancher-server とrancher-agent、そして一つ以上のkubernetes clusterによって構成されている。この中、rancher-agentは管理されたkubernetesに実行され、rancher-serverと通信し、クラスタの情報を送信する。\nrancher-serverはkubernetesを管理するためのWebUIとAPIを提供している。rancher-serverはHTTPSのみアクセスできる。\nインストール シングルノード シングルノードの構築は以下二つの方法があります。\n dockerで直接rancher-serverを実行 rkeで一つのノードに全てのroleを有効  rkeの方法は後でも出てくるので割愛、ここではdockerの方法を示す。\nrancher-serverを実行したいノードで、下記のコマンドを入力する。\n1 2 3  docker run -d --restart=unless-stopped \\  -p 80:80 -p 443:443 \\  rancher/rancher:latest   これでシングルノードのrancher-serverを起動した。http://\u0026lt;IP Address\u0026gt;でアクセスできる。\nマルチノード rkeを使ってHA環境のrancher-serverを構築する。\nrke(rancher k8s engine)はkubernetesを構築するためのコマンドで、環境を用意すればコマンド一つでクラスターを構築できる。\nマシンの用意 今回はmultipass でマシンを準備する。以下のコマンドで６台の仮想マシンを作成する。\n1 2 3 4 5 6  multipass launch -c 2 -m 4096M -d 20G --cloud-init=./cloud-init.yaml -n kmaster1 multipass launch -c 2 -m 4096M -d 20G --cloud-init=./cloud-init.yaml -n kmaster2 multipass launch -c 2 -m 4096M -d 20G --cloud-init=./cloud-init.yaml -n kmaster3 multipass launch -c 2 -m 4096M -d 20G --cloud-init=./cloud-init.yaml -n kworker1 multipass launch -c 2 -m 4096M -d 20G --cloud-init=./cloud-init.yaml -n kworker2 multipass launch -c 2 -m 4096M -d 20G --cloud-init=./cloud-init.yaml -n kworker3   コマンドにあったcloud-init.yamlファイルは、マシン立ち上がった後の後処理を実行する。今回の場合は以下の後処理を実行した。\n  dockerのインストール\n  ホストマシンsshキーの登録\n  Ubuntuユーザをdockerグループに登録\n  必要カーネルモジュールのロード\n  swap領域の停止\n  詳細の内容は以下に示す。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42  #cloud-config packages: - docker.io ssh_authorized_keys: - \u0026lt;rsa public key\u0026gt; runcmd: - usermod -aG docker ubuntu - modprobe br_netfilter - modprobe ip6_udp_tunnel - modprobe ip_set - modprobe ip_set_hash_ip - modprobe ip_set_hash_net - modprobe iptable_filter - modprobe iptable_nat - modprobe iptable_mangle - modprobe iptable_raw - modprobe nf_conntrack_netlink - modprobe nf_conntrack - modprobe nf_conntrack_ipv4 - modprobe nf_defrag_ipv4 - modprobe nf_nat - modprobe nf_nat_ipv4 - modprobe nf_nat_masquerade_ipv4 - modprobe nfnetlink - modprobe udp_tunnel - modprobe veth - modprobe vxlan - modprobe x_tables - modprobe xt_addrtype - modprobe xt_conntrack - modprobe xt_comment - modprobe xt_mark - modprobe xt_multiport - modprobe xt_nat - modprobe xt_recent - modprobe xt_set - modprobe xt_statistic - modprobe xt_tcpudp - swapoff -a   最終的に以下ようにな６台のマシンができた\n1 2 3 4 5 6 7 8  $ multipass list Name State IPv4 Image kmaster1 Running 10.131.158.97 Not Available kmaster2 Running 10.131.158.194 Not Available kmaster3 Running 10.131.158.121 Not Available kworker1 Running 10.131.158.133 Not Available kworker2 Running 10.131.158.247 Not Available kworker3 Running 10.131.158.166 Not Available   rkeでKubernetes環境作成 rkeで説明した通り、バイナリをダウンロードし実行権限を追加する。\n上記の６ノードの情報とそれぞれの役割を設定する。他にもいろいろ設定できるが、今回は割愛\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33  nodes:- address:10.131.158.97user:ubunturole:[controlplane, worker, etcd]- address:10.131.158.194user:ubunturole:[controlplane, worker, etcd]- address:10.131.158.121user:ubunturole:[controlplane, worker, etcd]- address:10.131.158.133user:ubunturole:[worker]- address:10.131.158.247user:ubunturole:[worker]- address:10.131.158.166user:ubunturole:[worker]system-images:kubernetes:rancher/hyperkube:v1.18.2services:etcd:snapshot:truecreation:6hretention:24ingress:provider:nginxoptions:use-forwarded-headers:\u0026#39;true\u0026#39;  rkeコマンドにこの`Yamlファイルをパラメータにして実行すると、Kubernetes環境が作成できる。\n1  rke_linux-amd64 up --config ./rancher_cluster.yaml   無事クラスタが作成された後、元のYamlファイル以外に、新しいファイルが二つ生成されます。\n1 2 3 4 5  $ ll total 132K -rw-r----- 1 wshi wshi 5.3K Jun 8 17:04 kube_config_rancher_cluster.yaml -rw-r----- 1 wshi wshi 119K Jun 8 17:07 rancher_cluster.rkestate -rw-rw-r-- 1 wshi wshi 748 Jun 8 16:53 rancher_cluster.yaml   この中のkube_config_rancher_cluster.yamlはクラスタにアクセスするための設定ファイルのため、kubectlに読み込まれるように、~/kube/configにコピーする。これによってクラスタにアクセスができるようになった。\n1 2 3 4 5 6 7 8  $ kubectl get node NAME STATUS ROLES AGE VERSION 10.131.158.121 Ready controlplane,etcd,worker 21h v1.17.4 10.131.158.133 Ready worker 21h v1.17.4 10.131.158.166 Ready worker 21h v1.17.4 10.131.158.194 Ready controlplane,etcd,worker 21h v1.17.4 10.131.158.247 Ready worker 21h v1.17.4 10.131.158.97 Ready controlplane,etcd,worker 21h v1.17.4   kubenetes環境でRancherをインストール rancherのドキュメント に従ってインストールします。ここではインストールの手順だけ抽出し、詳しい設定はドキュメントを参照してください。\n  helmのインストール\nhelmのホームページを参考にしてhelmをインストールする\n1  sudo snap install helm --classic     helmにrancherのリポジトリを追加\n今回はstableを選択する\n1  helm repo add rancher-stable https://releases.rancher.com/server-charts/stable     rancherインストールのためnamespaceを追加\nnamespaceの名前は必ずcattle-systemにする\n1  kubectl create namespace cattle-system     cert-managerをインストール\n他にも証明書作成の方法はありますが、今回はrancherに生成してもらう\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17  # Install the CustomResourceDefinition resources separately kubectl apply --validate=false -f https://github.com/jetstack/cert-manager/releases/download/v0.15.0/cert-manager.crds.yaml # Create the namespace for cert-manager kubectl create namespace cert-manager # Add the Jetstack Helm repository helm repo add jetstack https://charts.jetstack.io # Update your local Helm chart repository cache helm repo update # Install the cert-manager Helm chart helm install \\ cert-manager jetstack/cert-manager \\ --namespace cert-manager \\ --version v0.15.0   cert-managerの状態を確認\n1 2 3 4 5  $ kubectl get pods --namespace cert-manager NAME READY STATUS RESTARTS AGE cert-manager-766d5c494b-9cmcq 1/1 Running 0 15s cert-manager-cainjector-6649bbb695-cfmxq 1/1 Running 0 15s cert-manager-webhook-68d464c8b-5bmjt 1/1 Running 0 15s     rancher-serverをインストール\nRancher-generated certificatesを利用して、rancher-serverをインストール\n1 2 3  helm install rancher rancher-stable/rancher \\ --namespace cattle-system \\ --set hostname=rancher.my.org   rancher-serverの状態を確認\n1 2 3 4 5  $ kubectl get pod -n cattle-system -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES rancher-756b996499-fjnt9 1/1 Running 0 35m 10.42.0.4 10.131.158.247 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; rancher-756b996499-rkn8h 1/1 Running 0 35m 10.42.2.4 10.131.158.121 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; rancher-756b996499-wmczg 1/1 Running 0 35m 10.42.5.4 10.131.158.97 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt;   それぞれのノード上にPodがRunning状態であり、インストールは成功した。\n  ","permalink":"https://wenhan.blog/post/rancher-learning-from-zero/","summary":"紹介 Rancherはrancher-server とrancher-agent、そして一つ以上のkubernetes clusterによって構成されている。この中、rancher-agentは管理されたkubernetesに実行され、rancher-serverと通信し、クラスタの情報を送信する。\nrancher-serverはkubernetesを管理するためのWebUIとAPIを提供している。rancher-serverはHTTPSのみアクセスできる。\nインストール シングルノード シングルノードの構築は以下二つの方法があります。\n dockerで直接rancher-serverを実行 rkeで一つのノードに全てのroleを有効  rkeの方法は後でも出てくるので割愛、ここではdockerの方法を示す。\nrancher-serverを実行したいノードで、下記のコマンドを入力する。\n1 2 3  docker run -d --restart=unless-stopped \\  -p 80:80 -p 443:443 \\  rancher/rancher:latest   これでシングルノードのrancher-serverを起動した。http://\u0026lt;IP Address\u0026gt;でアクセスできる。\nマルチノード rkeを使ってHA環境のrancher-serverを構築する。\nrke(rancher k8s engine)はkubernetesを構築するためのコマンドで、環境を用意すればコマンド一つでクラスターを構築できる。\nマシンの用意 今回はmultipass でマシンを準備する。以下のコマンドで６台の仮想マシンを作成する。\n1 2 3 4 5 6  multipass launch -c 2 -m 4096M -d 20G --cloud-init=./cloud-init.yaml -n kmaster1 multipass launch -c 2 -m 4096M -d 20G --cloud-init=./cloud-init.yaml -n kmaster2 multipass launch -c 2 -m 4096M -d 20G --cloud-init=.","title":"Rancher ゼロから勉強 "},{"content":"Background Sometimes you want to build a reproducer for some installation issues. Instead of putting the actual CD-ROM in your machine, QEMU, a popular hardware virtualization solution, could help you to test it on virtual machines. Qemu can help you to do a GUI install with Desktop or live Server install ISO, or use text-mode installation with a Server install CD.\nInstall 1  $ sudo apt install qemu   And also you need a ISO for installation.\nsimplefied case 1  $ qemu-system-x86_64 -cdrom ubuntu.iso   But above command should not work and will be a kernel panic. Be default qemu will only allocates 128MB of memory by default which is not enough in most case. And ther is no hard drive attached to the VM for the instalation to complete.\nWith a specified hard drive and a proper memory 1 2 3 4 5  $ qemu-img create -f qcow2 disk.qcow2 10G Formatting \u0026#39;disk.qcow2\u0026#39;, fmt=qcow2 size=10737418240 cluster_size=65536 lazy_refcounts=off refcount_bits=16 $ qemu-system-x86_64 -cdrom ubuntu-18.04.4-desktop-amd64.iso \\ -hda disk.qcow2 \\ -m 4096   But only use QEMU to run a instance will be very slow. You need to add -enable-kvm option to let QEMU to run in KVM mode. To use this option you need to make sure KVM is supported by your processor and kernel.\nSo, try below command and you can have a GUI to install the OS.\n1 2 3 4  $ qemu-system-x86_64 -cdrom ubuntu-18.04.4-desktop-amd64.iso \\ -hda disk.qcow2 \\ -m 4096 \\ -kvm-enable   Other scenarios With more than one disks 1  $ qemu-system-x86_64 -cdrom ubuntu.iso -hda disk1.qcow2 -hdb disk2.qcow2   With a boot menu 1  $ qemu-system-x86_64 -boot menu=on -cdrom ubuntu.iso -hda disk.qcow2   With physical USB drive (requires root, hdb is used to avoid conflict) 1  $ qemu-system-x86_64 -usb /dev/sdb1 -hdb disk.qcow2   With an image as USB drive 1  $ qemu-system-x86_64 -usb usb.img -hdb disk.qcow2   refers http://manpages.ubuntu.com/manpages/bionic/man1/qemu-system.1.html\n","permalink":"https://wenhan.blog/post/install-linux-os-with-qemu-cli/","summary":"Background Sometimes you want to build a reproducer for some installation issues. Instead of putting the actual CD-ROM in your machine, QEMU, a popular hardware virtualization solution, could help you to test it on virtual machines. Qemu can help you to do a GUI install with Desktop or live Server install ISO, or use text-mode installation with a Server install CD.\nInstall 1  $ sudo apt install qemu   And also you need a ISO for installation.","title":"Install Linux OS With Qemu CLI"},{"content":"This artcle is a note for myself to build a private NAS station using Ubuntu 20.04. The main usage of my NAS is to store photoes. For other services, run webmin to control machine via WebUI, and run Emby/ Jellyfin for multi media. Above 2 will run in docker so I also run portainer to manage them.\nOverview Use Samba to share storage, and use PhotoSync to upload photoes from iPhone to NAS. Use smartctl to check the disk condition, if anything need attention send a mail to me.\nFile share (Samba) Install Samba by the following:\n1  sudo apt-get install samba smbfs   Configure samba settings by opening /etc/samba/smb.conf, If needed, change your workgroup\n1 2  # Change this to the workgroup/NT-domain name your Samba server will part of workgroup = WORKGROUP   Next is to set your share folder, input something like this at the end of the file.\n1 2 3 4 5 6 7  [share] comment = Share directory for my self-nas path = /share read only = no guest only = no guest ok = no share modes = yes   Restart smbd service and confirm the service is running.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19  wshi@nuc:~$ sudo systemctl restart smbd wshi@nuc:~$ sudo systemctl status smbd ● smbd.service - Samba SMB Daemon Loaded: loaded (/lib/systemd/system/smbd.service; enabled; vendor preset: enabled) Active: active (running) since Fri 2020-04-10 15:58:55 JST; 3s ago Docs: man:smbd(8) man:samba(7) man:smb.conf(5) Main PID: 7696 (smbd) Status: \u0026#34;smbd: ready to serve connections...\u0026#34; Tasks: 4 (limit: 4915) CGroup: /system.slice/smbd.service ├─7696 /usr/sbin/smbd --foreground --no-process-group ├─7712 /usr/sbin/smbd --foreground --no-process-group ├─7713 /usr/sbin/smbd --foreground --no-process-group └─7722 /usr/sbin/smbd --foreground --no-process-group Apr 10 15:58:55 nuc systemd[1]: Starting Samba SMB Daemon... Apr 10 15:58:55 nuc systemd[1]: Started Samba SMB Daemon.   Set the password of the user which you want to use to access the samba server. You can use this command again if you forgot the password.\n1  wshi@nuc:~$ sudo smbpasswd -a wshi   Finially, create the share folder and set the right permissions 0777\n1 2  $ sudo mkdir /share $ sudo chmod 0777 /share   Disk Check - SMART The NAS system will be 24x7 running so we need some script to monitor its health. SMART is a good tool for monitoring HDD,SSD and eMMC drives.\nInstallation 1  $ sudo apt-get install smartmontools   Confirm SMART status Scan hard disk,\n1 2 3 4  $ sudo smartctl --scan /dev/sda -d scsi # /dev/sda, SCSI device /dev/sdb -d sat # /dev/sdb [SAT], ATA device /dev/nvme0 -d nvme # /dev/nvme0, NVMe device   Ensure the hard disk support SMART and is enable\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18  $ sudo smartctl -i /dev/sda smartctl 7.1 2019-12-30 r5022 [x86_64-linux-5.4.0-25-generic] (local build) Copyright (C) 2002-19, Bruce Allen, Christian Franke, www.smartmontools.org === START OF INFORMATION SECTION === Model Family: Western Digital Green Device Model: WDC WD20EARX-00PASB0 Serial Number: WD-WCAZAE607205 LU WWN Device Id: 5 0014ee 20713dc7e Firmware Version: 51.0AB51 User Capacity: 2,000,398,934,016 bytes [2.00 TB] Sector Sizes: 512 bytes logical, 4096 bytes physical Device is: In smartctl database [for details use: -P show] ATA Version is: ATA8-ACS (minor revision not indicated) SATA Version is: SATA 3.0, 6.0 Gb/s (current: 6.0 Gb/s) Local Time is: Tue Apr 21 10:58:05 2020 JST SMART support is: Available - device has SMART capability. SMART support is: Enabled   The last 2 lines show whether SMART support is available and enabled.\nShow SMART infomation 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25  $ sudo smartctl -A /dev/sda smartctl 7.1 2019-12-30 r5022 [x86_64-linux-5.4.0-25-generic] (local build) Copyright (C) 2002-19, Bruce Allen, Christian Franke, www.smartmontools.org === START OF READ SMART DATA SECTION === SMART Attributes Data Structure revision number: 16 Vendor Specific SMART Attributes with Thresholds: ID# ATTRIBUTE_NAME FLAG VALUE WORST THRESH TYPE UPDATED WHEN_FAILED RAW_VALUE 1 Raw_Read_Error_Rate 0x002f 200 200 051 Pre-fail Always - 1 3 Spin_Up_Time 0x0027 165 157 021 Pre-fail Always - 6716 4 Start_Stop_Count 0x0032 100 100 000 Old_age Always - 665 5 Reallocated_Sector_Ct 0x0033 200 200 140 Pre-fail Always - 0 7 Seek_Error_Rate 0x002e 200 200 000 Old_age Always - 0 9 Power_On_Hours 0x0032 098 098 000 Old_age Always - 1946 10 Spin_Retry_Count 0x0032 100 100 000 Old_age Always - 0 11 Calibration_Retry_Count 0x0032 100 100 000 Old_age Always - 0 12 Power_Cycle_Count 0x0032 100 100 000 Old_age Always - 616 192 Power-Off_Retract_Count 0x0032 200 200 000 Old_age Always - 39 193 Load_Cycle_Count 0x0032 196 196 000 Old_age Always - 13842 194 Temperature_Celsius 0x0022 127 099 000 Old_age Always - 23 196 Reallocated_Event_Count 0x0032 200 200 000 Old_age Always - 0 197 Current_Pending_Sector 0x0032 200 200 000 Old_age Always - 0 198 Offline_Uncorrectable 0x0030 200 200 000 Old_age Offline - 0 199 UDMA_CRC_Error_Count 0x0032 200 200 000 Old_age Always - 0 200 Multi_Zone_Error_Rate 0x0008 200 200 000 Old_age Offline - 0   Available Tests for the disk(SCSI) There are 2 types of tests:\nShort Test This test is the rapid identification of a defective hard drive. There fore, a maximum run time is 2 min. This test checks the disk by dividing it into 3 different segments. The following areas are tested.\n Electrical Properties: The controller tests its own electronics, and since this is specific to each manufacturer, it cannot be explained exactly what is being tested. It is conceivable, for example, to test the internal RAM, the read/write circuits or the head electronics. Mechanical Properties: The exact sequence of the servos and the positioning mechanism to be tested is also specific to each manufacturer. Read/Verify: It will read a certain area of the disk and verify certain data, the size and position of the region that is read is also specific to each manufacturer.  Long Test This test is designed as the final test in production. There is no time restriction and the entire disk is checked and not just a section.\nThere are also other test which only available for ATA hard drive. Conveyance Test and Select Test.\nPerform a test Before performing a test, you can use following command to show the time duration of the various tests\n1  $ sudo smartctl -c /dev/sdc   Example output\n1 2 3 4 5 6 7 8  ... Short self-test routine recommended polling time: ( 2) minutes. Extended self-test routine recommended polling time: ( 353) minutes. Conveyance self-test routine recommended polling time: ( 5) minutes. ...   The following command starts the desired test (in Background Mode)\n1  $ sudo smartctl -t \u0026lt;short|long|conveyance|select\u0026gt; /dev/sda   For example,\n1 2 3 4 5 6 7 8 9 10 11  $ sudo smartctl -t short /dev/sda smartctl 7.1 2019-12-30 r5022 [x86_64-linux-5.4.0-25-generic] (local build) Copyright (C) 2002-19, Bruce Allen, Christian Franke, www.smartmontools.org === START OF OFFLINE IMMEDIATE AND SELF-TEST SECTION === Sending command: \u0026#34;Execute SMART Short self-test routine immediately in off-line mode\u0026#34;. Drive command \u0026#34;Execute SMART Short self-test routine immediately in off-line mode\u0026#34; successful. Testing has begun. Please wait 2 minutes for test to complete. Test will complete after Tue Apr 21 11:25:01 2020 JST Use smartctl -X to abort test.   The test will run in background and the priority of the test is low, which means the normal instructions continue to be processed by the hard disk. If the hard drive is busy, the test is paused and then continues at a lower load speed, so there is no interruption of the operation.\nThere is another Foreground mode which all commands will be answered during the test with a \u0026ldquo;CHECK CONDITION\u0026rdquo; status. Therefore, this mode is only recommended when the hard disk is not used. In principle, the background mode is the preferred mode.\nTo perform the tests in Foreground Mode a -C must be added to the command.\n1  $ sudo smartctl -t short -C /dev/sda   Verify the test result The test results are included in the output of the following:\n1  $ sudo smartctl -a /dev/sda   Example output\n1 2 3 4 5  ... SMART Self-test log structure revision number 1 Num Test_Description Status Remaining LifeTime(hours) LBA_of_first_error # 1 Short offline Completed without error 00% 1946 - ...   Or use the following, if only the test results should are displayed:\n1 2 3 4 5 6 7 8  $ sudo smartctl -l selftest /dev/sda smartctl 7.1 2019-12-30 r5022 [x86_64-linux-5.4.0-25-generic] (local build) Copyright (C) 2002-19, Bruce Allen, Christian Franke, www.smartmontools.org === START OF READ SMART DATA SECTION === SMART Self-test log structure revision number 1 Num Test_Description Status Remaining LifeTime(hours) LBA_of_first_error # 1 Short offline Completed without error 00% 1946 -   Force stop the test Use -X if you want to stop the test when performing.\n1 2 3 4 5 6 7  $ sudo smartctl -X /dev/sda smartctl 7.1 2019-12-30 r5022 [x86_64-linux-5.4.0-25-generic] (local build) Copyright (C) 2002-19, Bruce Allen, Christian Franke, www.smartmontools.org === START OF OFFLINE IMMEDIATE AND SELF-TEST SECTION === Sending command: \u0026#34;Abort SMART off-line mode self-test routine\u0026#34;. Self-testing aborted!   We can use this tool to check the disk status and send us email if anything need our attention.\nrefer [https://mekou.com/linux-magazine/smartctl-%E3%82%B3%E3%83%9E%E3%83%B3%E3%83%89%E3%81%A7%E3%83%87%E3%82%A3%E3%82%B9%E3%82%AF%E6%B4%BB%E5%8B%95%E3%81%AE%E8%A9%B3%E7%B4%B0%E6%83%85%E5%A0%B1%E3%82%92%E5%8F%8E%E9%9B%86/](smartctl コマンドでディスク活動の詳細情報を収集)\n[https://www.thomas-krenn.com/en/wiki/SMART_tests_with_smartctl#ATA.2FSCSI_Tests](SMART tests with smartctl)\nMail - sendEmail(CLI) The NAS system will be 24x7 running so we need some script to monitor its health. If anything is not good it\u0026rsquo;s necessary to inform me by sending a mail. SendEmail is a lightweight, completely command line-based SMTP email delivery program. If you have the need to send email from a command prompt this tool is perfect.\nInstall SendEmail by following command\n1  $ sudo apt install sendemail   OK, let\u0026rsquo;s send a test mail by it.\n1 2 3 4 5 6 7 8 9  $ sendEmail -f \u0026lt;FROM ADDRESS\u0026gt;@gmail.com \\ -s smtp.gmail.com:587 \\ -xu \u0026lt;USERNAME\u0026gt; \\ -xp \u0026lt;PASSWORD\u0026gt; \\ -t \u0026lt;TO ADDRESS\u0026gt;@gmail.com \\ -u \u0026#34;test title\u0026#34; \\ -m \u0026#34;test contents\u0026#34; Apr 06 14:33:24 nuc sendEmail[24438]: NOTICE =\u0026gt; Authentication not supported by the remote SMTP server! Apr 06 14:33:25 nuc sendEmail[24438]: ERROR =\u0026gt; Received: 530 5.7.0 Must issue a STARTTLS command first. mm18sm11195078pjb.39 - gsmtp   Oops, it failed with unsupported error. And looks related to tls.\nLet\u0026rsquo;s specify the tls supported and re-run the command.\n1 2 3 4 5 6 7 8 9  $ sendEmail -f \u0026lt;FROM ADDRESS\u0026gt;@gmail.com \\ -s smtp.gmail.com:587 \\ -xu \u0026lt;USERNAME\u0026gt; \\ -xp \u0026lt;PASSWORD\u0026gt; \\ -t \u0026lt;TO ADDRESS\u0026gt;@gmail.com \\ -u \u0026#34;test title\u0026#34; \\ -m \u0026#34;test contents\u0026#34; \\ -o tls=yes Apr 06 15:16:10 nuc sendEmail[25354]: ERROR =\u0026gt; No TLS support! SendEmail can\u0026#39;t load required libraries. (try installing Net::SSLeay and IO::Socket::SSL)   We got a different message, and the root cause is we need more libraries. There are 2 packages we need to install.\n1 2  $ sudo apt-get install libnet-ssleay-perl $ sudo apt-get install libio-socket-ssl-perl   Re-run the command and we can send email from CLI. :-)\n1  Apr 06 15:17:28 nuc sendEmail[25507]: Email was sent successfully!   docker management - portainer There are a lot of docker image for a better NAS life, so let\u0026rsquo;s install and setup Portainer first. Portainer gives you a detailed overview of your Docker environments and allows you to manage your containers, images, networks and volumes. Install protainer is easy as it can be deployed as a container. Use the following command to deploy the Portainer Server.\n1 2  $ docker volume create portainer_data $ docker run -d -p 8000:8000 -p 9000:9000 --name=portainer --restart=always -v /var/run/docker.sock:/var/run/docker.sock -v portainer_data:/data portainer/portainer   Please note the  -v /var/run/docker.sock:/var/run/docker.sock works in Linux environment only.\nIf your container runs successfully, You can now login to https://\u0026lt;SERVER IP ADDR\u0026gt;:9000/ to access the protainer dashboard. First it will ask you a new password for admin user. Connect your local docker engine environment and you can see something as follows.\nmulti-media - Emby server Emby is a media server designed to organize, play, and stream audio and video to a variety of devices.\nThe install is very easy, you can start a container to run Emby server. There is a Installation Guide on First pull the latest image\n1  docker pull emby/embyserver:latest   Then just launch a new container using the following command\n1 2 3 4 5 6 7 8 9 10 11 12  docker run -d \\ --volume /path/to/programdata:/config \\ # This is mandatory --volume /path/to/share1:/mnt/share1 \\ # To mount a first share --volume /path/to/share2:/mnt/share2 \\ # To mount a second share --device /dev/dri:/dev/dri \\ # To mount all render nodes for VAAPI/NVDEC/NVENC --runtime=nvidia \\ # To expose your NVIDIA GPU --publish 8096:8096 \\ # To expose the HTTP port --publish 8920:8920 \\ # To expose the HTTPS port --env UID=1000 \\ # The UID to run emby as (default: 2) --env GID=100 \\ # The GID to run emby as (default 2) --env GIDLIST=100 \\ # A comma-separated list of additional GIDs to run emby as (default: 2) emby/embyserver:latest   Above one is from the offical guide, but you don\u0026rsquo;t need to set all the options. Some of the options are not necessary to change, if you ignore it the default setting will work. Let me paste the command works for me.\n1 2 3 4 5  $ sudo docker run -d --volume /share/movie:/mnt/share1 \\ --publish 8096:8096 \\ --env UID=`id -u` \\ --env GID=`id -g` \\ emby/embyserver:latest   If your container runs successfully, You can now login to https://\u0026lt;SERVER IP ADDR\u0026gt;:8096/ to access the Emby site, and follow the guide to set your environment.\nThere is one important setting about the subtitles, you need to create a new account at https://www.opensubtitles.org/, and set your username/password in Emby. Then you should download subtitle in Emby, this is super helpful.\nWeb control pannel - webmin Webmin is a web-based interface for system administration for Unix. Using any modern web browser, you can setup user accounts, Apache, DNS, file sharing and much more. Webmin removes the need to manually edit Unix configuration files like /etc/passwd, and lets you manage a system from the console or remotely. See the standard modules page for a list of all the functions built into Webmin.\nThe install of webmin is easy. First need to import the webmin GPG key and the apt-repository. Then you can just install webmin via apt command.\n1 2 3  $ wget -q http://www.webmin.com/jcameron-key.asc -O- | sudo apt-key add - $ sudo add-apt-repository \u0026#34;deb [arch=amd64] http://download.webmin.com/download/repository sarge contrib\u0026#34; $ sudo apt install webmin   After the install, a message will be displayed as follows. Access the URL with the username and password in the host machine to login to the webUI.\n1 2 3  Webmin install complete. You can now login to https://\u0026lt;SERVER IP ADDR\u0026gt;:10000/ as root with your root password, or as any user who can use sudo to run commands as root.   The dashboard is like this Then you can check or modify contents/settings on the host machine. For example, to check the files, click Others-\u0026gt;FIle Manager.\nIf your system is behind a UFW firewall, you may need to open the 10000 port which is used by default to listen connections.\nTo alow traffic on port 10000 run the following command.\n1  $ sudo ufw allow 10000/tcp   Refers to\nhttps://www.digitalocean.com/community/tutorials/how-to-install-webmin-on-ubuntu-18-04\nhttps://linuxize.com/post/how-to-install-webmin-on-ubuntu-18-04/\n","permalink":"https://wenhan.blog/post/self-nas-environment-project/","summary":"This artcle is a note for myself to build a private NAS station using Ubuntu 20.04. The main usage of my NAS is to store photoes. For other services, run webmin to control machine via WebUI, and run Emby/ Jellyfin for multi media. Above 2 will run in docker so I also run portainer to manage them.\nOverview Use Samba to share storage, and use PhotoSync to upload photoes from iPhone to NAS.","title":"Self Nas Environment Project"},{"content":"Multipass is a very useful tools to create Ubuntu VM instance. It will provides a CLI to launch and manage the Linux instances. The downloading of a cloud image is also automatically, and a VM can be up and running within minutes.\nhttps://multipass.run/\nBut in my case, I tried multipass 1.1.0 to quick launch an instance, but it failed with a Network timeout error.\n1 2 3 4 5 6  $ multipass version multipass 1.1.0 multipassd 1.1.0 $ multipass launch launch failed: failed to download from \u0026#39;http://cloud-images.ubuntu.com/releases/server/releases/bionic/release-20200317/ubuntu-18.04-server-cloudimg-amd64.img\u0026#39;: Network timeout   I checked this link with wget and has no issue, so I assumpt the download time of the image may too long to some timeout value in multipass. Can we download it manually and start the instance by it? Sure we can. You can download the images and pass the image path as a parameter.\n1 2  $ multipass launch file:///home/wshi/Downloads/ubuntu-18.04-server-cloudimg-amd64.img Launched: lucrative-eelpout   One more thing about the download time in Japan. Using http://cloud-images.ubuntu.com seems slow to me. So I switched to the mirror of Toyama Univ. http://ubuntutym2.u-toyama.ac.jp/cloud-images/releases/. It\u0026rsquo;s 10 times faster than the origin one, hope this can help you.\n","permalink":"https://wenhan.blog/post/multipass-launch-failed-by-network-timeout/","summary":"Multipass is a very useful tools to create Ubuntu VM instance. It will provides a CLI to launch and manage the Linux instances. The downloading of a cloud image is also automatically, and a VM can be up and running within minutes.\nhttps://multipass.run/\nBut in my case, I tried multipass 1.1.0 to quick launch an instance, but it failed with a Network timeout error.\n1 2 3 4 5 6  $ multipass version multipass 1.","title":"Multipass Launch Failed by Network Timeout"},{"content":"HexoからHugoに 今までHexoでブログを書いたが、Golang勉強のついでにHugoに移しました。 理由はいろいろありますが、主な点は以下\n Hexoは複数のモジュールを使うので、たまにインストールがコケる それに対しHugoはバイナリ一つで十分 HTMLファイル生成のスピード、HexoよりHugoが断然に速い  Hugoのインストール方法や利用方法については割愛しましたが、今回のブログ移転で 実際に会った問題を整理する\nTagの付け方 Hexoの場合、以下のタグの付け方が大丈夫でしたが、Hugoの時はだめでした\n1  tag: Python   全部以下に統一すれば問題ない\n1 2  tags: - Python   Blog mdファイルの保存場所 これはthemeによって異なります。今回はEvenを利用したのでpostになっています。\n生成したブログページとGithub Actionの連携 Hexoの場合、deployのサブコマンドでGithubにPush出来ましたが、Hugoの場合はでき ません。Hugo deployコマンドは一応ありますが、AWS,GCE,Azure向けでした。 https://gohugo.io/hosting-and-deployment/hugo-deploy/\nそのためファイルを生成して手動でgithub pageのレポジトリーにpushする必要がありま す。生成したブログページのファイルがpublicディレクトリにあります。 Github Actionを利用すれば、新しい記事を書いてpushしたら、上記の処理が全自動に 出来ます。そのやり方は次の記事に纏めます。\nhttpsの対応 Hugoと関係ないが、ついでにCloudFlareを使ってhttpsへ対応した。\n","permalink":"https://wenhan.blog/post/moving-from-hexo-to-hugo/","summary":"HexoからHugoに 今までHexoでブログを書いたが、Golang勉強のついでにHugoに移しました。 理由はいろいろありますが、主な点は以下\n Hexoは複数のモジュールを使うので、たまにインストールがコケる それに対しHugoはバイナリ一つで十分 HTMLファイル生成のスピード、HexoよりHugoが断然に速い  Hugoのインストール方法や利用方法については割愛しましたが、今回のブログ移転で 実際に会った問題を整理する\nTagの付け方 Hexoの場合、以下のタグの付け方が大丈夫でしたが、Hugoの時はだめでした\n1  tag: Python   全部以下に統一すれば問題ない\n1 2  tags: - Python   Blog mdファイルの保存場所 これはthemeによって異なります。今回はEvenを利用したのでpostになっています。\n生成したブログページとGithub Actionの連携 Hexoの場合、deployのサブコマンドでGithubにPush出来ましたが、Hugoの場合はでき ません。Hugo deployコマンドは一応ありますが、AWS,GCE,Azure向けでした。 https://gohugo.io/hosting-and-deployment/hugo-deploy/\nそのためファイルを生成して手動でgithub pageのレポジトリーにpushする必要がありま す。生成したブログページのファイルがpublicディレクトリにあります。 Github Actionを利用すれば、新しい記事を書いてpushしたら、上記の処理が全自動に 出来ます。そのやり方は次の記事に纏めます。\nhttpsの対応 Hugoと関係ないが、ついでにCloudFlareを使ってhttpsへ対応した。","title":"Moving From Hexo to Hugo"},{"content":"I have a ASRock Fatal1ty B450 Gaming-ITX with the latest BIOS 3.40. As I want to power it on remotely, In Bios setting I enabled \u0026lsquo;Restore on AC/Power Loss\u0026rsquo; option on BIOS, but it\u0026rsquo;s not working.\nThen I found below and upgrade my bios firmware to 3.53, and everything works just fine! You can find the v3.53 on below link.\nhttp://forum.asrock.com/forum_posts.asp?TID=12059\u0026amp;title=fatal1ty-b450-gamingitx-restore-on-ac-power-loss\n","permalink":"https://wenhan.blog/post/asrock-fatal1ty-b450-gaming-itx-restore-on-ac-power-loss-not-working/","summary":"I have a ASRock Fatal1ty B450 Gaming-ITX with the latest BIOS 3.40. As I want to power it on remotely, In Bios setting I enabled \u0026lsquo;Restore on AC/Power Loss\u0026rsquo; option on BIOS, but it\u0026rsquo;s not working.\nThen I found below and upgrade my bios firmware to 3.53, and everything works just fine! You can find the v3.53 on below link.\nhttp://forum.asrock.com/forum_posts.asp?TID=12059\u0026amp;title=fatal1ty-b450-gamingitx-restore-on-ac-power-loss","title":"ASRock Fatal1ty B450 Gaming-ITX 'Restore on AC/Power Loss' not working"},{"content":"I will use nmcli to do this task.\nFirst you need to install network-manager package, and start the Daemon\n1 2  $ sudo apt install network-manager $ sudo systemctl start NetworkManager   Then let\u0026rsquo;s check the network interface status by below command\n1 2 3 4 5 6 7  $ nmcli dev status DEVICE TYPE STATE CONNECTION wlp2s0 wifi connected xibuka-wifi-5G enp0s31f6 ethernet connected netplan-enp0s31f6 p2p-dev-wlp2s0 wifi-p2p disconnected -- eth0 ethernet unavailable -- lo loopback unmanaged --   Next step is to check the available Wifi access points.\n1 2 3 4 5 6 7 8 9 10  $ nmcli dev wifi list SSID MODE CHAN RATE SIGNAL BARS SECURITY aterm-55ebc5-g Infra 11 405 Mbit/s 77 ▂▄▆_ WPA1 WPA2 xibuka-wifi-2G Infra 11 405 Mbit/s 77 ▂▄▆_ WPA1 WPA2 xibuka-wifi-5G Infra 36 405 Mbit/s 63 ▂▄▆_ WPA1 WPA2 aterm-55ebc5-a Infra 36 405 Mbit/s 54 ▂▄__ WPA1 WPA2 HG8045-0D9B-bg Infra 8 195 Mbit/s 49 ▂▄__ WPA1 WPA2 HG8045-0D9B-a Infra 44 405 Mbit/s 35 ▂▄__ WPA1 WPA2 HG8045-A39A-a Infra 108 405 Mbit/s 35 ▂▄__ WPA1 WPA2 MNG6300-F560-G Infra 11 130 Mbit/s 30 ▂___ WPA1 WPA2   If you can not see the SSID you want to use, you can do a re-scan by below command\n1  nmcli dev wifi rescan   OK, let\u0026rsquo;s try to connect to Wifi using below nmcli command\n1 2  $ sudo nmcli dev wifi connect xibuka-wifi-5G password \u0026#39;xxxxxxxxxx\u0026#39; Device \u0026#39;wlp2s0\u0026#39; successfully activated with \u0026#39;f1cc419e-7b51-4d99-8cff-2894dc054f19\u0026#39;.   Or you can use --ask option to input your password interactively and don\u0026rsquo;t display it.\n1 2 3  $ sudo nmcli --ask dev wifi connect xibuka-wifi-5G Password: Device \u0026#39;wlp2s0\u0026#39; successfully activated with \u0026#39;f1cc419e-7b51-4d99-8cff-2894dc054f19\u0026#39;.   Delete the established connections\n1  sudo nmcli con del xibuka-wifi-5G   That\u0026rsquo;s all! Hope this help you.\n","permalink":"https://wenhan.blog/post/access-wifi-point-via-command-line-cli/","summary":"I will use nmcli to do this task.\nFirst you need to install network-manager package, and start the Daemon\n1 2  $ sudo apt install network-manager $ sudo systemctl start NetworkManager   Then let\u0026rsquo;s check the network interface status by below command\n1 2 3 4 5 6 7  $ nmcli dev status DEVICE TYPE STATE CONNECTION wlp2s0 wifi connected xibuka-wifi-5G enp0s31f6 ethernet connected netplan-enp0s31f6 p2p-dev-wlp2s0 wifi-p2p disconnected -- eth0 ethernet unavailable -- lo loopback unmanaged --   Next step is to check the available Wifi access points.","title":"connect to wifi in Linux via nmcli command"},{"content":"exit script immediately when a command fails\n1 2  set -o errexit set -e   output error and exit script immediately when refer to a undefine variable.\n1 2  set -o nounset set -u   exit script even a command fails before a pipe\n1  set -o pipefail   ","permalink":"https://wenhan.blog/post/bash-head-options/","summary":"exit script immediately when a command fails\n1 2  set -o errexit set -e   output error and exit script immediately when refer to a undefine variable.\n1 2  set -o nounset set -u   exit script even a command fails before a pipe\n1  set -o pipefail   ","title":"The begining of bash script"},{"content":"I did a presentation about microk8s and snap at ContainerDays Tokyo. The slide(Japanese) can be found as below.\nspeakerdeck.com\nEnjoy!\n","permalink":"https://wenhan.blog/post/presentasion-about-microk8s-at-containerdaystokyo/","summary":"I did a presentation about microk8s and snap at ContainerDays Tokyo. The slide(Japanese) can be found as below.\nspeakerdeck.com\nEnjoy!","title":"Presentasion about microk8s at containerDaysTokyo"},{"content":"OpenStack frequently used command OpenStack Compute - Nova  list instances nova list openstack server list list/check flavor nova flavor-list nova flavor-show \u0026lt;name or ID\u0026gt; openstack flavor list openstack flavor show \u0026lt;name or ID\u0026gt; create flavor openstack flavor create --ram \u0026lt;ram\u0026gt; --vcpus \u0026lt;cpu number\u0026gt; --disk \u0026lt;size\u0026gt; --id \u0026lt;id\u0026gt; \u0026lt;name\u0026gt; nova flavor-create \u0026lt;name\u0026gt; \u0026lt;id\u0026gt; \u0026lt;ram\u0026gt; \u0026lt;disk\u0026gt; \u0026lt;vcpus\u0026gt; launch an instance nova boot \u0026lt;name\u0026gt; --image \u0026lt;image\u0026gt; --flavor \u0026lt;flavor\u0026gt; openstack server create --flavor \u0026lt;flavor\u0026gt; --image \u0026lt;image\u0026gt; \u0026lt;name\u0026gt; launch an instance with network openstack server create --flavor \u0026lt;flavor\u0026gt; --image \u0026lt;image\u0026gt; \u0026lt;name\u0026gt; net-id=\u0026lt;network\u0026gt; launch an instance with key-pair nova boot \u0026lt;name\u0026gt; --image \u0026lt;image\u0026gt; --flavor \u0026lt;flavor\u0026gt; --key-name \u0026lt;key-pair name\u0026gt; openstack server create --flavor \u0026lt;flavor\u0026gt; --image \u0026lt;image\u0026gt; \u0026lt;name\u0026gt; access instance via router ip netns list sudo ip netns exec \u0026lt;qrouter-id\u0026gt; ssh -i \u0026lt;key\u0026gt; user@ip launch an instance with custom port nova boot --image \u0026lt;image\u0026gt; --flavor \u0026lt;flavor\u0026gt; --nic port-id=\u0026lt;port-id\u0026gt; \u0026lt;instance name\u0026gt; delete an instance nova delete \u0026lt;ID\u0026gt; openstack server delete \u0026lt;ID or name\u0026gt;  Openstack Network - Neutron  list network openstack network list list subnetwork openstack subnet list --long create a network openstack network create \u0026lt;net name\u0026gt; create a subnetwork openstack subnet create \u0026lt;subnet name\u0026gt; --network \u0026lt;net name\u0026gt; --subnet-range \u0026lt;ip address\u0026gt;/\u0026lt;prefix\u0026gt; --gateway \u0026lt;gw ip\u0026gt; --allocation-pool start=IP_ADDR,end=IP_ADDR e.g. openstack subnet create practicesubnet --network practice --subnet-range 10.2.0.224/27 --gateway 10.2.0.225 --allocation-pool start=10.2.0.240,end=10.2.0.245 create port with specify IP address openstack subnet list --long to confirm avaiable address range openstack port create --network=\u0026lt;network\u0026gt; --fixed-ip subnet=private-subnet,ip-address=\u0026lt;ip_address\u0026gt; \u0026lt;port name\u0026gt; create port without specify IP address openstack port create \u0026lt;port name\u0026gt; --network \u0026lt;network\u0026gt; system will allocate one IP address for this port search ports with specified fixed IP addresses neutron port-list --fixed-ips ip_address=\u0026lt;IP1\u0026gt; ip_address=\u0026lt;IP2\u0026gt; create a router openstack router create \u0026lt;router\u0026gt; Link the router to the external provider network openstack router set \u0026lt;router\u0026gt; --external-gateway \u0026lt;public network\u0026gt; add subnet to router openstack router add subnet \u0026lt;router\u0026gt; \u0026lt;subnet\u0026gt; remove subnet from router openstack router remove subnet \u0026lt;router\u0026gt; \u0026lt;subnet\u0026gt; delete router openstack router delete \u0026lt;router\u0026gt; create external network openstack network create public --external --provider-network-type flat --provider-physical-network external manage floating IP neutron floatingip-create \nneutron floatingip-delete  neutron floatingip-associate  neutron floatingip-disassociate neutron floatingip-list   e.g. neutron floatingip-create public neutron floatingip-associate \u0026lt;fip ID\u0026gt; \u0026lt;port ID of instance's internal ip\u0026gt;\nOpenStack Image - Glance  image list and show detail glance image-list glance image-show \u0026lt;ID\u0026gt; openstack image list openstack image show \u0026lt;name or ID\u0026gt; check image file info qemu-img info \u0026lt;path/to/image\u0026gt; create image from file glance image-create --progress --name \u0026lt;name\u0026gt; --file /path/to/file --disk-format qcow2 --container-format bare --visibility public download image openstack image save \u0026lt;image\u0026gt; --file \u0026lt;save/to/file\u0026gt; delete an image glance image-delete \u0026lt;ID\u0026gt; openstack image delete \u0026lt;ID\u0026gt;  OpenStack block Storage - Cinder  volume list and show detail cinder list cinder show \u0026lt;ID\u0026gt; openstack volume list openstack volume show \u0026lt;name or ID\u0026gt; create new empty volume cinder create --name \u0026lt;vol name\u0026gt; \u0026lt;size in GiBs\u0026gt; create new volume from image cinder create --name \u0026lt;vol name\u0026gt; \u0026lt;size in GiBs\u0026gt; --image \u0026lt;ID or Name\u0026gt; attach volume to an instance openstack server add volume \u0026lt;instance\u0026gt; \u0026lt;volume\u0026gt; nova volume-attach \u0026lt;instance\u0026gt; \u0026lt;volume ID\u0026gt; \u0026lt;device\u0026gt; detach volume from an instance openstack server remove volume \u0026lt;instance\u0026gt; \u0026lt;volume\u0026gt; nova volume-detach \u0026lt;server\u0026gt; \u0026lt;volume\u0026gt; delete volume cinder delete \u0026lt;volume\u0026gt; openstack volume delete \u0026lt;volume\u0026gt;  OpenStack Identity - Keystone  issue a token openstack token issue check auth info source credrc.sh which the file looks like  1 2 3 4 5 6 7 8 9 10  export OS_AUTH_TYPE=password export OS_AUTH_URL=http://127.0.0.1:5000/v3 export OS_IDENTITY_API_VERSION=\u0026#34;3\u0026#34; export OS_TENANT_NAME=\u0026#34;demo\u0026#34; export OS_USERNAME=\u0026#34;demo\u0026#34; export OS_PASSWORD=\u0026#34;nova\u0026#34; export OS_PROJECT_DOMAIN_ID=\u0026#34;default\u0026#34; export OS_USER_DOMAIN_ID=\u0026#34;default\u0026#34; export OS_REGION_NAME=\u0026#34;RegionOne\u0026#34; alias osc=\u0026#34;openstack --os-cloud\u0026#34;    check auth info with export | grep OS_ create new project openstack project create --description \u0026quot;text\u0026quot; \u0026lt;project name\u0026gt; create and delete user openstack user create \u0026lt;name\u0026gt; --password pass openstack user delete \u0026lt;name\u0026gt; check role list openstack role list change user\u0026rsquo;s role in project openstack role add --user \u0026lt;user\u0026gt; --project \u0026lt;project\u0026gt; \u0026lt;role\u0026gt; show quota of a project openstack quota show \u0026lt;project id\u0026gt; nova quota-show --tenant \u0026lt;project id\u0026gt; update quota of a project openstack quota set --\u0026lt;key\u0026gt; \u0026lt;value\u0026gt; \u0026lt;project id\u0026gt; nova quota-update --\u0026lt;key\u0026gt; \u0026lt;value\u0026gt; \u0026lt;project id\u0026gt; change policy  1 2 3 4  vim /etc/glance/policy.json ... \u0026#34;add_image\u0026#34;: \u0026#34;role:admin\u0026#34;, ...    key pair create and list nova keypair-add \u0026lt;name\u0026gt; --pub-key \u0026lt;path/to/public/key\u0026gt; openstack keypair create \u0026lt;name\u0026gt; --pub-key \u0026lt;path/to/public/key\u0026gt; nova keypair-list add security group rule to allow SSH access openstack security group rule create \u0026lt;rule name\u0026gt; --ingress --dst-port 22:22 --protocol tcp --remote-ip 0.0.0.0/0 \u0026lt;group name\u0026gt; nova secgroup-add-rule \u0026lt;group name\u0026gt; \u0026lt;ip-proto\u0026gt; \u0026lt;from-port\u0026gt; \u0026lt;to-port\u0026gt; e.g. nova secgroup-add-rule novasg2 tcp 22 22 0.0.0.0/24 list security group and rule openstack security group list openstack security group rule list  OpenStack Object Storage - Swift The account is not the user account, but more like a namespace/project in swift. The container is like the directory.\n   Task Command     Get account info swift stat   create a container swift post \u0026lt;container\u0026gt;   list all containers in an account swift list   get the info of a container swift stat \u0026lt;container\u0026gt;   upload files/directory to a container swift upload --object-name \u0026lt;object\u0026gt; \u0026lt;containe\u0026gt; \u0026lt;file/firectory path\u0026gt;    list files in a container swift list \u0026lt;containe\u0026gt;   download file swift download \u0026lt;container\u0026gt; \u0026lt;object\u0026gt;   update meta data to container swift post --meta \u0026lt;color\u0026gt;:\u0026lt;value\u0026gt; \u0026lt;container\u0026gt;   delete object swift delete \u0026lt;container\u0026gt; \u0026lt;object\u0026gt;   upload files in specify segments swift upload \u0026lt;container\u0026gt; \u0026lt;object\u0026gt; --segment-size \u0026lt;size\u0026gt;   delete container swift delete \u0026lt;container\u0026gt;    e.g. swift upload uploads files/puppies.jpg --object-name picture\n adding a read ACL on the uploads container allowing anyone to read it except for people from gadget.example.com. swift post -r .r:*,-gadget.example.com uploads adding a write ACL to the uploads container allowing anyone in the phone project to write to it. swift post -w phone:* uploads  ","permalink":"https://wenhan.blog/post/openstack-frequently-used-command/","summary":"OpenStack frequently used command OpenStack Compute - Nova  list instances nova list openstack server list list/check flavor nova flavor-list nova flavor-show \u0026lt;name or ID\u0026gt; openstack flavor list openstack flavor show \u0026lt;name or ID\u0026gt; create flavor openstack flavor create --ram \u0026lt;ram\u0026gt; --vcpus \u0026lt;cpu number\u0026gt; --disk \u0026lt;size\u0026gt; --id \u0026lt;id\u0026gt; \u0026lt;name\u0026gt; nova flavor-create \u0026lt;name\u0026gt; \u0026lt;id\u0026gt; \u0026lt;ram\u0026gt; \u0026lt;disk\u0026gt; \u0026lt;vcpus\u0026gt; launch an instance nova boot \u0026lt;name\u0026gt; --image \u0026lt;image\u0026gt; --flavor \u0026lt;flavor\u0026gt; openstack server create --flavor \u0026lt;flavor\u0026gt; --image \u0026lt;image\u0026gt; \u0026lt;name\u0026gt; launch an instance with network openstack server create --flavor \u0026lt;flavor\u0026gt; --image \u0026lt;image\u0026gt; \u0026lt;name\u0026gt; net-id=\u0026lt;network\u0026gt; launch an instance with key-pair nova boot \u0026lt;name\u0026gt; --image \u0026lt;image\u0026gt; --flavor \u0026lt;flavor\u0026gt; --key-name \u0026lt;key-pair name\u0026gt; openstack server create --flavor \u0026lt;flavor\u0026gt; --image \u0026lt;image\u0026gt; \u0026lt;name\u0026gt; access instance via router ip netns list sudo ip netns exec \u0026lt;qrouter-id\u0026gt; ssh -i \u0026lt;key\u0026gt; user@ip launch an instance with custom port nova boot --image \u0026lt;image\u0026gt; --flavor \u0026lt;flavor\u0026gt; --nic port-id=\u0026lt;port-id\u0026gt; \u0026lt;instance name\u0026gt; delete an instance nova delete \u0026lt;ID\u0026gt; openstack server delete \u0026lt;ID or name\u0026gt;  Openstack Network - Neutron  list network openstack network list list subnetwork openstack subnet list --long create a network openstack network create \u0026lt;net name\u0026gt; create a subnetwork openstack subnet create \u0026lt;subnet name\u0026gt; --network \u0026lt;net name\u0026gt; --subnet-range \u0026lt;ip address\u0026gt;/\u0026lt;prefix\u0026gt; --gateway \u0026lt;gw ip\u0026gt; --allocation-pool start=IP_ADDR,end=IP_ADDR e.","title":"OpenStack frequently used command"},{"content":"Install  Install docker.io  1  apt install docker.io   If you install docker with other cgroup driver, you have to make sure that docker and Kubernetes will use same cgroup driver.\n1 2 3 4 5  cat \u0026lt;\u0026lt; EOF \u0026gt;\u0026gt; /etc/docker/daemon.json { \u0026#34;exec-opts\u0026#34;: [\u0026#34;native.cgroupdriver=systemd\u0026#34;] } EOF    install apt key and source to system  1 2 3 4 5  root@kube-master:~# curl -s https://packages.cloud.google.com/apt/doc/apt-key.gpg | apt-key add - OK root@kube-master:~# cat \u0026lt;\u0026lt;EOF \u0026gt;\u0026gt; /etc/apt/sources.list.d/kubernetes.list \u0026gt; deb http://apt.kubernetes.io/ kubernetes-xenial main \u0026gt; EOF   Then install kubernetes packages\n1 2  root@kube-master:~# apt update -y root@kube-master:~# apt install -y kubelet kubeadm kubectl    setup and config with kubeadm  You must choose a CNI when you execute kubeadm init, in this post I choose flunnel, so I have to add --pod-network-cidr options.\n1  root@k8sm:~# kubeadm init --pod-network-cidr=10.244.0.0/16   after waiting for some while, you should see below message that shows the install is complete\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16  Your Kubernetes master has initialized successfully! To start using your cluster, you need to run the following as a regular user: mkdir -p $HOME/.kube sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config sudo chown $(id -u):$(id -g) $HOME/.kube/config You should now deploy a pod network to the cluster. Run \u0026#34;kubectl apply -f [podnetwork].yaml\u0026#34; with one of the options listed at: https://kubernetes.io/docs/concepts/cluster-administration/addons/ You can now join any number of machines by running the following on each node as root: kubeadm join 192.168.122.75:6443 --token .....\u0026lt;snip\u0026gt;   Follow the instruction, run below comand as a regular user\n1 2 3  mkdir -p $HOME/.kube sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config sudo chown $(id -u):$(id -g) $HOME/.kube/config   And you need to memo the command start with kubeadm join, you will need to exeute this command on your kubernetes node to join to the cluster\nIn order for your pods to communicate with one another, you\u0026rsquo;ll need to install pod networking. We are going to use Flannel for our Container Network Interface (CNI) because it\u0026rsquo;s easy to install and reliable. Enter this command:\n1  kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml   Next run below command to make sure everything is coming up.\n1  kubectl get pods --all-namespaces   If you see the coredns-xxxxxx pod is running, and your master node is Ready, your cluster is ready to accept worker nodes.\n1 2 3 4 5 6 7 8 9 10 11 12 13  wshi@k8sm:~$ kubectl get pods --all-namespaces NAMESPACE NAME READY STATUS RESTARTS AGE kube-system coredns-78fcdf6894-ltgw2 1/1 Running 0 17m kube-system coredns-78fcdf6894-n8hw2 1/1 Running 0 17m kube-system etcd-k8sm 1/1 Running 0 16m kube-system kube-apiserver-k8sm 1/1 Running 0 16m kube-system kube-controller-manager-k8sm 1/1 Running 0 16m kube-system kube-flannel-ds-amd64-ktcqm 1/1 Running 0 1m kube-system kube-proxy-nczhf 1/1 Running 0 17m kube-system kube-scheduler-k8sm 1/1 Running 0 16m wshi@k8sm:~$ kubectl get node NAME STATUS ROLES AGE VERSION k8sm Ready master 17m v1.11.2    setup other node and join to the cluster  For the rest worker nodes, you just need to install kubectl, kubeadm, kubelet and docker refer above, then execute the kubeadm join ... command which was mentioned before. After a while, you should see all worker nodes are ready to use.\n1 2 3 4 5  wshi@k8sm:~$ kubectl get node NAME STATUS ROLES AGE VERSION k8sm Ready master 44m v1.11.2 k8sn1 Ready \u0026lt;none\u0026gt; 11m v1.11.2 k8sn2 Ready \u0026lt;none\u0026gt; 11m v1.11.2   Run a Job Applications that running inside a pod are called \u0026ldquo;jobs\u0026rdquo;.\nMost Kubernetes objects are created using yaml. Here is a sample yaml for a job which uses perl to calculate pi to 2000 digits and then stops.\n1 2 3 4 5 6 7 8 9 10 11 12 13  apiVersion:batch/v1kind:Jobmetadata:name:pispec:template:spec:containers:- name:piimage:perlcommand:[\u0026#34;perl\u0026#34;,\u0026#34;-Mbignum=bpi\u0026#34;,\u0026#34;-wle\u0026#34;,\u0026#34;print bpi(2000)\u0026#34;]restartPolicy:NeverbackoffLimit:4  Create this yaml file on your master node and call it \u0026ldquo;pi-job.yaml\u0026rdquo;. Run the job with the command:\n1  kubectl create -f pi-job.yaml   Get the detail information of this job with the command:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61  $ kubectl get pod NAME READY STATUS RESTARTS AGE ... pi-72c7r 0/1 Completed 0 3m $ kubectl describe pod pi-72c7r Name: pi-72c7r Namespace: default Node: juju-cfb27c-2/10.188.44.225 Start Time: Wed, 29 Aug 2018 02:45:34 +0000 Labels: controller-uid=9a903f30-ab35-11e8-9b51-feb3e5f3b327 job-name=pi Annotations: \u0026lt;none\u0026gt; Status: Succeeded IP: 10.1.33.7 Controlled By: Job/pi Containers: pi: Container ID: docker://0d48f71cc6a2825cf4113f237170e63b06e1e310eca2e950dc979b48f26fb41f Image: perl Image ID: docker-pullable://perl@sha256:a264b269d0ea9687ea1485e47a0f4039b2dab99fc9c6e3faf001b452b57d6087 Port: \u0026lt;none\u0026gt; Host Port: \u0026lt;none\u0026gt; Command: perl -Mbignum=bpi -wle print bpi(2000) State: Terminated Reason: Completed Exit Code: 0 Started: Wed, 29 Aug 2018 02:47:01 +0000 Finished: Wed, 29 Aug 2018 02:47:07 +0000 Ready: False Restart Count: 0 Environment: \u0026lt;none\u0026gt; Mounts: /var/run/secrets/kubernetes.io/serviceaccount from default-token-pkk4t (ro) Conditions: Type Status Initialized True Ready False ContainersReady False PodScheduled True Volumes: default-token-pkk4t: Type: Secret (a volume populated by a Secret) SecretName: default-token-pkk4t Optional: false QoS Class: BestEffort Node-Selectors: \u0026lt;none\u0026gt; Tolerations: node.kubernetes.io/not-ready:NoExecute for 300s node.kubernetes.io/unreachable:NoExecute for 300s Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal Scheduled 3m default-scheduler Successfully assigned default/pi-72c7r to juju-cfb27c-2 Normal Pulling 3m kubelet, juju-cfb27c-2 pulling image \u0026#34;perl\u0026#34; Normal Pulled 1m kubelet, juju-cfb27c-2 Successfully pulled image \u0026#34;perl\u0026#34; Normal Created 1m kubelet, juju-cfb27c-2 Created container Normal Started 1m kubelet, juju-cfb27c-2 Started container   And view the log(STDOUT) with below command:\n1 2  $ kubectl logs pi-72c7r 3.1415926535897932384626433832795028841971693993751058209749445923078164062862089986280348253421170679821480865132823066470938446095505822317253594081284811174502841027019385211055596446229489549303819644288109756659334461284756482337867831652712019091456485669234603486104543266482133936072602491412737245870066063155881748815209209628292540917153643678925903600113305305488204665213841469519415116094330572703657595919530921861173819326117931051185480744623799627495673518857527248912279381830119491298336733624406566430860213949463952247371907021798609437027705392171762931767523846748184676694051320005681271452635608277857713427577896091736371787214684409012249534301465495853710507922796892589235420199561121290219608640344181598136297747713099605187072113499999983729780499510597317328160963185950244594553469083026425223082533446850352619311881710100031378387528865875332083814206171776691473035982534904287554687311595628638823537875937519577818577805321712268066130019278766111959092164201989380952572010654858632788659361533818279682303019520353018529689957736225994138912497217752834791315155748572424541506959508295331168617278558890750983817546374649393192550604009277016711390098488240128583616035637076601047101819429555961989467678374494482553797747268471040475346462080466842590694912933136770289891521047521620569660240580381501935112533824300355876402474964732639141992726042699227967823547816360093417216412199245863150302861829745557067498385054945885869269956909272107975093029553211653449872027559602364806654991198818347977535663698074265425278625518184175746728909777727938000816470600161452491921732172147723501414419735685481613611573525521334757418494684385233239073941433345477624168625189835694855620992192221842725502542568876717904946016534668049886272327917860857843838279679766814541009538837863609506800642251252051173929848960841284886269456042419652850222106611863067442786220391949450471237137869609563643719172874677646575739624138908658326459958133904780275898   Here is another example YAML file for job which use the image \u0026ldquo;busybox\u0026rdquo; and sleep for 10 seconds\n1 2 3 4 5 6 7 8 9 10 11 12 13  apiVersion:batch/v1kind:Jobmetadata:name:busyboxspec:template:spec:containers:- name:busyboximage:busyboxcommand:[\u0026#34;sleep\u0026#34;,\u0026#34;10\u0026#34;]restartPolicy:NeverbackoffLimit:4  Deploy a Pod Pods usually represent running applications in a Kubernetes cluster. Here is an example of some yaml which defines a pod:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14  apiVersion:v1kind:Podmetadata:name:alpinenamespace:defaultspec:containers:- name:alpineimage:alpinecommand:- sleep- \u0026#34;3600\u0026#34;imagePullPolicy:IfNotPresentrestartPolicy:Always  run a Pod 1  kubectl create -f alpine.yaml   delete a Pod 1  kubectl delete -f alpine.yaml   Or\n1  kubectl delete pod alpine   1  kubectl delete pod/alpine   Examine the current status 1 2 3  kubectl get nodes kubectl describe node node-name kubectl get pods --all-namespaces -o wide   Use -n will specify the namespace in use\n1  kubectl get pods -n kube-system   Deployment A yaml file for an nginx deployment\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19  apiVersion: apps/v1beta2 kind: Deployment metadata: name: nginx-deployment spec: selector: matchLabels: app: nginx replicas: 2 template: metadata: labels: app: nginx spec: containers: - name: nginx image: nginx:1.7.9 ports: - containerPort: 80   Create the deployment and pod 1  kubectl create -f nginx-deployment.yaml   Find the detail info\n1  kubectl describe deployment nginx-deployment   Check pod is running on which node\n1 2 3  $ kubectl get pod nginx-deployment-7fc9b7bd96-c6wwh -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE nginx-deployment-7fc9b7bd96-c6wwh 1/1 Running 0 21h 10.1.33.6 juju-cfb27c-2 \u0026lt;none\u0026gt;   rollout image version Change the image version to 1.8, you run below command\n1  kubectl set image deployment nginx-deployment nginx=nginx:1.8   Or, you can update the line in the yaml to 1.8 version of the image, and apply the changes with\n1  kubectl apply -f nginx-deployment.yaml   Check the status of the rollout with below command\n1  kubectl rollout status deployment nginx-deployment   Undo the previous rollout\n1  kubectl rollout undo deployment nginx-deployment   View the history\n1  kubectl rollout history deployment nginx-deployment   Go to a specific point in history\n1  kubectl rollout history deployment nginx-deployment --revision=x   Setting Container Environment Variables Deploy a pod to print Environment Variables\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17  apiVersion: v1 kind: Pod metadata: name: env-dump spec: containers: - name: busybox image: busybox command: - env env: - name: STUDENT_NAME value: \u0026#34;Your Name\u0026#34; - name: SCHOOL value: \u0026#34;Linux Academy\u0026#34; - name: KUBERNETES value: \u0026#34;is awesome\u0026#34;   After executed the pod, you can check Environment Viriables by log\n1 2 3 4 5 6  $ kubectl logs env-dump .... STUDENT_NAME=Your Name SCHOOL=Linux Academy KUBERNETES=is awesome ....   Scaling pod command line Use scale to deployment with \u0026ndash;replicas=X\n1 2 3 4 5 6 7 8 9 10 11 12 13 14  $ kubectl get deployment NAME DESIRED CURRENT UP-TO-DATE AVAILABLE AGE nginx-deployment 2 2 2 2 21h $ kubectl scale deployment nginx-deployment --replicas=3 deployment.extensions/nginx-deployment scaled $ kubectl get deployment NAME DESIRED CURRENT UP-TO-DATE AVAILABLE AGE nginx-deployment 3 3 3 3 21h $ kubectl get pod NAME READY STATUS RESTARTS AGE ... nginx-deployment-7fc9b7bd96-c6wwh 1/1 Running 0 21h nginx-deployment-7fc9b7bd96-kddj5 1/1 Running 0 31s nginx-deployment-7fc9b7bd96-s86gc 1/1 Running 0 21h   yaml file Update replicas: x part in yaml file, and apply the changes with\n1  kubectl apply -f nginx-deployment.yml   Replication Controllers, Replica Sets, and Deployments Deployments replaced the older ReplicationController functionality, but it never hurts to know where you came from. Deployments are easier to work with, and here\u0026rsquo;s a brief exercise to show you how. A Replication Controller ensures that a specified number of pod replicas are running at any one time. In other words, a Replication Controller makes sure that a pod or a homogeneous set of pods is always up and available.\nTo maintain three copies of an nginx container\nReplication Controllers 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19  apiVersion: v1 kind: ReplicationController metadata: name: nginx spec: replicas: 3 selector: app: nginx template: metadata: name: nginx labels: app: nginx spec: containers: - name: nginx image: nginx ports: - containerPort: 80   ReplicaSet 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21  apiVersion: apps/v1beta2 kind: ReplicaSet metadata: name: nginx labels: app: nginx spec: replicas: 3 selector: matchLabels: app: nginx template: metadata: labels: app: nginx spec: containers: - name: nginx image: nginx ports: - containerPort: 80   Deployment 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21  apiVersion: apps/v1beta2 # for versions before 1.8.0 use apps/v1beta1 kind: Deployment metadata: name: nginx-deployment labels: app: nginx spec: replicas: 3 selector: matchLabels: app: nginx template: metadata: labels: app: nginx spec: containers: - name: nginx image: nginx ports: - containerPort: 80   Label Label node with colors\n1 2 3 4  kubectl label node node1-name color=black kubectl label node node2-name color=red kubectl label node node3-name color=green kubectl label node node4-name color=blue   Label all pod in default namespace by using --all\n1  kubectl label pods -n default color=white --all   Get pod/node/etc with specific label\n1  kubectl get pods -l color=white -n default   Get pod with multi labels\n1  kubectl get pods -l color=white,app=nginx   DaemonSet Deploy nginx pod on all node with\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21  apiVersion: apps/v1 kind: DaemonSet metadata: name: cthulu labels: daemon: \u0026#34;yup\u0026#34; spec: selector: matchLabels: daemon: \u0026#34;pod\u0026#34; template: metadata: labels: daemon: pod spec: tolerations: - key: node-role.kubernetes.io/master effect: NoSchedule containers: - name: cthulu-jr image: nginx   Confirm that pod is running on each node\n1 2 3 4 5  $ kubectl get pod -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE cthulu-kvbk9 1/1 Running 0 2m 10.1.33.8 juju-cfb27c-2 \u0026lt;none\u0026gt; cthulu-t7hfc 1/1 Running 0 2m 10.1.45.13 juju-cfb27c-1 \u0026lt;none\u0026gt; cthulu-x8hdf 1/1 Running 0 2m 10.1.31.9 juju-cfb27c-3 \u0026lt;none\u0026gt;   Label a Node \u0026amp; Schedule a Pod Label a Node to let you can schedule a pod on it.\n1  kubectl label node juju-cfb27c-3 deploy=here   use nodeSelector in yaml file to let a pod being deployed on the specific node\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16  apiVersion: v1 kind: Pod metadata: name: busybox namespace: default spec: containers: - name: busybox image: busybox command: - sleep - \u0026#34;300\u0026#34; imagePullPolicy: IfNotPresent restartPolicy: Always nodeSelector: deploy: here   Confirm\n1 2 3  $ kubectl get pod busybox -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE busybox 1/1 Running 0 10s 10.1.31.10 juju-cfb27c-3 \u0026lt;none\u0026gt;   Specific Schedulers Ordinarily, we don\u0026rsquo;t need to specify the scheduler\u0026rsquo;s name in the spec because everyone uses a single default one. Sometimes, however, developers need to have custom schedulers in charge of placing pods due to legacy or specialized hardware constraints.\nUse schedulerName in yaml to specific a customer scheduler\n1 2 3 4 5 6 7 8 9 10 11 12 13  apiVersion: v1 kind: Pod metadata: name: annotation-default-scheduler labels: name: multischeduler annotations: scheduledBy: custom-scheduler spec: schedulerName: custom-scheduler containers: - name: pod-container image: k8s.gcr.io/pause:2.0   Logs View the current logs of a pod\n1  kubectl logs pod-name   View the current logs of a pod interactively\n1  kubectl logs pod-name -f   Print last 10 lines of the log.\n1  kubectl logs pod-name --tail=10   Log file in master/node machine can be found at /var/log/containers directory\nNode maintenance Maintenance a node by preventing the scheduler from putting new pods on to it and evicting any existing pods. Ignore the DaemonSets \u0026ndash; those pods are only providing services to other local pods and will come back up when the node comes back up.\nIn this example I\u0026rsquo;m going to remove juju-cfb27c-2 from cluster.\n1 2 3  root@juju-cfb27c-0:~# kubectl drain juju-cfb27c-2 --ignore-daemonsets node/juju-cfb27c-2 cordoned WARNING: Ignoring DaemonSet-managed pods: cthulu-kvbk9, nginx-ingress-kubernetes-worker-controller-t6qh9   juju-cfb27c-2 is marked as \u0026ldquo;SchedulingDisabled\u0026rdquo;\n1 2 3 4 5  # kubectl get node NAME STATUS ROLES AGE VERSION juju-cfb27c-1 Ready \u0026lt;none\u0026gt; 4d v1.11.2 juju-cfb27c-2 Ready,SchedulingDisabled \u0026lt;none\u0026gt; 4d v1.11.2 juju-cfb27c-3 Ready \u0026lt;none\u0026gt; 4d v1.11.2   Now -2 node can be shutdown and do maintenance work, no pod will be schedule on it. If you create some new pods, they will only placed on juju-cfb27c-1 and -3.\nNext, when -2 node is ready to use, you can get it back with\n1 2 3 4 5 6 7  # kubectl uncordon juju-cfb27c-2 node/juju-cfb27c-2 uncordoned # kubectl get node NAME STATUS ROLES AGE VERSION juju-cfb27c-1 Ready \u0026lt;none\u0026gt; 4d v1.11.2 juju-cfb27c-2 Ready \u0026lt;none\u0026gt; 4d v1.11.2 juju-cfb27c-3 Ready \u0026lt;none\u0026gt; 4d v1.11.2   Upgrading Kubernetes Components Confirm the current version with\n1  kubectl get nodes    Upgrade kubeadm on the master node  1  sudo apt upgrade kubeadm   And confirm the version of kubeadm with\n1  kubeadm version    check the upgrade plan  1  sudo kubeadm upgrade plan    apply the upgrade plan  1  sudo kubeadm upgrade apply v1.x.x    upgrade kubelet  Before upgrade kubelet, first you need to drain the node which you want to upgrade\n1  kubectl drain NODENAME --ignore-daemonsets   Then, update kubelet manaully with\n1 2  sudo apt update sudo apt upgrade kubelet   Don\u0026rsquo;t forget to make your node avaliable after the upgrade.\n1  kubectl uncordon NODENAME   Network Inbound Node Port Requirements  Master Nodes  TCP 6443 \u0026ndash; Kubernetes API Server TCP 2379-2380 \u0026ndash; etcd server client API TCP 10250 \u0026ndash; Kubelet API TCP 10251 \u0026ndash; Kube-scheduler TCP 10252 \u0026ndash; kube-controller-manager TCP 10255 \u0026ndash; Read-only Kubelet API   Worker Nodes  TCP 10250 \u0026ndash; Kubelet API TCP 10255 \u0026ndash; Read-only Kubelet API TCP 30000-32767 \u0026ndash; Node Port Services    export pod to the internet 1  # kubectl expost deployment NAME --type=\u0026#34;NodePort\u0026#34; --port XX   Deploying a Load Balancer 1 2 3 4 5 6 7 8 9 10 11 12 13 14  Kind:Service apiVersion: v1 metadata: name: la-lb-service spec: selector: app: la-lb ports: - protocol: TCP port: 80 targetPort:9376 clusterIP: 10.0.171.223 loadBalancerIP: 78.12.23.17 type: LoadBalancer   ","permalink":"https://wenhan.blog/post/certified-kubernetes-administrator-cka-learning-note/","summary":"Install  Install docker.io  1  apt install docker.io   If you install docker with other cgroup driver, you have to make sure that docker and Kubernetes will use same cgroup driver.\n1 2 3 4 5  cat \u0026lt;\u0026lt; EOF \u0026gt;\u0026gt; /etc/docker/daemon.json { \u0026#34;exec-opts\u0026#34;: [\u0026#34;native.cgroupdriver=systemd\u0026#34;] } EOF    install apt key and source to system  1 2 3 4 5  root@kube-master:~# curl -s https://packages.","title":"Certified Kubernetes Administrator (CKA) learning note"},{"content":"Juju is a deploy tool which supports a very wide range of cloud providers, like AWS, Azure, Google Cloud Platform, MAAS and LXD. This artcle will focus on how to build an OpenStack test environment using Juju and LXD.\ninstalling LXD It is very easy to install LXD, just run below command\n1  $ sudo apt-install lxd   If you can\u0026rsquo;t find lxd package, run below command to add PPA(Personal Package Archive) to find LXD package. Then re-run the install command.\n1 2 3  $ sudo apt-add-repository ppa:ubuntu-lxc/stable $ sudo apt update $ sudo apt dist-upgrade   Configuring LXD Run below command to config lxd setting step by step\n1 2 3 4 5 6 7 8  $ sudo lxd init Do you want to configure a new storage pool (yes/no) [default=yes]? Name of the storage backend to use (dir or zfs) [default=dir]: Would you like LXD to be available over the network (yes/no) [default=no]? Do you want to configure the LXD bridge (yes/no) [default=yes]? Warning: Stopping lxd.service, but it can still be activated by: lxd.socket LXD has been successfully configured.   Installing juju Run below command to install juju\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33  $ sudo apt install juju Reading package lists... Done Building dependency tree Reading state information... Done The following additional packages will be installed: distro-info juju-2.0 Suggested packages: shunit2 juju-core The following NEW packages will be installed: distro-info juju juju-2.0 0 upgraded, 3 newly installed, 0 to remove and 0 not upgraded. Need to get 32.0 MB of archives. After this operation, 168 MB of additional disk space will be used. Do you want to continue? [Y/n] Get:1 http://archive.ubuntu.com/ubuntu xenial/main amd64 distro-info amd64 0.14build1 [20.0 kB] Get:2 http://archive.ubuntu.com/ubuntu xenial-updates/main amd64 juju-2.0 amd64 2.3.7-0ubuntu0.16.04.1 [32.0 MB] Get:3 http://archive.ubuntu.com/ubuntu xenial-updates/main amd64 juju all 2.3.7-0ubuntu0.16.04.1 [10.2 kB] Fetched 32.0 MB in 4s (6,778 kB/s) Preconfiguring packages ... Selecting previously unselected package distro-info. (Reading database ... 60478 files and directories currently installed.) Preparing to unpack .../distro-info_0.14build1_amd64.deb ... Unpacking distro-info (0.14build1) ... Selecting previously unselected package juju-2.0. Preparing to unpack .../juju-2.0_2.3.7-0ubuntu0.16.04.1_amd64.deb ... Unpacking juju-2.0 (2.3.7-0ubuntu0.16.04.1) ... Selecting previously unselected package juju. Preparing to unpack .../juju_2.3.7-0ubuntu0.16.04.1_all.deb ... Unpacking juju (2.3.7-0ubuntu0.16.04.1) ... Processing triggers for man-db (2.7.5-1) ... Setting up distro-info (0.14build1) ... Setting up juju-2.0 (2.3.7-0ubuntu0.16.04.1) ... Setting up juju (2.3.7-0ubuntu0.16.04.1) ...   Once the install is done, we can bootstrap a new controller using LXD. This means that juju will create a new LXD container for management service.\nWe can create a controller called juju-controller with below command\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20  $ juju bootstrap localhost juju-controller Since Juju 2 is being run for the first time, downloading latest cloud information. Fetching latest public cloud list... Your list of public clouds is up to date, see `juju clouds`. Creating Juju controller \u0026#34;juju-controller\u0026#34; on localhost/localhost Looking for packaged Juju agent version 2.3.7 for amd64 To configure your system to better support LXD containers, please see: https://github.com/lxc/lxd/blob/master/doc/production-setup.md Launching controller instance(s) on localhost/localhost... - juju-9ccefc-0 (arch=amd64) s) Installing Juju agent on bootstrap instance Fetching Juju GUI 2.12.3 Waiting for address Attempting to connect to 10.229.139.129:22 Connected to 10.229.139.129 Running machine configuration script... Bootstrap agent now started Contacting Juju controller at 10.229.139.129 to verify accessibility... Bootstrap complete, \u0026#34;juju-controller\u0026#34; controller now available Controller machines are in the \u0026#34;controller\u0026#34; model Initial model \u0026#34;default\u0026#34; added   After this, we can see a new LXD container is running.\n1 2 3 4 5 6  $ lxc list +---------------+---------+-----------------------+------+------------+-----------+ | NAME | STATE | IPV4 | IPV6 | TYPE | SNAPSHOTS | +---------------+---------+-----------------------+------+------------+-----------+ | juju-9ccefc-0 | RUNNING | 10.229.139.129 (eth0) | | PERSISTENT | 0 | +---------------+---------+-----------------------+------+------------+-----------+   And run juju status to check there is nothing running yet.\n1 2 3 4 5 6 7 8 9  $ juju status Model Controller Cloud/Region Version SLA default juju-controller localhost/localhost 2.3.7 unsupported App Version Status Scale Charm Store Rev OS Notes Unit Workload Agent Machine Public address Ports Message Machine State DNS Inst id Series AZ Message   Deploy minecraft server OK, now we are ready for deploy minecraft server! Run below command to tell juju you want to deploy it. and it should return immediately. However it doesn\u0026rsquo;t mean the service is ready. Run juju status to check the status.\n1 2 3 4 5 6 7 8 9 10 11 12  $ juju status Model Controller Cloud/Region Version SLA default juju-controller localhost/localhost 2.3.7 unsupported App Version Status Scale Charm Store Rev OS Notes minecraft waiting 0/1 minecraft jujucharms 3 ubuntu Unit Workload Agent Machine Public address Ports Message minecraft/0 waiting allocating 0 10.229.139.124 waiting for machine Machine State DNS Inst id Series AZ Message 0 pending 10.229.139.124 juju-72dd35-0 trusty Running   From above we can see juju is working on creating the server. And we also can check a new container is created for minecraft server, by lxc list command.\n1 2 3 4 5 6 7 8  $ lxc list +---------------+---------+-----------------------+------+------------+-----------+ | NAME | STATE | IPV4 | IPV6 | TYPE | SNAPSHOTS | +---------------+---------+-----------------------+------+------------+-----------+ | juju-72dd35-0 | RUNNING | 10.229.139.124 (eth0) | | PERSISTENT | 0 | +---------------+---------+-----------------------+------+------------+-----------+ | juju-9ccefc-0 | RUNNING | 10.229.139.129 (eth0) | | PERSISTENT | 0 | +---------------+---------+-----------------------+------+------------+-----------+   After a while, the deploying is done and the service is active.\n1 2 3 4 5 6 7 8 9 10 11 12  $ juju status Model Controller Cloud/Region Version SLA default juju-controller localhost/localhost 2.3.7 unsupported App Version Status Scale Charm Store Rev OS Notes minecraft active 1 minecraft jujucharms 3 ubuntu Unit Workload Agent Machine Public address Ports Message minecraft/0* active idle 0 10.229.139.124 25565/tcp Ready Machine State DNS Inst id Series AZ Message 0 started 10.229.139.124 juju-72dd35-0 trusty Running   Now you can start your minecraft client, and point to 10.229.139.124 on port 25565 to play with your all new minecraft server!\nIf you want to get rid of it, just run below command. Every service or server related to minecraft server will be gone.\n1  $ juju destroy-service minecraft   You can also destory juju controller and all service/server created by it. The easiest way to destory everything is as below.\n1 2 3 4 5 6 7 8 9 10 11  $ juju destroy-controller juju-controller --destroy-all-models WARNING! This command will destroy the \u0026#34;juju-controller\u0026#34; controller. This includes all machines, applications, data and other resources. Continue? (y/N):y Destroying controller Waiting for hosted model resources to be reclaimed Waiting on 1 model, 1 machine, 1 application Waiting on 1 model, 1 machine, 1 application Waiting on 1 model, 1 machine All hosted models reclaimed, cleaning up controller machines   We can confirm that all containers are gone.\n1 2 3 4  $ lxc list +------+-------+------+------+------+-----------+ | NAME | STATE | IPV4 | IPV6 | TYPE | SNAPSHOTS | +------+-------+------+------+------+-----------+   Deploy minecraft server You can deploy a more complicated environment such as OpenStack using Juju.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140  $jujudeploycs:bundle/openstack-base-55Locatedbundle\u0026#34;cs:bundle/openstack-base-55\u0026#34;Resolvingcharm:cs:ceph-mon-25Resolvingcharm:cs:ceph-osd-262Resolvingcharm:cs:ceph-radosgw-258Resolvingcharm:cs:cinder-272Resolvingcharm:cs:cinder-ceph-233Resolvingcharm:cs:glance-265Resolvingcharm:cs:keystone-281Resolvingcharm:cs:percona-cluster-266Resolvingcharm:cs:neutron-api-260Resolvingcharm:cs:neutron-gateway-252Resolvingcharm:cs:neutron-openvswitch-250Resolvingcharm:cs:nova-cloud-controller-310Resolvingcharm:cs:nova-compute-284Resolvingcharm:cs:ntp-24Resolvingcharm:cs:openstack-dashboard-259Resolvingcharm:cs:rabbitmq-server-74Executingchanges:-uploadcharmcs:ceph-mon-25forseriesbionic-deployapplicationceph-mononbionicusingcs:ceph-mon-25-setannotationsforceph-mon-uploadcharmcs:ceph-osd-262forseriesbionic-deployapplicationceph-osdonbionicusingcs:ceph-osd-262-setannotationsforceph-osd-uploadcharmcs:ceph-radosgw-258forseriesbionic-deployapplicationceph-radosgwonbionicusingcs:ceph-radosgw-258-setannotationsforceph-radosgw-uploadcharmcs:cinder-272forseriesbionic-deployapplicationcinderonbionicusingcs:cinder-272-setannotationsforcinder-uploadcharmcs:cinder-ceph-233forseriesbionic-deployapplicationcinder-cephonbionicusingcs:cinder-ceph-233-setannotationsforcinder-ceph-uploadcharmcs:glance-265forseriesbionic-deployapplicationglanceonbionicusingcs:glance-265-setannotationsforglance-uploadcharmcs:keystone-281forseriesbionic-deployapplicationkeystoneonbionicusingcs:keystone-281-setannotationsforkeystone-uploadcharmcs:percona-cluster-266forseriesbionic-deployapplicationmysqlonbionicusingcs:percona-cluster-266-setannotationsformysql-uploadcharmcs:neutron-api-260forseriesbionic-deployapplicationneutron-apionbionicusingcs:neutron-api-260-setannotationsforneutron-api-uploadcharmcs:neutron-gateway-252forseriesbionic-deployapplicationneutron-gatewayonbionicusingcs:neutron-gateway-252-setannotationsforneutron-gateway-uploadcharmcs:neutron-openvswitch-250forseriesbionic-deployapplicationneutron-openvswitchonbionicusingcs:neutron-openvswitch-250-setannotationsforneutron-openvswitch-uploadcharmcs:nova-cloud-controller-310forseriesbionic-deployapplicationnova-cloud-controlleronbionicusingcs:nova-cloud-controller-310-setannotationsfornova-cloud-controller-uploadcharmcs:nova-compute-284forseriesbionic-deployapplicationnova-computeonbionicusingcs:nova-compute-284-setannotationsfornova-compute-uploadcharmcs:ntp-24forseriesbionic-deployapplicationntponbionicusingcs:ntp-24-setannotationsforntp-uploadcharmcs:openstack-dashboard-259forseriesbionic-deployapplicationopenstack-dashboardonbionicusingcs:openstack-dashboard-259-setannotationsforopenstack-dashboard-uploadcharmcs:rabbitmq-server-74forseriesbionic-deployapplicationrabbitmq-serveronbionicusingcs:rabbitmq-server-74-setannotationsforrabbitmq-server-addnewmachine0-addnewmachine1-addnewmachine2-addnewmachine3-addrelationnova-compute:amqp-rabbitmq-server:amqp-addrelationneutron-gateway:amqp-rabbitmq-server:amqp-addrelationkeystone:shared-db-mysql:shared-db-addrelationnova-cloud-controller:identity-service-keystone:identity-service-addrelationglance:identity-service-keystone:identity-service-addrelationneutron-api:identity-service-keystone:identity-service-addrelationneutron-openvswitch:neutron-plugin-api-neutron-api:neutron-plugin-api-addrelationneutron-api:shared-db-mysql:shared-db-addrelationneutron-api:amqp-rabbitmq-server:amqp-addrelationneutron-gateway:neutron-plugin-api-neutron-api:neutron-plugin-api-addrelationglance:shared-db-mysql:shared-db-addrelationglance:amqp-rabbitmq-server:amqp-addrelationnova-cloud-controller:image-service-glance:image-service-addrelationnova-compute:image-service-glance:image-service-addrelationnova-cloud-controller:cloud-compute-nova-compute:cloud-compute-addrelationnova-cloud-controller:amqp-rabbitmq-server:amqp-addrelationnova-cloud-controller:quantum-network-service-neutron-gateway:quantum-network-service-addrelationnova-compute:neutron-plugin-neutron-openvswitch:neutron-plugin-addrelationneutron-openvswitch:amqp-rabbitmq-server:amqp-addrelationopenstack-dashboard:identity-service-keystone:identity-service-addrelationnova-cloud-controller:shared-db-mysql:shared-db-addrelationnova-cloud-controller:neutron-api-neutron-api:neutron-api-addrelationcinder:image-service-glance:image-service-addrelationcinder:amqp-rabbitmq-server:amqp-addrelationcinder:identity-service-keystone:identity-service-addrelationcinder:cinder-volume-service-nova-cloud-controller:cinder-volume-service-addrelationcinder-ceph:storage-backend-cinder:storage-backend-addrelationceph-mon:client-nova-compute:ceph-addrelationnova-compute:ceph-access-cinder-ceph:ceph-access-addrelationcinder:shared-db-mysql:shared-db-addrelationceph-mon:client-cinder-ceph:ceph-addrelationceph-mon:client-glance:ceph-addrelationceph-osd:mon-ceph-mon:osd-addrelationntp:juju-info-nova-compute:juju-info-addrelationntp:juju-info-neutron-gateway:juju-info-addrelationceph-radosgw:mon-ceph-mon:radosgw-addrelationceph-radosgw:identity-service-keystone:identity-service-addunitceph-osd/0tonewmachine1-addunitceph-osd/1tonewmachine2-addunitceph-osd/2tonewmachine3-addunitneutron-gateway/0tonewmachine0-addunitnova-compute/0tonewmachine1-addunitnova-compute/1tonewmachine2-addunitnova-compute/2tonewmachine3-addlxdcontainer1/lxd/0onnewmachine1-addlxdcontainer2/lxd/0onnewmachine2-addlxdcontainer3/lxd/0onnewmachine3-addlxdcontainer0/lxd/0onnewmachine0-addlxdcontainer1/lxd/1onnewmachine1-addlxdcontainer2/lxd/1onnewmachine2-addlxdcontainer3/lxd/1onnewmachine3-addlxdcontainer0/lxd/1onnewmachine0-addlxdcontainer1/lxd/2onnewmachine1-addlxdcontainer2/lxd/2onnewmachine2-addlxdcontainer3/lxd/2onnewmachine3-addlxdcontainer0/lxd/2onnewmachine0-addunitceph-mon/0to1/lxd/0-addunitceph-mon/1to2/lxd/0-addunitceph-mon/2to3/lxd/0-addunitceph-radosgw/0to0/lxd/0-addunitcinder/0to1/lxd/1-addunitglance/0to2/lxd/1-addunitkeystone/0to3/lxd/1-addunitmysql/0to0/lxd/1-addunitneutron-api/0to1/lxd/2-addunitnova-cloud-controller/0to2/lxd/2-addunitopenstack-dashboard/0to3/lxd/2-addunitrabbitmq-server/0to0/lxd/2Deployofbundlecompleted.  ","permalink":"https://wenhan.blog/post/juju-use-juju-to-deploy-minecraft-server-in-lxd/","summary":"Juju is a deploy tool which supports a very wide range of cloud providers, like AWS, Azure, Google Cloud Platform, MAAS and LXD. This artcle will focus on how to build an OpenStack test environment using Juju and LXD.\ninstalling LXD It is very easy to install LXD, just run below command\n1  $ sudo apt-install lxd   If you can\u0026rsquo;t find lxd package, run below command to add PPA(Personal Package Archive) to find LXD package.","title":"[Juju] Use juju to deploy minecraft server in LXD"},{"content":"S3 Suspend not supported by default There is an issue about suspend on Thinkpad X1 Carbon when using Ubuntu 18.04. When you close the lid suspend does not works well. It will continue cose some power and get you laptop hot.\nThe root cause is the 6th gen X1 Carbon supports S0i3(Which is also known as Windows Modern Standby) but does not support the S3 sleep state.\nS0i3 sleep support After some researching, the workaround can be this:\n add below kernel parameter to enable s0i3 sleep support This disables wakeup/resume via lid open  1  acpi.ec_no_wakeup=1    enable Thunderbolt BIOS Assist Mode in BIOS configure It is in Config -\u0026gt; Thunderbolt BIOS Assist Mode - Set to \u0026quot;Enabled\u0026quot;\n  disable SD card reader\n  More details about this issue can be found at\nX1 Carbon Gen 6 cannot enter deep sleep (S3 state aka Suspend-to-RAM) on Linux Suspend issues X1 Carbon 6th gen S0i3 sleep broken\nI will test this work around later and update result.\nAnother workaround is at https://delta-xi.net/#056, but I haven\u0026rsquo;t test for it.\nTest Update: Over 8 hours sleep, battery only costed from 99% to 91%, which I think is OK. Also the temperature is low, so I think the workaround is useful.\n","permalink":"https://wenhan.blog/post/suspend-issue-on-thinkpad-x1c-6th-with-ubuntu-18-04/","summary":"S3 Suspend not supported by default There is an issue about suspend on Thinkpad X1 Carbon when using Ubuntu 18.04. When you close the lid suspend does not works well. It will continue cose some power and get you laptop hot.\nThe root cause is the 6th gen X1 Carbon supports S0i3(Which is also known as Windows Modern Standby) but does not support the S3 sleep state.\nS0i3 sleep support After some researching, the workaround can be this:","title":"Suspend issue on Thinkpad X1C 6th with Ubuntu 18.04"},{"content":"I follow below URL to try to install and test MaaS,\nhttps://docs.maas.io/2.1/en/installconfig-lxd-install\nAt the profile edit part, from above documents\n lxc profile edit maas replace the {} after config with the following (excluding config:): 1 2 3 4 5 6  config: raw.lxc: |- lxc.cgroup.devices.allow = c 10:237 rwm lxc.aa_profile = unconfined lxc.cgroup.devices.allow = b 7:* rwm security.privileged: \u0026#34;true\u0026#34;    At the launch step, I hit below issue, 1 2 3  $ lxc launch -p maas ubuntu:16.04 xenial-maas Creating xenial-maas Error: Failed container creation: Failed to load raw.lxc     This is because I\u0026rsquo;m using LXD 3.0, and the configuration key above is old. Depending on this comment, lxc.aa_profile is changed to lxc.apparmor.profile from lxd 2.1.\nSo the workaround is\n lxc profile edit maas again replace the lxc.aa_profile with lxc.apparmor.profile 1 2 3 4 5 6  config: raw.lxc: |- lxc.cgroup.devices.allow = c 10:237 rwm lxc.apparmor.profile = unconfined lxc.cgroup.devices.allow = b 7:* rwm security.privileged: \u0026#34;true\u0026#34;    re-run the launch command 1 2 3  $ lxc launch -p maas ubuntu:16.04 xenial-maas Creating xenial-maas Starting xenial-maas     ","permalink":"https://wenhan.blog/post/error-failed-container-creation-failed-to-load-raw-lxc/","summary":"I follow below URL to try to install and test MaaS,\nhttps://docs.maas.io/2.1/en/installconfig-lxd-install\nAt the profile edit part, from above documents\n lxc profile edit maas replace the {} after config with the following (excluding config:): 1 2 3 4 5 6  config: raw.lxc: |- lxc.cgroup.devices.allow = c 10:237 rwm lxc.aa_profile = unconfined lxc.cgroup.devices.allow = b 7:* rwm security.privileged: \u0026#34;true\u0026#34;    At the launch step, I hit below issue, 1 2 3  $ lxc launch -p maas ubuntu:16.","title":"Error: Failed container creation: Failed to load raw.lxc"},{"content":"こちらの記事はではAnsibleを紹介するための文章で、元Red Hat社員のJingjing Shiが作成し、Wenhan Shiが日本語に翻訳しました。内容の校正はHideki SaitoとKento Yagisawaが協力しています。Ansibleの基礎知識から、実際の現場で利用できる実運用まで紹介しています。\n本文内にあるすべてのansible playbookの例は、以下のgithubのURLから利用できる。 https://github.com/ansible-book/ansible-first-book-examples もし不備やコメントがありましたら、作者shijingjing02@163.comまたは翻訳者shibunkan@gmail.com に連絡してください。\n詳細の内容を下記三つに分けて説明します。\nAnsible 入門 - 紹介 Ansible 入門 - 基本 Ansible 入門 - 応用\n本章では、簡単な例を使ってAnsibleの基本的な使い方を説明する。\n インストール 管理対象のサーバー（サーバリストの管理） コマンドラインによるサーバー管理(Ad-hoc command) スクリプトによるサーバー管理(スクリプト言語 Playbook) モジュール  インストール ここでは Red Hat 系 Linux でのインストールを前提に解説にする。他のOSに関していAnsibleのWebページをご参照ください。\n管理者ノード Ansible パッケージをインストール 1 2 3 4  # Redhat/CentOS Linuxの場合, epolリポジトリをインストールする必要がある。 # Fedoraの場合、デフォルトのリポジトリにAnsibleを含まれているため，直接インストールできる sudo yum install epel-release sudo yum install ansible -y   管理者ノードからリモートノードの接続設定 SSH keyによる認証パスワードレス）方式を設定する。\n1 2 3 4 5 6  # ssh key を生成 ssh-keygen # リモートノードにssh keyをコピー ssh-copy-id remoteuser@remoteserver # リモートノードをknows_hostsに追加（ssh Keyの保存確認がなくなる） ssh-keyscan remote_servers \u0026gt;\u0026gt; ~/.ssh/known_hosts   設定確認 管理者ノードで下記コマンドを実行し、パスワードの入力とSSH keyの保存確認がなかったら設定完了。\n1  ssh remoteuser@remoteserver   リモートノード Python 2.4以上が必要だが、通常はデフォルトでインストールされている。そのため、別途パッケージのインストールは必要ない。\n管理対象のサーバー（サーバーリストの管理） サーバリスト（Host Inventory）とは Host InventoryはAnsibleの設定ファイルであり、管理対象となるリモートノードの一覧をAnsibleに教える。 また、下記のように状況に応じてリモートノードのカテゴリ化もできる。 使い道によってカテゴリ化する：データベースノード、サービスノード。 ロケーションによってカテゴリ化する：中部データセンター、西部データセンター\nHost Inventoryファイル デフォルトのファイルパス： /etc/ansible/hosts\n他のファイルパスへ変更することもできる、詳細については後程紹介する。\n例：\n一番シンプルのhosts ファイル\n1 2 3  192.168.1.50aserver.example.orgbserver.example.org  カテゴリを持つhosts ファイル\n1 2 3 4 5 6 7 8 9 10  mail.example.com[webservers]foo.example.combar.example.com[dbservers]one.example.comtwo.example.comthree.example.com  コマンドラインによるサーバー管理(Ad-hoc command) Ansibleではコマンドラインツールを提供していて、オフィシャルドキュメントでは Ad-Hoc Commandsと名付けている。ansibleコマンドのフォーマットを以下に示す。\n1  ansible \u0026lt;host-pattern\u0026gt; [options]   Ansible コマンドができること コマンドの構文の詳細を置いといて、\u0026ldquo;命令”モジュールの説明が終わったら構文の理解が深まりにいく。ここでは、下記のコマンドの例を通して、ansibleコマンドの役割を体感しましょう\n環境を確認する ansible管理者ノードからユーザ bruce で各リモートノードをアクセスすることを確認する。\n1  ansible all -m ping -u bruce   コマンド実行 現在bash ユーザと同名のユーザがすべてのリモートノードに対し\u0026quot;echo”コマンドを実行する。\n1  ansible all -a \u0026#34;/bin/echo hello\u0026#34;   ファイルコピー /etc/hosts ファイル を全リモートノードのweb グループにコピーし、コピー先は/tmp/hosts\n1  ansible web -m copy -a \u0026#34;src=/etc/hosts dest=/tmp/hosts\u0026#34;   パッケージインストール 全リモートノードのweb グループのマシンにyum でacmeパッケージをインストール\n1  ansible web -m yum -a \u0026#34;name=acme state=present\u0026#34;   ユーザ追加 1  ansible all -m user -a \u0026#34;name=foo password=\u0026lt;crypted password here\u0026gt;\u0026#34;   パッケージダウンロード 1  ansible web -m git -a \u0026#34;repo=git://foo.example.org/repo.git dest=/srv/myapp version=HEAD\u0026#34;   サービス起動 1  ansible web -m service -a \u0026#34;name=httpd state=started\u0026#34;   並列実行 リブート命令を10並列で実行する。\n1  ansible lb -a \u0026#34;/sbin/reboot\u0026#34; -f 10   リモートノード情報取得 1  ansible all -m setup   スクリプトによるサーバー管理 重複な入力を避けるため、Ansibleをスクリプトの実行に対応する。AnsibleのスクリプトはPlaybookと読んで、YAML形式で、拡張子はymlである。Note: YAMLとJSONは似ていて、データを表示するフォーマットである。\nPlaybookの実行方法 1  ansible-playbook deploy.yml   Playbookの例 Playbook deploy.ymlを通して、webグループのリモートサーバに対しapacheをデプロイする。ステップとしては\n Apacheパッケージをインストール 設定ファイルhttpdをコピーし、コピー完了後apacheサービスを再起動することを保証 デフォルトweb pageのindex.htmlをコピー Apacheサービスを起動  このPlaybook deploy.yml 内では以下のキーワードを含まれている。\n   キーワード 内容 備考     hosts リモートノードのIP、またはグループ名、またはキーワードall    remote_user 実行ユーザ    vars 変数    tasks[^1] Playbookのコア部分、処理内容actionを順番通りに実行する。各actionはansible moduleを利用する。 action の構文：module: module_parameter=module_value。   handers Playbookのイベントで、デフォルトでは実行されず、action内でトリガーの条件に満足したら実行する。複数のトリガー条件にマッチしても1回のみ実行する。    [^1]よく利用するモジュールはyum, copy, templateなど。Ansibleの中のmodule は、bash の中のyum, copyの命令に似ている。詳細は後程紹介する。      以下はdeploy.ymlの内容を示す。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23  ---- hosts:webvars:http_port:80max_clients:200remote_user:roottasks:- name:ensure apache is at the latest versionyum:pkg=httpd state=latest- name:Write the configuration filetemplate:src=templates/httpd.conf.j2 dest=/etc/httpd/conf/httpd.confnotify:- restart apache- name:Write the default index.html filetemplate:src=templates/index.html.j2 dest=/var/www/html/index.html- name:ensure apache is runningservice:name=httpd state=startedhandlers:- name:restart apacheservice:name=httpd state=restarted  YAMLが分からなかったら、上記deploy.ymlをJSON形式に変換することも可能だ。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37  [ { \u0026#34;hosts\u0026#34;: \u0026#34;web\u0026#34;, \u0026#34;vars\u0026#34;: { \u0026#34;http_port\u0026#34;: 80, \u0026#34;max_clients\u0026#34;: 200 }, \u0026#34;remote_user\u0026#34;: \u0026#34;root\u0026#34;, \u0026#34;tasks\u0026#34;: [ { \u0026#34;name\u0026#34;: \u0026#34;ensure apache is at the latest version\u0026#34;, \u0026#34;yum\u0026#34;: \u0026#34;pkg=httpd state=latest\u0026#34; }, { \u0026#34;name\u0026#34;: \u0026#34;Write the configuration file\u0026#34;, \u0026#34;template\u0026#34;: \u0026#34;src=templates/httpd.conf.j2 dest=/etc/httpd/conf/httpd.conf\u0026#34;, \u0026#34;notify\u0026#34;: [ \u0026#34;restart apache\u0026#34; ] }, { \u0026#34;name\u0026#34;: \u0026#34;Write the default index.html file\u0026#34;, \u0026#34;template\u0026#34;: \u0026#34;src=templates/index.html.j2 dest=/var/www/html/index.html\u0026#34; }, { \u0026#34;name\u0026#34;: \u0026#34;ensure apache is running\u0026#34;, \u0026#34;service\u0026#34;: \u0026#34;name=httpd state=started\u0026#34; } ], \u0026#34;handlers\u0026#34;: [ { \u0026#34;name\u0026#34;: \u0026#34;restart apache\u0026#34;, \u0026#34;service\u0026#34;: \u0026#34;name=httpd state=restarted\u0026#34; } ] } ]   JSON と YAMLをお互いに変換するオンラインツール：http://www.json2yaml.com/\nPlay vs. Playbook Playbookは Ansibleで実行可能な YAMLファイルである。一般的な構文を以下の例に示す。\n1 2 3 4 5 6  ---- hosts:webremote_user:roottasks:- name:ensure apache is at the latest versionyum:pkg=httpd state=latest  一つのPlaybookファイルの中で、二つのリモートノードグループに対しそれぞれ異る操作を存在することができる。例えば、webノードにhttpdパッケージのインストールと、lbノードにmysqlパッケージのインストール処理を一つのPlaybook ファイルに設定することができる。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14  ---# apache をインストールのplay- hosts:webremote_user:roottasks:- name:ensure apache is at the latest versionyum:pkg=httpd state=latest# mysql をインストールのplay- hosts:lbremote_user:roottasks:- name:ensure mysqld is at the latest versionyum:pkg=mariadb state=latest  上記の例では、一つのサーバに対する操作処理は、一つのPlayになる。一般的には一つのPlaybookでは一つのPlayしか実行しないため、その場合にはPlaybookとPlayは同じである。\nAnsibleのモジュール Ansibleのモジュールとは Bashを利用する時、コマンドラインでもスクリプト内でも、cd, ls, copy, yumなどの命令を実行する必要がある。AnsibleのモジュールはAnsibleの命令であり、Ansibleのコマンドラインやスクリプト上にて実行されている。よく使うモジュールとしてyum, copy, templateなどがある。\nBashでのコマンドを利用するときに、さまざまなオプションが利用できて、このオプションはコマンドごとに定義されている。それと同じように、Ansibleのモジュールを実行する時に複数のオプションをつけることができて、各モジュールのオプションはモジュール内に定義されている。\nモジュールの利用方法は以下のドキュメントを参照で http://docs.ansible.com/ansible/modules_by_category.html\nコマンドラインにてAnsibleモジュールを利用 Ansible コマンドラインでは、以下の様にモジュールを利用できる。\n1 2 3  -m \u0026lt;モジュール名\u0026gt; -a モジュールのパラメータ   例えば\n1 2 3 4  # module copyを利用し、管理者ノードの/etc/hostsをリモートノードの/tmp/hostsにコピー ansible all -m copy -a \u0026#34;src=/etc/hosts dest=/tmp/hosts\u0026#34; # module yumを利用しリモートノードweb上にhttpdパッケージをインストール ansible web -m yum -a \u0026#34;name=httpd state=present\u0026#34;   PlaybookにてAnsibleモジュールを利用 Playbookスクリプトでは、task内一つのactionは一つのモジュールを実行している。各actionに対し、\n: の前はモジュールの名前\n: の後ろはモジュールのパラメータ\n1 2 3 4 5 6 7 8  ---tasks:- name:ensure apache is at the latest versionyum:pkg=httpd state=latest- name:write the apache config filetemplate:src=templates/httpd.conf.j2 dest=/etc/httpd/conf/httpd.conf- name:ensure apache is runningservice:name=httpd state=started  Ansibleモジュールの特性  Linuxのコマンドのように、Ansibleのモジュールはコマンドライン上で実行することもできるし、Playbook内に書いて実行することもできる。 各モジュールのパラメータや状態の判断は、このモジュールの仕様によって決められている。そのため、モジュールを利用する前に、このモジュールのドキュメントを参照することが必要である。  オンラインドキュメントの参照は：http://docs.ansible.com/ansible/list_of_all_modules.html ansible-doc コマンドからモジュールの仕様が確認できる。   Ansibleはよく利用するモジュールを多数公開していますが、APIを公開しているので、ユーザが自分のモジュールの開発もできる。AnsibleのモジュールはPythonで書かれている。  よく使うAnsibleモジュールの紹介 Linuxを利用する時、コマンド命令が分からないとLinux 利用することはできない。これと同じようにAnsibleを利用するために基本のモジュールを理解することが大事である。\nこれからは普段よく使われているモジュールを紹介する。\n  テスト＆確認用\n ping: リモートノードにpingして、正常に接続ができる場合は\u0026quot;pong”を返す。 debug: デバッグ用、情報を出力のみ、Linuxのecho コマンドに似ている。    ファイル操作\n copy: ローカルのファイルをリモートノードにコピー template: ローカルのファイルをリモートノードにコピーし、変数を変更 file: ファイルのパーミッションや属性を設定    システム操作\n user: ユーザアカウント管理 yum: red hat 系Linuxのパッケージ管理 service: サービスの管理 firewalld: ファイアウォール内のサービスやポートの管理    Shell命令実行\n shell: リモートノード上shell命令を実行、$HOMEや\u0026rdquo;\u0026lt;”, ”\u0026gt;”, \u0026ldquo;|”と“\u0026amp;”などは利用可能 command: リモートノード上shell命令を実行、$HOMEや\u0026rdquo;\u0026lt;”, ”\u0026gt;”, \u0026ldquo;|”と“\u0026amp;”などは利用不可    ping 管理者ノードからリモートノードへの接続状態を確認するときに一番よく使われているモジュールである。ただShellのpingコマンドみたいにただリモートノードを\u0026quot;ping”するだけではなく、パスワードレスのSSHログインと、リモートノードのpythonのversionを確認し、両方問題なかったらpongを返す。\npingモジュールを使う時にパラメータが必要ない。リモートノードの接続状況を確認するためコマンドラインでの利用がplaybookの中より多くなっている。下記はコマンドラインでのpingモジュールの例を示す。\n1  ansible servers -m ping   debug Shellのechoコマンドに似ていて。メッセージを出力する。 msgパラメータに出力したいメッセージ内容を設定する。下の例では、システム情報をメッセージ内容に含めて出力している。Ansibleが実行する前にシステム情報を集めて定数にしているため、playbook内に定義しなくても利用できる。\n1 2  - debug:msg:\u0026#34;System \\{\\{ inventory_hostname \\}\\} has gateway \\{\\{ ansible_default_ipv4.gateway \\}\\}\u0026#34;  実行結果\n1 2 3 4  TASK [debug] ******************************************************************* ok: [localhost] =\u0026gt; { \u0026#34;msg\u0026#34;: \u0026#34;System localhost has gateway 192.168.50.1\u0026#34; }   出力したい情報をvar パラメータを定義し出力させることができる。情報はシステム情報でも、モジュール実行結果を取り込むことができ、キーワードregesterを用いて変数に設定する。 例えば、システム情報を出力するため、以下の内容をPlaybookに記入する。\n1 2 3  - name:Display all variables/facts known for a hostdebug:var:hostvars[inventory_hostname][\u0026#34;ansible_default_ipv4\u0026#34;][\u0026#34;gateway\u0026#34;]  実行結果\n1 2 3 4  TASK [Display part of variables/facts known for a host] ************************ ok: [localhost] =\u0026gt; { \u0026#34;hostvars[inventory_hostname][\\\u0026#34;ansible_default_ipv4\\\u0026#34;][\\\u0026#34;gateway\\\u0026#34;]\u0026#34;: \u0026#34;192.168.50.1\u0026#34; }   また、モジュールの実行結果など動的な情報を出力するためのPlaybook内容を以下に示す。\n1 2 3 4 5  - shell:/usr/bin/uptimeregister:result- debug:var:result  実行結果\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21  TASK [command] ***************************************************************** changed: [localhost] TASK [debug] ******************************************************************* ok: [localhost] =\u0026gt; { \u0026#34;result\u0026#34;: { \u0026#34;changed\u0026#34;: true, \u0026#34;cmd\u0026#34;: \u0026#34;/usr/bin/uptime\u0026#34;, \u0026#34;delta\u0026#34;: \u0026#34;0:00:00.003212\u0026#34;, \u0026#34;end\u0026#34;: \u0026#34;2017-01-01 21:30:02.817443\u0026#34;, \u0026#34;rc\u0026#34;: 0, \u0026#34;start\u0026#34;: \u0026#34;2017-01-01 21:30:02.814231\u0026#34;, \u0026#34;stderr\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;stdout\u0026#34;: \u0026#34; 21:30:02 up 12:38, 8 users, load average: 1.13, 1.31, 1.14\u0026#34;, \u0026#34;stdout_lines\u0026#34;: [ \u0026#34; 21:30:02 up 12:38, 8 users, load average: 1.13, 1.31, 1.14\u0026#34; ], \u0026#34;warnings\u0026#34;: [] } }   copy ローカルマシン上のファイルをリモートノードにコピーし、適切なファイルパーミッションを設定する。注意すべきなのは、ファイルをコピーする前に、コピー元とコピー先のファイルのchecksumを比較する。同一の場合はコピーせず、OK状態を返す。異なっている場合のみコピーを実行し、changed状態を返す。\nファイルパーミッション設定 modeパラメータを使ってパーミッションを設定する。設定の方式は数字でも、符号形式\u0026quot;u=rw,g=r,o=r”, ”u+rw,g-wx,o-rwx” でも問題ない。\n1 2 3 4 5 6  - copy:src:/srv/myfiles/foo.confdest:/etc/foo.confowner:foogroup:foomode:0644  リモートノード上ファイルのバックアップ backupパラメータがyesになっている時に、コピーする前にリモートノード上のファイルのバックアップを取るようになる。もしコピー元＆先のファイルが同一であれば、コピー処理が実行されず、バックアップも実行されなくなる。\n1 2 3 4  - copy:src:sudoersdest:/tmpbackup:yes  コピー後の検証 validate パラメータに検証の為のコマンドを設定する。通常の場合、検証が必要なのはコピー後のファイルなので、%s で指定することができる。copyモジュールにvalidate変数が存在する場合は、コピー処理が正常に終了したことに加えて、validataの命令も正常と返す時のみ、copyモジュールの実行が成功になる。\n以下の例では、visudo -cf /etc/sudoers は sudoers ファイルを検証するための命令である。\n1 2 3 4  - copy:src:/mine/sudoersdest:/etc/sudoersvalidate:\u0026#39;visudo -cf %s\u0026#39;  template 普通のファイルをコピーする場合は、copyモジュールで充分だが、コピー先ファイルの内容を動的に変更したい場合、templateモジュールを利用する必要がある。 例えば、apacheをインストールした後、テストのためリモートノードにindex.htmlファイルをコピーし、リモートノードのホスト名とIPアドレスを表示させたい時、templateを利用したほうがいい。 Index.html内、変更したい部分を変数として記入する。templateはpythonのj2テンプレートを利用している。j2を理解する必要がないが、変数の書き方は\\{\\{ \\}\\}で囲むことを分かれば問題がない。\ntemplateファイルの構文 Templateなので、可読性のため j2 拡張子をファイルにつける。下記の index.html.j2 ファイルに二つの変数ansible_hostname と ansible_default_ipv4.address が書かれている。\n1 2 3 4 5 6 7 8 9 10 11  \u0026lt;html\u0026gt; \u0026lt;title\u0026gt;Demo\u0026lt;/title\u0026gt; \u0026lt;body\u0026gt; \u0026lt;div class=\u0026#34;block\u0026#34; style=\u0026#34;height: 99%;\u0026#34;\u0026gt; \u0026lt;div class=\u0026#34;centered\u0026#34;\u0026gt; \u0026lt;h1\u0026gt;#46 Demo\u0026lt;/h1\u0026gt; \u0026lt;p\u0026gt;Served by \\{\\{ ansible_hostname \\}\\} (\\{\\{ ansible_default_ipv4.address \\}\\}).\u0026lt;/p\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt;   facts変数を利用するtemplate index.html.j2で使われている変数ansible_hostnameとansible_default_ipv4.addressはfacts変数であり、ansibleはその内容を調べ、直接Playbookで利用することができるし、templateで利用することもできる。そのため、templateにこの変数を記入した場合は特にパラメータを入力する必要はない。\n1 2  - name:Write the default index.html filetemplate:src=templates/index.html.j2 dest=/var/www/html/index.html  一般変数を利用するtemplate 例えば、httpd.conf.j2をリモートノードにコピーした後、デフォルトのhttpポートを設定したい場合、templateの一般変数を利用して実現できる。templateファイル httpd.conf.j2の中での一般変数の利用方法は同じく\\{\\{\\}\\}:\n1 2 3 4  ServerRoot \u0026#34;/etc/httpd\u0026#34;...Listen \\{\\{ http_port \\}\\}...  一般変数はtemplateを読み込む時ではなく、Playbook内のvars キーワードで定義される。もちろんPlaybook内で直接利用できる変数は、template内でも利用できる。inventory内の変数の定義については後章で説明する。\n1 2 3 4 5 6 7 8  - hosts:localhostvars:http_port:8080remote_user:roottasks:- name:Write the configuration filetemplate:src=templates/httpd.conf.j2 dest=/etc/httpd/conf/httpd.conf  templateモジュールはcopyモジュールと同じく、ファイルをリモートノードにコピーするだけではなく、パーミッションの設定、バックアップ、検証機能などもできる。\n1 2 3 4 5 6 7 8  - template:src:etc/ssh/sshd_config.j2dest:/etc/ssh/sshd_config.j2owner:rootgroup:rootmode:\u0026#39;0600\u0026#39;validate:/usr/sbin/sshd -t %sbackup:yes  file fileモジュールは、リモートノード上のファイル、シンボリック、フォルダの作成、削除、またはパーミションの設定を行う。\nファイルのパーミッションの変更 mode変数には直接数字(頭1文字は 0)でも、パーミッション設定内容でも、パーミッションの追加/削除でも設定することができます。詳細については以下の例で示す。\n1 2 3 4 5 6 7  - file:path:/etc/foo.confowner:foogroup:foomode:0644#mode: \u0026#34;u=rw,g=r,o=r\u0026#34;#mode: \u0026#34;u+rw,g-wx,o-rwx\u0026#34;  シンボリックリンク作成 ここで注意すべきところは、srcとdestパラメータの意味はcopyモジュールと異なり、fileモジュールの操作対象のファイルは全てリモートノード上にある。\n1 2 3 4 5 6  - file:src:/file/to/link/todest:/path/to/symlinkowner:foogroup:foostate:link  新しいファイル作成 Touchコマンドのように新しいファイルを作成\n1 2 3 4  - file:path:/etc/foo.confstate:touchmode:\u0026#34;u=rw,g=r,o=r\u0026#34;  新しいフォルダを作成\n1 2 3 4 5  # create a directory if it doesn\u0026#39;t exist- file:path:/etc/some_directorystate:directorymode:0755  user ユーザモジュールはリモートノードのユーザアカウントを追加／削除／変更することができ、アカウントの属性を設定することもできる。\nアカウント追加 ユーザアカウントjohndを追加、uid を 1040に、primary groupをadminに設定する。\n1 2 3 4 5  - user:name:johndcomment:\u0026#34;John Doe\u0026#34;uid:1040group:admin  ユーザアカウントjamesを作成し、このアカウントに二つのグループを追加する。\n1 2 3 4 5  - user:name:jamesshell:/bin/bashgroups:admins,developersappend:yes  アカウント削除 ユーザアカウントjohndを削除する。\n1 2 3 4  - user:name:johndstate:absentremove:yes  アカウントの属性を変更 アカウントjsmithに対し2048-bit のSSH keyを作成し、~jsmith/.ssh/id_rsaに保存する。\n1 2 3 4 5  - user:name:jsmithgenerate_ssh_key:yesssh_key_bits:2048ssh_key_file:.ssh/id_rsa  アカウントの無効時刻を追加\n1 2 3 4 5  - user:name:james18shell:/bin/zshgroups:developersexpires:1422403387  yum yum モジュールはRed Hat系Linuxのパッケージを管理する。RHEL, CentOS, fedora 21以下のOSをサポートする。fedora 22以上からはdnfを利用するため、dnfモジュールの利用をお勧めする。\nパッケージのinstallとremove 最新のパッケージをインストール。もし古いパッケージが存在する場合、最新versionにアップデートする。\n1 2 3 4  - name:install the latest version of Apacheyum:name:httpdstate:latest  指定したversionのパッケージをinstall\n1 2 3 4  - name:install one specific version of Apacheyum:name:httpd-2.2.29-1.4.amzn1state:present  httpdパッケージをremove\n1 2 3 4  - name:remove the Apache packageyum:name:httpdstate:absent  指定したリポジトリtestingからパッケージをinstall\n1 2 3 4 5  - name:install the latest version of Apache from the testing repoyum:name:httpdenablerepo:testingstate:present  パッケージグループをinstall\n1 2 3 4 5 6 7 8 9  - name:install the \u0026#39;Development tools\u0026#39; package groupyum:name:\u0026#34;@Development tools\u0026#34;state:present- name:install the \u0026#39;Gnome desktop\u0026#39; environment groupyum:name:\u0026#34;@^gnome-desktop-environment\u0026#34;state:present  ローカルファイルからパッケージをinstall\n1 2 3 4  - name:install nginx rpm from a local fileyum:name:/usr/local/src/nginx-release-centos-6-0.el6.ngx.noarch.rpmstate:present  URLからパッケージをinstall\n1 2 3 4  - name:install the nginx rpm from a remote repoyum:name:http://nginx.org/packages/centos/6/noarch/RPMS/nginx-release-centos-6-0.el6.ngx.noarch.rpmstate:present  service リモートノード上のサービスを管理する。一般的によく利用されるサービスは、httpd, sshd, nfs, crondなどがある。\n起動/停止/再起動 httpdサービスを起動\n1 2 3  - service:name:httpdstate:started  httpdサービスを停止\n1 2 3  - service:name:httpdstate:stopped  httpdサービスを再起動\n1 2 3  - service:name:httpdstate:restarted  httpdサービスをreload\n1 2 3  - service:name:httpdstate:reloaded  httpdサービスをブート時自動起動に設定\n1 2 3  - service:name:httpdenabled:yes  ネットワークサービスのポートを起動する\n1 2 3 4  - service:name:networkstate:restartedargs:eth0  firewalld firewalldモジュールは、サービスやポートに対しfirewalldのルールを設定する。実行中のルールと、永久のルールの両方が設定できる。ただし、firewalldモジュールの条件として、リモートノードのfirewalldのversionは0.2.11以上である必要がある。\nサービスに対しfirewalldルールを追加 1 2 3 4 5 6 7 8 9 10  - firewalld:service:httpspermanent:truestate:enabled- firewalld:zone:dmzservice:httppermanent:truestate:enabled  ポートに対しfirewalldルールを追加 1 2 3 4 5 6 7 8 9  - firewalld:port:8081/tcppermanent:truestate:disabled- firewalld:port:161-162/udppermanent:truestate:enabled  その他のfirewalldルールの例 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21  - firewalld:rich_rule:\u0026#39;rule service name=\u0026#34;ftp\u0026#34; audit limit value=\u0026#34;1/m\u0026#34; accept\u0026#39;permanent:truestate:enabled- firewalld:source:192.0.2.0/24zone:internalstate:enabled- firewalld:zone:trustedinterface:eth2permanent:truestate:enabled- firewalld:masquerade:yesstate:enabledpermanent:truezone:dmz  shell shellモジュールは/bin/shを使ってリモートノードで命令を実行する。 もし命令はyumやcopyモジュールで実現できるなら、shellまたはcommandのような命令モジュールが必要なくなる。 また、shellモジュールは操作後の戻り値やstatusをチェックしていないため、実行する必要がない場合でも、もう一回実行される。\n$home, $HOME, \u0026ldquo;\u0026lt;”, \u0026ldquo;\u0026gt;”, “|”, “;” と“\u0026amp;”が利用できる。\n $home  1 2  - name:test $homeshell:echo \u0026#34;Test1\u0026#34; \u0026gt; ~/tmp/test1   \u0026amp;\u0026amp;  1  - shell:service jboss start \u0026amp;\u0026amp; chkconfig jboss on   \u0026gt;\u0026gt;  1  - shell:echo foo \u0026gt;\u0026gt; /tmp/testfoo  スクリプト実行 1  - shell:somescript.sh \u0026gt;\u0026gt; somelog.txt  命令実行前にカレントディレクトリを変更\n1 2 3  - shell:somescript.sh \u0026gt;\u0026gt; somelog.txtargs:chdir:somedir/  命令実行前にカレントディレクトリを変更し、somelog.txtが存在しない時にのみactionを実行する。\n1 2 3 4  - shell:somescript.sh \u0026gt;\u0026gt; somelog.txtargs:chdir:somedir/creates:somelog.txt  Bashを指定し命令を実行する\n1 2 3  - shell:cat \u0026lt; /tmp/\\*txtargs:executable:/bin/bash  Command Shellモジュールと似ていて、リモートノードで命令を実行する。しかし、commandモジュールは $HOME または \u0026ldquo;\u0026lt;\u0026rdquo;, “\u0026gt;”, “|”, “;” and “\u0026amp;”を利用できない\nShell と似た部分  一つの命令を実行する  1  - command:/sbin/shutdown -t now   命令を実行する前にカレントディレクトリを変更し、databaseファイルがない時に命令を実行  1 2 3 4  - command:/usr/bin/make_database.sh arg1 arg2args:chdir:somedir/creates:/path/to/database  Shell と異る部分  パラメータを渡す方法はShellより一つ多い  1  - command:/usr/bin/make_database.sh arg1 arg2 creates=/path/to/database   \u0026amp;\u0026amp; または \u0026raquo; は利用できない。  下記の書き方で、~/tmp/test3と~/tmp/test4は作成できない。\n1 2  - name:test $homecommand:echo \u0026#34;test3\u0026#34; \u0026gt; ~/tmp/test3 \u0026amp;\u0026amp; echo \u0026#34;test4\u0026#34; \u0026gt; ~/tmp/test4  ","permalink":"https://wenhan.blog/post/getting-started-with-ansible-jp-basic/","summary":"こちらの記事はではAnsibleを紹介するための文章で、元Red Hat社員のJingjing Shiが作成し、Wenhan Shiが日本語に翻訳しました。内容の校正はHideki SaitoとKento Yagisawaが協力しています。Ansibleの基礎知識から、実際の現場で利用できる実運用まで紹介しています。\n本文内にあるすべてのansible playbookの例は、以下のgithubのURLから利用できる。 https://github.com/ansible-book/ansible-first-book-examples もし不備やコメントがありましたら、作者shijingjing02@163.comまたは翻訳者shibunkan@gmail.com に連絡してください。\n詳細の内容を下記三つに分けて説明します。\nAnsible 入門 - 紹介 Ansible 入門 - 基本 Ansible 入門 - 応用\n本章では、簡単な例を使ってAnsibleの基本的な使い方を説明する。\n インストール 管理対象のサーバー（サーバリストの管理） コマンドラインによるサーバー管理(Ad-hoc command) スクリプトによるサーバー管理(スクリプト言語 Playbook) モジュール  インストール ここでは Red Hat 系 Linux でのインストールを前提に解説にする。他のOSに関していAnsibleのWebページをご参照ください。\n管理者ノード Ansible パッケージをインストール 1 2 3 4  # Redhat/CentOS Linuxの場合, epolリポジトリをインストールする必要がある。 # Fedoraの場合、デフォルトのリポジトリにAnsibleを含まれているため，直接インストールできる sudo yum install epel-release sudo yum install ansible -y   管理者ノードからリモートノードの接続設定 SSH keyによる認証パスワードレス）方式を設定する。\n1 2 3 4 5 6  # ssh key を生成 ssh-keygen # リモートノードにssh keyをコピー ssh-copy-id remoteuser@remoteserver # リモートノードをknows_hostsに追加（ssh Keyの保存確認がなくなる） ssh-keyscan remote_servers \u0026gt;\u0026gt; ~/.","title":"Ansible 入門 - 基本操作(チュートリアル)"},{"content":"こちらの記事はではAnsibleを紹介するための文章で、元Red Hat社員のJingjing Shiが作成し、Wenhan Shiが日本語に翻訳しました。内容の校正はHideki SaitoとKento Yagisawaが協力しています。Ansibleの基礎知識から、実際の現場で利用できる実運用まで紹介しています。\n本文内にあるすべてのansible playbookの例は、以下のgithubのURLから利用できる。 https://github.com/ansible-book/ansible-first-book-examples もし不備やコメントがありましたら、作者shijingjing02@163.comまたは翻訳者shibunkan@gmail.com に連絡してください。\n詳細の内容を下記三つに分けて説明します。\nAnsible 入門 - 紹介 Ansible 入門 - 基本 Ansible 入門 - 応用\n本章では、もっと自由に運用するために、Ansibleの高度な使い方を説明する。\n Ansibleの設定ファイル サーバーリスト管理（Host Inventory） Playbookの上級な書き方 Extraモジュールの利用  Ansible設定ファイル 設定内容  基本の設定内容     項目 詳細 内容（デフォルト）     inventory リモートノードリスト管理ファイル /etc/ansible/hosts   library 拡張モジュールフォルダ /usr/share/my_modules/   remote_tmp リモートノード上のファイル一時保存場所 $HOME/.ansible/tmp   local_tmp 管理者ノード上のファイル一時保存場所 $HOME/.ansible/tmp     高度の設定内容     項目 詳細 内容（デフォルト）     accelerate_port 接続ポート番号 5099   accelerate_timeout タイムアウト 30   accelerate_connect_timeout 接続タイムアウト 5    上記は設定できる内容の一部しかすぎない。以下のAnsible設定ファイルの全体を通して、どんなことができるのはは理解できる\nhttps://raw.githubusercontent.com/ansible/ansible/devel/examples/ansible.cfg\n設定ファイル内のキーワードについて不明な点があった場合、下記のURLから詳細を参照できる。\nhttp://docs.ansible.com/ansible/intro_configuration.html#explanation-of-values-by-section\n優先順位 設定ファイルのデフォルトのパスは/etc/ansible/ansible.cfg。Ansibleは以下の順番で設定ファイルを検索し、最初に見つけたファイルの設定内容を読み込む。\n ANSIBLE_CONFIG (an environment variable) ansible.cfg (in the current directory) .ansible.cfg (in the home directory) /etc/ansible/ansible.cfg  Ansible 1.5以前の順番は\n ansible.cfg (in the current directory) ANSIBLE_CONFIG (an environment variable) .ansible.cfg (in the home directory) /etc/ansible/ansible.cfg  Host Inventory(サーバーリスト) サーバーリストとは、管理対象のリモートノード情報と、カテゴリやグループなどの情報をAnsibleに教えるためのファイルである。カテゴリは自分のニーズ、ロケーション、役割などによって分類できる。\nファイルパス デフォルトのファイルパス /etc/ansible/hosts\nファイルパスの変更 /etc/ansible/ansible.cfg の以下の部分を変更\n1 2 3  ...inventory = /etc/ansible/hosts...  コマンドライン上で指定 1  ansible-playbook -i hosts site.yml   或者参数–inventory-file\n1  ansible-playbook --inventory-file hosts site.yml   サーバーのカテゴリ化 []内でグループ名を書く\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16  mail.example.com[webservers]foo.example.combar.example.com[dbservers]one.example.comtwo.example.comthree.example.com[webservers]www[01:50].example.com[databases]db-[a:f].example.com  グループは子グループを持つこともできる。以下の例では、southeaseグループは[usa:children]の子グループであり、atlantaとreleighは[southease:children]の子グループである。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17  [atlanta]host1host2[raleigh]host2host3[southeast:children]atlantaraleigh[usa:children]southeastnortheastsouthwestnorthwest  接続パラメータと変数 パラメータ リモートノードに接続する方法とユーザを、パラメータに設定できる。\n1 2 3 4 5 6 7 8 9  [targets]localhost ansible_connection=localother1.example.com ansible_connection=ssh ansible_user=mpdehaanother2.example.com ansible_connection=ssh ansible_user=mdehaan[atlanta]host1 http_port=80 maxRequestsPerChild=808host2 http_port=303 maxRequestsPerChild=909  指定可能なパラメータの一覧は以下を参照する。 http://docs.ansible.com/ansible/intro_inventory.html#list-of-behavioral-inventory-parameters\n変数 グループに対して変数を設定することができる。\n1 2 3 4 5 6 7  [atlanta]host1host2[atlanta:vars]ntp_server=ntp.atlanta.example.comproxy=proxy.atlanta.example.com  ディレクトリ構造での変数保存 例えば、inventoryファイルは/etc/ansible/hostsの場合、関連するhostsとgroup変数は以下のディレクトリに保存できる。\n1 2 3  /etc/ansible/group_vars/raleigh # can optionally end in \u0026#39;.yml\u0026#39;, \u0026#39;.yaml\u0026#39;, or \u0026#39;.json\u0026#39; /etc/ansible/group_vars/webservers /etc/ansible/host_vars/foosball   /etc/ansible/group_vars/raleigh ファイルの内容は\n1 2 3  ---ntp_server:acme.example.orgdatabase_server:storage.example.org  もし対応する名前はディレクトリ名の場合、Ansibleはこのディレクトリ以下のすべてのファイルの内容を読み込む。\n1 2  /etc/ansible/group_vars/raleigh/db_settings /etc/ansible/group_vars/raleigh/cluster_settings   group_vars/とhost_vars/ はinventory/以下またはplaybook/以下に置くことができる。もし両方(inventory/またはplaybook/)の下にファイルがある場合、playbook以下の配置はinventoryより優先的に読み込まれる。\nAnsibleのスクリプト(Playbook) フォーマット AnsibleのスクリプトはYAML形式を利用している。Playbookを書く前に既存のPlaybookを参考したほうが効率良く作成することができる効率。\nオフィシャルの例 Ansibleの公式Githubリポジトリに存在するテスト済みのPlaybookをシェアしている。\nhttps://github.com/ansible/ansible-examples\n他のユーザがシェアしたPlaybook 「Ansible Galaxy」というPlaybookをシェアするためのサイトがある。このサイトにある例はユーザが自分でアップロードしたもので、Playbookの勉強や参考にするとは良いが、実際に利用する前には必ずテストが必要である。\nhttps://galaxy.ansible.com/\nPlaybookの基本 本節ではPlaybookを書くための基本を説明する。\nPlaybookの実行 1  ansible-playbook deploy.yml   詳細な実行ログを出力\n1  ansible-playbook playbook.yml --verbose   影響するリモートノードの一覧\n1  ansible-playbook playbook.yml --list-hosts   スクリプトを並行に実行する\n1  ansible-playbook playbook.yml -f 10   簡単なPlaybookの例 基本のplaybookのスクリプトの例では、以下三つの部分がある。\n どのサーバー上にどのユーザで実行  hosts users   実行するタスク  tasks   イベントハンドル  handles    deploy.yml\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18  ---- hosts:webserversvars:http_port:80max_clients:200user:roottasks:- name:ensure apache is at the latest versionyum:pkg=httpd state=latest- name:write the apache config filetemplate:src=/srv/httpd.j2 dest=/etc/httpd.confnotify:- restart apache- name:ensure apache is runningservice:name=httpd state=startedhandlers:- name:restart apacheservice:name=httpd state=restarted  対象サーバと実行ユーザ（hostsとuser）    key 意味     hosts リモートサーバーのIPアドレス、ホスト名、グループ名。またはallキーワードですべてのリモートノードを指定   user リモートサーバー上で実行するユーザ名   become 他のユーザに切り替えて実行する、値は yes または no   become_method becomeと一緒に利用、‘sudo’ / ’su’ / ’pbrun’ / ’pfexec’ / ’doas’などを指定   become_user becomeと一緒に利用、rootまたは他のユーザ名    スクリプト上でbecomeを利用する時、\u0026ndash;ask-become-passを追加することで、playbook実行後にsudoパスワードを入力することが要求される。\n1  ansible-playbook deploy.yml --ask-become-pass   実行するタスク(Tasks)  タスクはPlaybookの上から下まで実行する。途中でエラーが発生した場合、Playbookの実行が中止される。スクリプトファイルを修正した後、再度スクリプトを実行する。 各taskはモジュールの読み出しであり、パラメータと変数を適切に設定する。 各taskはname 属性を持ったほうが、人間にとって理解しやすくなる。name属性は内容を出力するだけで、実行の進捗をユーザに提示することができる。  構文 Tasksの基本構文を以下に示す。\n1 2 3  tasks:- name:make sure apache is runningservice:name=httpd state=running  name変数は必須ではないので、上記の例を下記のように記載することもできる。\n1 2  tasks:- service:name=httpd state=running  name変数が書かれた場合、Playbookがこのタスクを実行する時に、name変数の内容が出力されスクリプトの進捗が分かりやすくなる。\n1 2  TASK: [make sure apache is running] ************************************************************* changed: [yourhost]   Name変数が書かれていない場合、taskのモジュール実行の構文がそのまま出力される。複数のモジュールが使われている場合、スクリプトの進捗が分からなくなる。\n1 2  TASK: [service name=httpd state=running] ************************************** changed: [yourhost]   変数の渡し方 上記の構文の例では、モジュールに変数を渡す方法を示している：key=value\n1 2 3  tasks:- name:make sure apache is runningservice:name=httpd state=running  変数が多い場合、改行して複数行に書くこともできる。\n1 2 3 4  tasks:- name:Copy ansible inventory file to clientcopy:src=/etc/ansible/hosts dest=/etc/ansible/hostsowner=root group=root mode=0644  またはYAMLのハッシュ形式でパラメータを入力する。\n1 2 3 4 5 6 7 8  tasks:- name:Copy ansible inventory file to clientcopy:src:/etc/ansible/hostsdest:/etc/ansible/hostsowner:rootgroup:rootmode:0644  実行状態 タスク内の各actionは一つのモジュールを実行し、最初にリモートノードの状態を確認し、実行が必要かどうかを判断する。\n モジュールが実行された場合、モジュールからactionへの戻り値は changed になる。 実行が必要ないと判断した時、モジュールからactionへの戻り値は OK になる。  実行必要性の判断は各モジュール内で決めている。例えば、copy モジュールの判断方式は、ファイルのchecksum値を比較する。copyモジュールのコードを以下に示す。\nhttps://github.com/ansible/ansible-modules-core/blob/devel/files/copy.py\n以下のように、copyモジュールを例にする\n1 2 3  tasks:- name:Copy the /etc/hostscopy:src=/etc/hosts dest=/etc/hosts  最初に実行する時に、TASKの状態がchangedになっている。\n2回目実行すると、コピー元とコピー先のファイルが同じのめ、コピー作業が必要ないと判断し実行されていない。TASKの状態がOKになっている。\nリモートノードvm-rhel7-1上の/etc/hostsファイルを変更した後、もう一度実行すると、vm-rhel7-1のみファイルがコピーされ、TASKの状態がchangedになる。\nイベントハンドラ(handler) 定義 一般的なプログラミング言語にevent処理があるように、handleとはplaybookスクリプトでのevent処理である。\ntasksと似ていて、Handlers内の各handlerはモジュールを一回実行している。ただ、tasksと異なっている部分は、tasksは書かれた順によって実行されるが、handlersはtasks内に呼び出す(notify)時のみ実行される。\nまた、tasksが実行された後、changedまたはOKの状態になっていて、handlerはtasks実行後状態がchangedの時のみ実行される。\n役割 もしtasksの中にapachの設定ファイルを変更した後、apacheを再起動する必要がある。そのほかにapache のプラグインをインストールした後にも、apacheを再起動する必要がある。この場合、「apacheを再起動する」ことは、handlerとして実行することができる。\nhandlerは全てのタスクが実行した後に実行する。複数のtasksに呼び出されても、handlerは一回のみ実行する。以下の例では、handlerは一回のみ実行される。 https://github.com/ansible-book/ansible-first-book-examples/blob/master/handlers_state.yml\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19  ---- hosts:lbremote_user:rootvars:random_number1:\u0026#34;\\{\\{ 10000 | random \\}\\}\u0026#34;random_number2:\u0026#34;\\{\\{ 10000000000 | random \\}\\}\u0026#34;tasks:- name:Copy the /etc/hosts to /tmp/hosts.\\{\\{ random_number1 \\}\\}copy:src=/etc/hosts dest=/tmp/hosts.\\{\\{ random_number1 \\}\\}notify:- call in every action- name:Copy the /etc/hosts to /tmp/hosts.\\{\\{ random_number2 \\}\\}copy:src=/etc/hosts dest=/tmp/hosts.\\{\\{ random_number2 \\}\\}notify:- call in every actionhandlers:- name:call in every actiondebug:msg=\u0026#34;call in every action, but execute only one time\u0026#34;  tasksのactionの実行状態がchangedの場合のみ、handlerが呼び出されて(notify)実行される。下記のスクリプトが2回実行された時、実行結果が異る。\n 1回目実行の時、tasksの状態が両方changedのため、二つのhandlerが実行される。 2回目実行の時、  1つ目のtaskの状態がOKのため、\u0026ldquo;call by /tmp/hosts”handlerが実行されない。 2つ目のtaskの状態がchangedのため、\u0026ldquo;call by /tmp/hosts.random_number” handlerが実行される    テストのコードを以下を参照する。 https://github.com/shijingjing1221/ansible-first-book-examples/blob/master/handlers_execution_time.yml\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20  ---- hosts:lbremote_user:rootvars:random_number:\u0026#34;\\{\\{ 10000 | random \\}\\}\u0026#34;tasks:- name:Copy the /etc/hosts to /tmp/hostscopy:src=/etc/hosts dest=/tmp/hostsnotify:- call by /tmp/hosts- name:Copy the /etc/hosts to /tmp/hosts.\\{\\{ random_number \\}\\}copy:src=/etc/hosts dest=/tmp/hosts.\\{\\{ random_number \\}\\}notify:- call by /tmp/hosts.random_numberhandlers:- name:call by /tmp/hostsdebug:msg=\u0026#34;call first time\u0026#34;- name:call by /tmp/hosts.random_numberdebug:msg=\u0026#34;call by /tmp/hosts.random_number\u0026#34;  handlersはnotifyされる順番ではなく、定義された順番で実行される。下記の例では、定義の順番が1\u0026gt;2\u0026gt;3であり、notifyの順番が3\u0026gt;2\u0026gt;1であっても、実際の実行順番が1\u0026gt;2\u0026gt;3になる。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25  ---- hosts:lbremote_user:rootgather_facts:novars:random_number1:\u0026#34;\\{\\{ 10000 | random \\}\\}\u0026#34;random_number2:\u0026#34;\\{\\{ 10000000000 | random \\}\\}\u0026#34;tasks:- name:Copy the /etc/hosts to /tmp/hosts.\\{\\{ random_number1 \\}\\}copy:src=/etc/hosts dest=/tmp/hosts.\\{\\{ random_number1 \\}\\}notify:- define the 3nd handler- name:Copy the /etc/hosts to /tmp/hosts.\\{\\{ random_number2 \\}\\}copy:src=/etc/hosts dest=/tmp/hosts.\\{\\{ random_number2 \\}\\}notify:- define the 2nd handler- define the 1nd handlerhandlers:- name:define the 1nd handlerdebug:msg=\u0026#34;define the 1nd handler\u0026#34;- name:define the 2nd handlerdebug:msg=\u0026#34;define the 2nd handler\u0026#34;- name:define the 3nd handlerdebug:msg=\u0026#34;define the 3nd handler\u0026#34;  Playbookで利用可能な変数 本節では、よく使う幾つかの変数を紹介する。次の章からは、複雑な場面での変数の使い方などを紹介する。\n一般的に、Playbookで利用できる変数は以下になる。\n Playbook中ユーザが定義する変数 ユーザが定義する必要がなく, AnsibleがPlaybookを実行する前に収集したリモートノードのシステム情報として使える変数 テンプレートでは上記 2つ種類の変数が直接利用できる。 taskの実行結果を変数として使える。この変数は\u0026lt;register変数\u0026gt;となる Playbookをもっと汎用性高く、動的に使いやすくするために、ユーザが実行中に変数を入力することができる。この変数は\u0026lt;extra変数\u0026gt;となる  Playbook中で定義した変数 ユーザはPlaybook中に、varsキーワードで変数を定義することができる。使用する時は\\{\\{\\}\\}で囲んで利用する。下記の例では、ユーザがhttp_port変数を定義し、80と設定している。tasks firewalldの中で、\\{\\{ http_port \\}\\}で内容を読み込んでいる。\n1 2 3 4 5 6 7 8  ---- hosts:webvars:http_port:80remote_user:roottasks:- name:insert firewalld rule for httpdfirewalld:port=\\{\\{ http_port \\}\\}/tcp permanent=true state=enabled immediate=yes  変数を単独ファイルに置く 変数が多い時、または複数のPlaybook中に共用されたい時に、変数を単独の変数ファイルに置くことができる。var_filesパラメータに該当する変数ファイル名を設定すると、同じ使い方で変数を参照できる。\nPlaybook\n1 2 3 4 5 6 7  - hosts:webremote_user:rootvars_files:- vars/server_vars.ymltasks:- name:insert firewalld rule for httpdfirewalld:port=\\{\\{ http_port \\}\\}/tcp permanent=true state=enabled immediate=yes  vars/server_vars.yml\n1  http_port:80  複雑変数の扱い方 利用したい変数は簡単な文字列や数字ではなく、一つのオブジェクトにしたい時がある。その時に以下のようなYAMLのハッシュ形式(または辞書形式)にオブジェクトの内容を設定する。\n1 2 3  foo:field1:onefield2:two  こちらの変数を利用したい時は、中括弧またはピリオドで参照できる。\n1 2  foo[\u0026#39;field1\u0026#39;]foo.field1  YAMLの落とし穴 YAMLの変数構文がAnsibleの変数構文と一緒に書くと構文エラーになる場合がある。詳細については、コロン(:)の後ろにすぐ大括弧 { を書くことはできなく、かならずダブルクォーテーション \u0026quot; で囲む必要がある。もしエラー内容が YAMLの構文エラーの場合、ダブルクォーテーション “ を追加して解決する。\nこのYAMLは構文エラーになる。\n1 2 3  - hosts:app_serversvars:app_path:\\{\\{ base_path \\}\\}/22  解決方法は、変数の前後にダブルクォーテーションを追加する。\n1 2 3  - hosts:app_serversvars:app_path:\u0026#34;\\{\\{ base_path \\}\\}/22\u0026#34;  システム変数(facts) Ansibleはsetup モジュールを通じてリモートノードのシステム情報を収集している。こちらのシステム情報はfactsと呼び、直接変数として利用することができる。 コマンドラインからsetupモジュールを実行して利用可能な変数の一覧が確認できる。\n1  ansible all -m setup -u root   Playbookの中では、facts変数を直接利用することができる。\n1 2 3 4 5 6 7 8 9 10 11 12  ---- hosts:alluser:roottasks:- name:echo systemshell:echo \\{\\{ ansible_os_family \\}\\}- name install ntp on Debian linuxapt:name=git state=installedwhen:ansible_os_family == \u0026#34;Debian\u0026#34;- name install ntp on redhat linuxyum:name=git state=presentwhen:ansible_os_family == \u0026#34;RedHat\u0026#34;  複雑なfacts変数 一般変数と同じく、以下のようなハッシュ変数形式のfacts変数も存在する。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28  ... \u0026#34;ansible_ens3\u0026#34;: { \u0026#34;active\u0026#34;: true, \u0026#34;device\u0026#34;: \u0026#34;ens3\u0026#34;, \u0026#34;ipv4\u0026#34;: { \u0026#34;address\u0026#34;: \u0026#34;10.66.192.234\u0026#34;, \u0026#34;netmask\u0026#34;: \u0026#34;255.255.254.0\u0026#34;, \u0026#34;network\u0026#34;: \u0026#34;10.66.192.0\u0026#34; }, \u0026#34;ipv6\u0026#34;: [ { \u0026#34;address\u0026#34;: \u0026#34;2620:52:0:42c0:5054:ff:fef2:e2a3\u0026#34;, \u0026#34;prefix\u0026#34;: \u0026#34;64\u0026#34;, \u0026#34;scope\u0026#34;: \u0026#34;global\u0026#34; }, { \u0026#34;address\u0026#34;: \u0026#34;fe80::5054:ff:fef2:e2a3\u0026#34;, \u0026#34;prefix\u0026#34;: \u0026#34;64\u0026#34;, \u0026#34;scope\u0026#34;: \u0026#34;link\u0026#34; } ], \u0026#34;macaddress\u0026#34;: \u0026#34;52:54:00:f2:e2:a3\u0026#34;, \u0026#34;module\u0026#34;: \u0026#34;8139cp\u0026#34;, \u0026#34;mtu\u0026#34;: 1500, \u0026#34;promisc\u0026#34;: false, \u0026#34;type\u0026#34;: \u0026#34;ether\u0026#34; }, ...   以下の二つの方法でハッシュ形式のfacts変数の属性を参照する。\n 中括弧  1  \\{\\{ ansible_ens3[\u0026#34;ipv4\u0026#34;][\u0026#34;address\u0026#34;] \\}\\}   ピリオド  1  \\{\\{ ansible_ens3.ipv4.address \\}\\}  factsの無効化 Playbookでは、gather_factsパラメータがfacts変数の収集を設定できる。下記のように no と設定すると、上記のfacts変数はこのPlaybookでの利用ができなくなる。\n1 2  - hosts:whatevergather_facts:no  テンプレートで定義した変数 テンプレートはAnsibleでよく使われているため、テンプレートファイルでの変数の使い方を説明する。\n変数の定義 Playbook内で定義した変数、factsシステム変数、inventory内で定義したhostとgroup変数は、直接テンプレートで利用できる。Playbook内で利用できる変数は、テンプレートファイルでも利用できる。\n下記のPlaybookスクリプトではtemplate モジュールでindex.html.j2ファイルをコピーし、その中の変数をPlaybookで定義した変数の値に設定する。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26  ---- hosts:webvars:http_port:80defined_name:\u0026#34;Hello My name is Jingjng\u0026#34;remote_user:roottasks:- name:ensure apache is at the latest versionyum:pkg=httpd state=latest- name:Write the configuration filetemplate:src=templates/httpd.conf.j2 dest=/etc/httpd/conf/httpd.confnotify:- restart apache- name:Write the default index.html filetemplate:src=templates/index2.html.j2 dest=/var/www/html/index.html- name:ensure apache is runningservice:name=httpd state=started- name:insert firewalld rule for httpdfirewalld:port=\\{\\{ http_port \\}\\}/tcp permanent=true state=enabled immediate=yeshandlers:- name:restart apacheservice:name=httpd state=restarted  変数の利用 Ansibleのテンプレートファイルが利用する構文はPython のテンプレート言語Jinja2。下記のテンプレートの例index.html.j2の中で、変数を直接利用している。\n システム変数 \\{\\{ ansible_hostname \\}\\}, \\{\\{ ansible_default_ipv4.address \\}\\} ユーザ定義した変数 \\{\\{ defined_name \\}\\}  Index.html.j2ファイルの内容\n1 2 3 4 5 6 7 8 9 10 11  \u0026lt;html\u0026gt; \u0026lt;title\u0026gt;Demo\u0026lt;/title\u0026gt; \u0026lt;body\u0026gt; \u0026lt;div class=\u0026#34;block\u0026#34; style=\u0026#34;height: 99%;\u0026#34;\u0026gt; \u0026lt;div class=\u0026#34;centered\u0026#34;\u0026gt; \u0026lt;h1\u0026gt;#46 Demo \\{\\{ defined_name \\}\\}\u0026lt;/h1\u0026gt; \u0026lt;p\u0026gt;Served by \\{\\{ ansible_hostname \\}\\} (\\{\\{ ansible_default_ipv4.address \\}\\}).\u0026lt;/p\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt;   タスクの実行結果を変数に設定 taskの実行結果は変数の値に代入できる。この場合はregisterキーワードを利用し、taskの実行結果を変数に設定し、その後にあるactionに利用される。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14  ---- hosts:webtasks:- shell:lsregister:resultignore_errors:True- shell:echo \u0026#34;\\{\\{ result.stdout \\}\\}\u0026#34;when:result.rc == 5- debug:msg=\u0026#34;\\{\\{ result.stdout \\}\\}\u0026#34;  Register変数はよくdebug モジュールと一緒に利用され、actionの実行の詳細情報を得てトラブルシュッティングに役立つ。\nコマンドラインで変数を渡す 汎用性と便利性のため、コマンドラインでPlaybookに変数の値を渡すことができる。この場合はansible-playbookの\u0026ndash;extra-varsオプションを利用する。\nコマンドライン変数の定義 下記のように、release.ymlファイルでは、hostsとuserは変数として定義され、コマンドラインから値を設定する必要がある。\n1 2 3 4 5 6 7  ---- hosts:\u0026#39;\\{\\{ hosts \\}\\}\u0026#39;remote_user:\u0026#39;\\{\\{ user \\}\\}\u0026#39;tasks:- ...  コマンドライン変数の利用 以下のようにコマンドラインから変数の値を設定する。\n1  ansible-playbook e33_var_in_command.yml --extra-vars \u0026#34;hosts=web user=root\u0026#34;   そのほかに、JSON形式でも利用できる。\n1  ansible-playbook e33_var_in_command.yml --extra-vars \u0026#34;{\u0026#39;hosts\u0026#39;:\u0026#39;vm-rhel7-1\u0026#39;, \u0026#39;user\u0026#39;:\u0026#39;root\u0026#39;}\u0026#34;   また、変数と値をJSONファイルに書いて、コマンドラインに渡すこともできる。\n1  ansible-playbook e33_var_in_command.yml --extra-vars \u0026#34;@vars.json\u0026#34;   条件判断、ループとブロック  when： 条件判断のキーワード。ifに似ている loop：循環処理（ループ）のキーワード。forに似ている block：複数のtasksを一つのブロックにまとめて、異常処理の対応などに利用できる。  条件判断(when) 例えば、特定のversionのシステム上にパッケージをインストールや、ディスク空き容量がない時にファイルを削除するなど、特定の条件に満足している時のみ、特定のtaskを実行したい場合、Playbook内のwhenを利用する。 サーバーがDebian Linuxの場合すぐシャットダウンする\n1 2 3 4  tasks:- name:\u0026#34;shutdown Debian flavored systems\u0026#34;command:/sbin/shutdown -t nowwhen:ansible_os_family == \u0026#34;Debian\u0026#34;  actionの実行結果によって、次に実行するactionを決める\n1 2 3 4 5 6 7 8 9 10  tasks:- command:/bin/falseregister:resultignore_errors:True- command:/bin/somethingwhen:result|failed- command:/bin/something_elsewhen:result|success- command:/bin/still/something_elsewhen:result|skipped  リモートノードのシステム変数factsも 条件としてwhenに判断されることができる。また、| intの書き方で返り値の型変換もできる。\n1 2 3 4 5  ---- hosts:webtasks:- debug:msg=\u0026#34;only on Red Hat 7, derivatives, and later\u0026#34;when:ansible_os_family == \u0026#34;RedHat\u0026#34; and ansible_lsb.major_release|int \u0026gt;= 6  条件式 真偽（True/False）判断の条件式\n1 2 3 4 5 6 7 8 9 10  vars:epic:truetasks:- shell:echo \u0026#34;This certainly is epic!\u0026#34;when:epictasks:- shell:echo \u0026#34;This certainly isn’t epic!\u0026#34;when:not epic  定義有り無し判断の条件式\n1 2 3 4 5 6  tasks:- shell:echo \u0026#34;I\u0026#39;ve got \u0026#39;\\{\\{ foo \\}\\}\u0026#39; and am not afraid to use it!\u0026#34;when:foo is defined- fail:msg=\u0026#34;Bailing out. this play requires \u0026#39;bar\u0026#39;\u0026#34;when:bar is not defined  数値に対する条件式\n1 2 3 4  tasks:- command:echo \\{\\{ item \\}\\}with_items:[0,2,4,6,8,10]when:item \u0026gt; 5  includeと一緒に利用\n1 2  - include:tasks/sometasks.ymlwhen:\u0026#34;\u0026#39;reticulating splines\u0026#39; in output\u0026#34;  Roleと一緒に利用\n1 2 3  - hosts:webserversroles:- {role: debian_stock_config, when:ansible_os_family == \u0026#39;Debian\u0026#39; }  循環処理(loop) 標準のループ(with_items) コードの読み易さと簡潔のため、重複のタスク内容を以下のように書くことができる。\n1 2 3 4 5  - name:add several usersuser:name=\\{\\{ item \\}\\} state=present groups=wheelwith_items:- testuser1- testuser2  変数ファイルまたはPlaybookのvarsでYAMLリストを定義した場合、下記のように処理をループすることができる。\n1 2 3 4 5 6  vars:somelist:[\u0026#34;testuser1\u0026#34;,\u0026#34;testuser2\u0026#34;]tasks:-name:add several useruser:name=\\{\\{ item \\}\\} state=present groups=wheelwith_items:\u0026#34;\\{\\{somelist\\}\\}\u0026#34;  with_itemsキーワードで操作できるのは簡単なデータだけではなく、ハッシュ型（辞書型）の変数の値もループできる。\n1 2 3 4 5  - name:add several usersuser:name=\\{\\{ item.name \\}\\} state=present groups=\\{\\{ item.groups \\}\\}with_items:- {name: \u0026#39;testuser1\u0026#39;, groups:\u0026#39;wheel\u0026#39;}- {name: \u0026#39;testuser2\u0026#39;, groups:\u0026#39;root\u0026#39;}  注意：whenとwith_items（または他のループ）を同時に使用する場合、ループの各実行項目に対してwhenが条件判断を行う。\nネストループ(with_nested) 階層形式のデータもループすることができる。\n1 2 3 4 5  - name:give users access to multiple databasesmysql_user:name=\\{\\{ item[0] \\}\\} priv=\\{\\{ item[1] \\}\\}.*:ALL append_privs=yes password=foowith_nested:- [\u0026#39;alice\u0026#39;,\u0026#39;bob\u0026#39;]- [\u0026#39;clientdb\u0026#39;,\u0026#39;employeedb\u0026#39;,\u0026#39;providerd\u0026#39;]  または\n1 2 3 4 5  - name:give users access to multiple databasesmysql_user:name=\\{\\{ item.0 \\}\\} priv=\\{\\{ item.1 \\}\\}.*:ALL append_privs=yes password=foowith_nested:- [\u0026#39;alice\u0026#39;,\u0026#39;bob\u0026#39;]- [\u0026#39;clientdb\u0026#39;,\u0026#39;employeedb\u0026#39;,\u0026#39;providerd\u0026#39;]  ハッシュループ(with_dict) 1 2 3 4 5 6 7 8 9 10 11 12 13  ---vars:users:alice:name:Alice Appleworthtelephone:123-456-7890bob:name:Bob Bananaramatelephone:987-654-3210tasks:- name:Print phone recordsdebug:msg=\u0026#34;User \\{\\{ item.key \\}\\} is \\{\\{ item.value.name \\}\\} (\\{\\{ item.value.telephone \\}\\})\u0026#34;with_dict:\u0026#34;\\{\\{users\\}\\}\u0026#34;  ファイルリストループ(with_fileglob) with_fileglobを利用し、フォルダ以下のファイルをループに代入することができる。\n1 2 3 4 5 6 7 8 9  tasks:# first ensure our target directory exists- file:dest=/etc/fooapp state=directory# copy each file over that matches the given pattern- copy:src=\\{\\{ item \\}\\} dest=/etc/fooapp/ owner=root mode=600with_fileglob:- /playbooks/files/fooapp/*  ブロック(block) 複数actionをブロックに纏めて、設定した条件に合致した場合、全actionが一括に実行される。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14  tasks:- block:- yum:name=\\{\\{ item \\}\\} state=installedwith_items:- httpd- memcached- template:src=templates/src.j2 dest=/etc/foo.conf- service:name=bar state=started enabled=Truewhen:ansible_distribution == \u0026#39;CentOS\u0026#39;become:truebecome_user:root  異常処理をするときに、blockが便利でよく使われている。\n1 2 3 4 5 6 7 8 9 10 11  tasks:- block:- debug:msg=\u0026#39;i execute normally\u0026#39;- command:/bin/false- debug:msg=\u0026#39;i never execute, cause ERROR!\u0026#39;rescue:- debug:msg=\u0026#39;I caught an error\u0026#39;- command:/bin/false- debug:msg=\u0026#39;I also never execute :-(\u0026#39;always:- debug:msg=\u0026#34;this always executes\u0026#34;  既存Playbook の読み込む 既存のPlaybookを利用することで、開発時間が節約できるし、メンテナンスもしやすくなる。他のPlaybookファイルを読み込むために、以下二つが方法がある。\n include:  既存のPlaybook スクリプトを読み込んで利用する、簡単で利用しやすい。   role:  Playbookの\u0026quot;函数”みたいで、使い方はちょっと複雑ですが、includeより強力でいろんなことができる。Ansible Galaxyはrole方式でPlaybookをシェアしている。    Playbookファイルの include 他のPlaybookファイルを include することで、そのファイル内のtasksが利用できる。また、includeでtasksを複数のファイルに分割することで、Playbookの肥大化を防ぐことができる。\n基本の使い方 直接yamlファイルをincludeする。 tasks/firewall_httpd_default.yml\n1 2 3 4 5  ---# possibly saved as tasks/firewall_httpd_default.yml- name:insert firewalld rule for httpdfirewalld:port=80/tcp permanent=true state=enabled immediate=yes  他のPlaybookでのinclude方法は\n1 2  tasks:- include:tasks/firewall_httpd_default.yml  高度な使い方 includeされるPlaybookの中で変数を定義することができる。下記の例では、includeされたtasks/firewall_httpd_default.ymlの中に{{ port }}でport変数を定義した。\n1 2 3  ---- name:insert firewalld rule for httpdfirewalld:port=\\{\\{ port \\}\\}/tcp permanent=true state=enabled immediate=yes  変数の値をincludeされるPlaybookに渡すために、以下の方法がある。\nincludeされるPlaybookの後ろに、変数名と値を指定 1 2 3 4  tasks:- include:tasks/firewall.yml port=80- include:tasks/firewall.yml port=3260- include:tasks/firewall.yml port=423  YAMLのハッシュ形式で変数と値を渡す 1 2 3 4 5 6 7 8  tasks:- include:wordpress.ymlvars:wp_user:timmyssh_keys:- keys/one.txt- keys/two.txt  taskをJSON形式に書いて、変数と値を設定する 1 2  tasks:- {include: wordpress.yml, wp_user: timmy, ssh_keys:[\u0026#39;keys/one.txt\u0026#39;,\u0026#39;keys/two.txt\u0026#39;]}  Playbook中で定義済みの変数は、特に渡す必要がなく、そのままりようされる。 1 2 3 4 5 6 7  ---- hosts:lbvars:port:3206remote_user:roottasks:- include:tasks/firewall.yml  Includeの落とし穴  handlers内でPlaybook を include する。  書き方は以下に示す。\n1 2  handlers:- include:handlers/handlers.yml  ただし、上記の使い方は、有効と書いたドキュメントがあるが、無効と書いたドキュメントもあり、矛盾している。\n無効と書いたURL：http://docs.ansible.com/ansible/playbooks_intro.html 有効と書いたURL：http://docs.ansible.com/ansible/playbooks_roles.html#task-include-files-and-encouraging-reuse\n下記のテストの結果、Ansible 1.9では、includeされたhandlerを呼び出すことができないが、Ansible 2.0以降であれば、includeされたhandlerを呼び出すことができる。そのため、利用しているAnsibleのversionを確認する必要がある。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18  ---- hosts:lbuser:rootgather_facts:novars:random_number:\u0026#34;\\{\\{ 10000 | random \\}\\}\u0026#34;tasks:- name:Copy the /etc/hosts to /tmp/hosts.\\{\\{ random_number \\}\\}copy:src=/etc/hosts dest=/tmp/hosts.\\{\\{ random_number \\}\\}notify:- restart apache- restart apache in handlershandlers:- include:handlers/handlers.yml- name:restart apachedebug:msg=\u0026#34;This is the handler restart apache\u0026#34;   Playbook include  推奨されない使い方で、パラメータが利用できないなどの制限がある。\n1 2 3 4 5 6 7 8 9 10 11 12 13  - name:this is a play at the top level of a filehosts:allremote_user:roottasks:- name:say hitags:fooshell:echo \u0026#34;hi...\u0026#34;# 全体include，またはplaybook include- include:load_balancers.yml- include:webservers.yml- include:dbservers.yml  Playbookの\u0026quot;Package”(role) Roleはincludeよりもっと柔軟で強力なコードシェア方法を提供する。includeは他のプログラミング言語の\u0026quot;include”と似たような使い方で、1つのファイルした利用できない。それに対し、roleは”Package”みたいな使い方で、複数のファイルを同時参照によって1つの機能を利用できる。例えば、apacheのインストールと設定をする時、tasksでパッケージのインストール、テンプレートファイルと設定ファイルのコピー、handlerでのサービス再起動など、こちらの処理を1つのroleに纏めることによって、他のPlaybookに参照し利用されることができる。\nAnsibleではroleの利用を推奨していて、roleをシェアするためのプラットフォーム Ansible Galaxy https://galaxy.ansible.com/ を提供している。そこでは他の人が書いたroleを参照し利用することができる。\nroleのディレクトリ構造 Ansibleでは、決まったディレクトリ構造でroleを定義する。具体的な例を以下に示す。\nこの例では、名前がmyroleのroleが定義され、site.ymlで参照している。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19  site.ymlroles/├── myrole├── tasks│ └── main.yml├── handlers│ └── main.yml├── defaults│ └── main.yml├── vars│ └── main.yml├── files├── templates├── README.md├── meta│ └── main.yml└── tests├── inventory└── test.yml  site.ymlでroleを参照\n1 2 3 4  ---- hosts:webserversroles:- myrole  roleを作成する時に上記すべてのディレクトリとファイルが必須ではないが、このroleが実現したい機能と仕様によって対応するディレクトリとファイルを作成する。下記は各ファイルの役割をしめす 。\n   ファイル 役割     roles/x/tasks/main.yml ファイル内のtasksがPlaybookに追加される。そのため、このファイルはroleの入り口である。このroleの詳細を知りたい場合は、このファイルから参照したほうが良い   roles/x/handlers/main.yml ファイル内のhandlersがPlaybookに追加される。   roles/x/vars/main.yml ファイル内のvariablesがPlaybookに追加される。   roles/x/meta/main.yml ファイル内のロール情報がPlaybookのrolesリストに追加される。    roles/x/tasks/main.yml でのすべてのtasksは、roles/x/{files,templates,tasks}のファイルを直接参照でき、ファイルパスを指定する必要がない。自分でroleを書くときに、一般的には roles/x/tasks/main.yml を作成する。他のファイルとディレクトリは、必要によって追加する。\n以下では、具体の例を通してroleの書き方と使い方を紹介する。\nRoleで変数を使う 変数付きのmyroleのディレクトリ構造を以下に示す。\n1 2 3 4 5  main.yml roles role_with_var tasks main.yml   以下の様に、roles/myrole/tasks/main.yml で\\{\\{ \\}\\}を使って変数を定義する。\n1 2 3  ---- name:use paramdebug:msg=\u0026#34;\\{\\{ param \\}\\}\u0026#34;  それで、main.ymlで以下のようにmyroleを利用することができる\n1 2 3 4 5 6  ---- hosts:webserversroles:- {role: myrole, param:\u0026#39;Call some_role for the 1st time\u0026#39;}- {role: myrole, param:\u0026#39;Call some_role for the 2nd time\u0026#39;}  YAMLの辞書型に書くと\n1 2 3 4 5 6 7 8  ---- hosts:webserversroles:- role:myroleparam:\u0026#39;Call some_role for the 1st time\u0026#39;- role:myroleparam:\u0026#39;Call some_role for the 2nd time\u0026#39;  変数のデフォルト値 変数のデフォルト値が設定された場合、roleを読み込むときにもし変数に対し新しい値が渡された場合、その新しい値を利用する。もし変数に新しい値を渡していない場合、変数はデフォルト値を利用する。\nデフォルト値の設定はとても簡単で、上記のrole_with_varを例にする\n1 2 3 4 5 6 7  main.yml roles: myrole tasks main.yml defaults main.yml   roles/myrole/defaults/main.ymlでparamの値を定義する。\n1  param:\u0026#34;I am the default value\u0026#34;  これによって、main.ymlでは以下の二つの方法でroleを読み込むことができる。\n1 2 3 4 5  ---- hosts:webserversroles:- role_with_var- {role: role_with_var, param:\u0026#39;I am the value from external\u0026#39;}  他の例については https://github.com/shijingjing1221/ansible-first-book-examples/blob/master/role_vars.ymlを参照する\nroleとwhenと一緒に実行 下記の例では、my_roleはRedHat系のサーバー上しか有効にならない。\n1 2 3 4  ---- hosts:webserversroles:- {role: my_role, when:\u0026#34;ansible_os_family == \u0026#39;RedHat\u0026#39;\u0026#34;}  YAML辞書型で書くと以下になる。\n1 2 3 4 5 6  ---- hosts:webserversroles:- role:my_rolewhen:\u0026#34;ansible_os_family == \u0026#39;RedHat\u0026#39;\u0026#34;  roleとtaskの実行順番 Playbookでは、roleとtasksが同時にある時に、どちらが先に実行になる？答えが以下のようになる。\npre_tasks \u0026gt; role \u0026gt; tasks \u0026gt; post_tasks\n例で示すと、以下のPlaybookに対し\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19  ---- hosts:lbuser:rootpre_tasks:- name:preshell:echo \u0026#39;hello\u0026#39;roles:- {role:some_role }tasks:- name:taskshell:echo \u0026#39;still busy\u0026#39;post_tasks:- name:postshell:echo \u0026#39;goodbye\u0026#39;  実際に実行した結果は以下になる。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22  PLAY [lb] ********************************************************************** TASK [setup] ******************************************************************* ok: [rhel7u3] TASK [pre] ********************************************************************* changed: [rhel7u3] TASK [some_role : some role] *************************************************** ok: [rhel7u3] =\u0026gt; { \u0026#34;msg\u0026#34;: \u0026#34;Im some role\u0026#34; } TASK [task] ******************************************************************** changed: [rhel7u3] TASK [post] ******************************************************************** changed: [rhel7u3] PLAY RECAP ********************************************************************* rhel7u3 : ok=5 changed=3 unreachable=0 failed=0   tagsで一部のtasksを実行 Playbookの内容が多い時、一部しか実行したくない場合、Playbookのtagsを使って一部のtasksを実行できる。\ntagsの基本の使い方 以下の例では、example.ymlの中にpackagesとconfiguration 二つのtagが書かれている。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17  tasks:- yum:name=\\{\\{ item \\}\\} state=installedwith_items:- httpdtags:- packages- name:copy httpd.conftemplate:src=templates/httpd.conf.j2 dest=/etc/httpd/conf/httpd.conftags:- configuration- name:copy index.htmltemplate:src=templates/index.html.j2 dest=/var/www/html/index.htmltags:- configuration  tag変数を付けずに実行すると、すべてのtasksが実行される。\n1  ansible-playbook example.yml   パッケージのインストールのみ実行したい時に、tagsでpackagesを指定してPlaybook を実行する。\n1  ansible-playbook example.yml --tags \u0026#34;packages\u0026#34;   configurationの部分を実行しない場合、skip-tagsでconfigurationを指定してPlaybook を実行する。\n1  ansible-playbook example.yml --skip-tags \u0026#34;configuration\u0026#34;   特殊の意味を持つtags  always  タグの名前がユーザで決めるが、alwaysに設定するとタグの意味が特殊になる。Playbookを実行するときに、alwaysタグを実行しないことを明示的に示さないと、このタグは実行される。\n下記の例では、\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15  tasks:- debug:msg=\u0026#34;Always print this debug message\u0026#34;tags:- always- yum:name=\\{\\{ item \\}\\} state=installedwith_items:- httpdtags:- packages- template:src=templates/httpd.conf.j2 dest=/etc/httpd/conf/httpd.conftags:- configuration  packagesタグのみ実行しても、alwaysタグのtaskも実行される。\n1  ansible-playbook tags_always.yml --tags \u0026#34;packages\u0026#34;    tagged, untagged, all  以下のPlaybookに対し\n1 2 3 4 5 6 7  tasks:- debug:msg=\u0026#34;I am not tagged\u0026#34;tags:- tag1- debug:msg=\u0026#34;I am not tagged\u0026#34;  それぞれのコマンドにtagged, untagged, allを指定して実行してみましょう\n1 2 3  ansible-playbook tags_tagged_untagged_all.yml --tags tagged ansible-playbook tags_tagged_untagged_all.yml --tags untagged ansible-playbook tags_tagged_untagged_all.yml --tags all   includeとroleでtagsを利用 下記のようにincludeの中にtagsを設定することができる。\n1 2  - include:foo.ymltags:[web,foo]  rolesの中のタグを参照したい場合は、以下の例で示す。\n1 2  roles:- {role: webserver, port: 5000, tags:[\u0026#39;web\u0026#39;,\u0026#39;foo\u0026#39;]}  Ansible の拡張モジュール (Extra Modules) Ansibleのモジュールドキュメント(http://docs.ansible.com/ansible/latest/modules/modules_by_category.html)を参照する時に、各ページの一番下に、このモジュールは\u0026quot;Core module”または”Extra Module”であることを示している。例えば、yumはcore moduleが、yum_repositoryはextra moduleである。\n Core Module  特に設定やインストールする必要がなく、Ansibleをインストール後に使える。 よく使われているmodule テスト済み   Extra Module  設定やインストールをしてから利用する。 比較的に使用頻度が低い bugが存在している可能性がある。    Extra Moduleの使い方 以下の手順でExtra moduleをのインストールと設定を行うと、コマンドラインとPlaybookで利用できる。Extra Module は Core Module と同じ使い方で利用できる。 注意：Extra moduleがテストされ問題ない場合はCore Moduleに移動される。\nAnsible module extraをダウンロード 1  git clone https://github.com/ansible/ansible-modules-extras.git   例として、ダウンロード先は/home/jshi/software/になっている。このパスは後で利用する。\nextraモジュールが使えるように設定内容を修正  方法1：デフォルトのAnsible設定ファイルを変更  /etc/ansible/ansible.cfgを編集し、下記 1 行を追加\n1  library = /home/jshi/software/ansible-modules-extras/   方法2：カレントディレクトリの下にあるansible.cfgを変更  この方法は、同じくカレントディレクトリにあるplaybookのみに有効になる。他の親ディレクトリや子ディレクトリのPlaybookには無効になる。\n下記のディレクトリ構造に対し、ansible.cfgを変更した後、use_extra_module.ymlのみextra moduleが利用できる。\n1 2 3 4  library/ansible-modules-extras ansible.cfg use_extra_module.yml subfolder/use_extra_module_will_throw_error.yml   カレントディレクトリにあるため、設定内容は相対パスでも大丈夫。\n1  library = library/ansible-modules-extras/    方法3：環境変数を修正する  1  export ANSIBLE_LIBRARY=/project/demo/demoansible/library/ansible-module-extras   もし再起動後も有効にしたい場合、~/.bashrcでANSIBLE_LIBRARY環境変数を設定する。\n1 2 3 4 5 6 7  $ echo \u0026gt;\u0026gt;~/.bashrc \u0026lt;\u0026lt;EOF export ANSIBLE_LIBRARY=/project/demo/demoansible/library/ansible-module-extras EOF $ source ~/.bashrc   コマンドラインでモジュールの使い方を参照 Bashのman命令みたいに、ansibleはコマンドラインでmoduleの使い方が参照できる。コマンド名は ansible-doc である。\n1  ansible-doc module_name   Core Moduleに対しはどこで実行しても問題がない、例えばyumのを参照したい場合\n1  ansible-doc yum   Extra Moduleに対しては、extra moduleの利用を設定したディレクトリの下でコマンドを実行する。\n1  ansible-doc yum_repository   より良いPlaybookの書き方  Includeとroleを使って、重複がないコードを書く 大きいファイルを小さいファイルに分割して書く  https://github.com/ansible/ansible-examples\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38  production # inventory file for production servers staging # inventory file for staging environment group_vars/ group1 # here we assign variables to particular groups group2 # \u0026#34;\u0026#34; host_vars/ hostname1 # if systems need specific variables, put them here hostname2 # \u0026#34;\u0026#34; library/ # if any custom modules, put them here (optional) filter_plugins/ # if any custom filter plugins, put them here (optional) site.yml # master playbook webservers.yml # playbook for webserver tier dbservers.yml # playbook for dbserver tier roles/ common/ # this hierarchy represents a \u0026#34;role\u0026#34; tasks/ # main.yml # \u0026lt;-- tasks file can include smaller files if warranted handlers/ # main.yml # \u0026lt;-- handlers file templates/ # \u0026lt;-- files for use with the template resource ntp.conf.j2 # \u0026lt;------- templates end in .j2 files/ # bar.txt # \u0026lt;-- files for use with the copy resource foo.sh # \u0026lt;-- script files for use with the script resource vars/ # main.yml # \u0026lt;-- variables associated with this role defaults/ # main.yml # \u0026lt;-- default lower priority variables for this role meta/ # main.yml # \u0026lt;-- role dependencies webtier/ # same kind of structure as \u0026#34;common\u0026#34; was above, done for the webtier role monitoring/ # \u0026#34;\u0026#34; fooapp/ # \u0026#34;\u0026#34;   参考資料 Ansibleのビデオセッション(英文) https://sysadmincasts.com/episodes/43-19-minutes-with-ansible-part-1-4\n本資料にあるすべての例\nhttps://github.com/shijingjing1221/ansible-first-book-examples/\nYAMLの基本文法と紹介\nhttps://en.wikipedia.org/wiki/YAML\nhttp://www.yamllint.com/\nAnsible Tower紹介\nhttps://www.ansible.com/tower\nCopyRight 此书电子版免费供大家下载阅读，如果您已为此副本付费，请立即申请退款并联系作者举报此行为。请注意，虽然此书电子版免费供大家阅读，但这并不代表作者放弃了版权，您在未经授权的情况下依然不得以任何方式复制或抄袭本书内容。此书的电子版目前仅授权图灵社区和gitbook.com两个平台发布，如果您通过其他渠道获取到了此副本，则是侵权行为，请到上述两个平台下载合法授权的副本。获取合法授权副本的好处是可以及时得到此书的最新版本，早期版本中的错误会被及时纠正。感谢您对版权保护工作所做出的贡献。\n作者邮箱:shijingjing02@163.com\n作者Github: https://github.com/shijingjing1221/\nDemo https://www.youtube.com/watch?v=vOa4_1fgNPU\n","permalink":"https://wenhan.blog/post/getting-started-with-ansible-jp-adv/","summary":"こちらの記事はではAnsibleを紹介するための文章で、元Red Hat社員のJingjing Shiが作成し、Wenhan Shiが日本語に翻訳しました。内容の校正はHideki SaitoとKento Yagisawaが協力しています。Ansibleの基礎知識から、実際の現場で利用できる実運用まで紹介しています。\n本文内にあるすべてのansible playbookの例は、以下のgithubのURLから利用できる。 https://github.com/ansible-book/ansible-first-book-examples もし不備やコメントがありましたら、作者shijingjing02@163.comまたは翻訳者shibunkan@gmail.com に連絡してください。\n詳細の内容を下記三つに分けて説明します。\nAnsible 入門 - 紹介 Ansible 入門 - 基本 Ansible 入門 - 応用\n本章では、もっと自由に運用するために、Ansibleの高度な使い方を説明する。\n Ansibleの設定ファイル サーバーリスト管理（Host Inventory） Playbookの上級な書き方 Extraモジュールの利用  Ansible設定ファイル 設定内容  基本の設定内容     項目 詳細 内容（デフォルト）     inventory リモートノードリスト管理ファイル /etc/ansible/hosts   library 拡張モジュールフォルダ /usr/share/my_modules/   remote_tmp リモートノード上のファイル一時保存場所 $HOME/.ansible/tmp   local_tmp 管理者ノード上のファイル一時保存場所 $HOME/.ansible/tmp     高度の設定内容     項目 詳細 内容（デフォルト）     accelerate_port 接続ポート番号 5099   accelerate_timeout タイムアウト 30   accelerate_connect_timeout 接続タイムアウト 5    上記は設定できる内容の一部しかすぎない。以下のAnsible設定ファイルの全体を通して、どんなことができるのはは理解できる","title":"Ansible 入門 - 応用"},{"content":"こちらの記事はではAnsibleを紹介するための文章で、元Red Hat社員のJingjing Shiが作成し、Wenhan Shiが日本語に翻訳しました。内容の校正はHideki SaitoとKento Yagisawaが協力しています。Ansibleの基礎知識から、実際の現場で利用できる実運用まで紹介しています。\n本文内にあるすべてのansible playbookの例は、以下のgithubのURLから利用できる。 https://github.com/ansible-book/ansible-first-book-examples もし不備やコメントがありましたら、作者shijingjing02@163.comまたは翻訳者shibunkan@gmail.com に連絡してください。\n詳細の内容を下記三つに分けて説明します。\nAnsible 入門 - 紹介 Ansible 入門 - 基本 Ansible 入門 - 応用\n本章では、Ansibleの基礎部分の紹介を説明する。\nAnsibleとは Ansibleとはなにか？ Ansibleは、複数のサーバを一括でコントロールする構成管理ツールである。コントロールできるサーバ対象はリモートの仮想マシンでも物理マシンでも問ローカルのサーバマシンも問題ない。\nAnsibleでなにができる？ Ansibleでは、SSHなどを利用し管理ノードからリモートノードへの通信を行う。理論上では、サーバ管理者がsshでサーバにログインした後できるすべてのことに対し、Ansibleも同じことができる。\n例えば\n ファイルコピー パッケージインストール デーモンサービス起動 その他  アーキテクチャ 管理者ノードとリモートノードの間では、SSHプロトコルを利用して通信を行っている。そのため、Ansible環境を設定する時に、管理者ノードからリモートノードへSSHログインできることが必要である。ただし、SSHログインはパスワードレスに設定しなければいけない。詳細な設定方法は後ほど説明する。\nSSH接続 管理者ノードでAnsibleをインストールし、スクリプトの編集を行う。管理者ノードでAnsibleのコマンドやスクリプトを実行する時に、管理対象のサーバにSSHで接続する。管理対象のサーバの上にパッケージをインストールする必要がない。 多種類のサーバに対応 AnsibleはRedHat系、Debian系のLinuxと、Windows系のサーバを同時に管理することができる。管理者ノードはスクリプトを実行する時のみリモートサーバに接続するため、他の同期処理がない。そのため、通常の状態では、停電などの異常状態はAnsibleを影響しない。 Ansible Towerのアーキテクチャ なぜAnsible Towerが必要なのか？ Ansible Towerは企業向けの有償ソフトウェアである。 前章のAnsible アーキテクチャとこれからのAnsibleのインストールでは、すべてのAnsibleでの管理対象となるサーバは、sshの公開鍵認証によるパスワードレスSSH接続を設定する必要がある。一般ユーザの場合、数台の仮想サーバやリモートサーバのみを管理するため特に問題ないが、企業ユーザに対し業務プロセスと安全性を確保しにくくなる。\n 1台のリモートサーバを追加する毎に、パスワードレスなSSH接続の設定を手動で行うことは非常に非効率的である。企業レベルのサーバが数百台、数千台規模になると、各サーバ管理者が自分の管理者ノードで全サーバに対しパスワードレスSSH接続の設定が必要となり、作業量は膨大になる。 管理者がパスワードレスSSH接続の為のssh keyを入手したり、他人にコピーしたりすると、本番環境に対し重大なセキュリティの問題になる。  Ansible Towerでなにができるのか？ Ansible Towerは企業ユーザ向けに開発されている、集中管理や権限による制限、ジョブスケジューリング機能などを提供するソフトウェアである。このソフトウェアは管理者にWebUIやREST APIを提供し、Playbookの実行やワークフローテンプレートによる条件分岐などをサポートする。\n 管理者は、Ansible Tower上にサーバのssh keyを使用したりシェアしたりすることができるが、ssh keyの内容参照やコピーすることはできない。 Ansible Towerでは、各管理者がシェアしているplaybookスクリプトを参照するとこができるため、重複する作業を減らすことができる。 また、Ansible Towerは現在すべてのサーバでのplaybookの実行状況を集計/表示するため、状態の確認や統計もできる。  下記の図でAnsible Towerのアーキテクチャを示している。\n","permalink":"https://wenhan.blog/post/getting-started-with-ansible-jp-intro/","summary":"こちらの記事はではAnsibleを紹介するための文章で、元Red Hat社員のJingjing Shiが作成し、Wenhan Shiが日本語に翻訳しました。内容の校正はHideki SaitoとKento Yagisawaが協力しています。Ansibleの基礎知識から、実際の現場で利用できる実運用まで紹介しています。\n本文内にあるすべてのansible playbookの例は、以下のgithubのURLから利用できる。 https://github.com/ansible-book/ansible-first-book-examples もし不備やコメントがありましたら、作者shijingjing02@163.comまたは翻訳者shibunkan@gmail.com に連絡してください。\n詳細の内容を下記三つに分けて説明します。\nAnsible 入門 - 紹介 Ansible 入門 - 基本 Ansible 入門 - 応用\n本章では、Ansibleの基礎部分の紹介を説明する。\nAnsibleとは Ansibleとはなにか？ Ansibleは、複数のサーバを一括でコントロールする構成管理ツールである。コントロールできるサーバ対象はリモートの仮想マシンでも物理マシンでも問ローカルのサーバマシンも問題ない。\nAnsibleでなにができる？ Ansibleでは、SSHなどを利用し管理ノードからリモートノードへの通信を行う。理論上では、サーバ管理者がsshでサーバにログインした後できるすべてのことに対し、Ansibleも同じことができる。\n例えば\n ファイルコピー パッケージインストール デーモンサービス起動 その他  アーキテクチャ 管理者ノードとリモートノードの間では、SSHプロトコルを利用して通信を行っている。そのため、Ansible環境を設定する時に、管理者ノードからリモートノードへSSHログインできることが必要である。ただし、SSHログインはパスワードレスに設定しなければいけない。詳細な設定方法は後ほど説明する。\nSSH接続 管理者ノードでAnsibleをインストールし、スクリプトの編集を行う。管理者ノードでAnsibleのコマンドやスクリプトを実行する時に、管理対象のサーバにSSHで接続する。管理対象のサーバの上にパッケージをインストールする必要がない。 多種類のサーバに対応 AnsibleはRedHat系、Debian系のLinuxと、Windows系のサーバを同時に管理することができる。管理者ノードはスクリプトを実行する時のみリモートサーバに接続するため、他の同期処理がない。そのため、通常の状態では、停電などの異常状態はAnsibleを影響しない。 Ansible Towerのアーキテクチャ なぜAnsible Towerが必要なのか？ Ansible Towerは企業向けの有償ソフトウェアである。 前章のAnsible アーキテクチャとこれからのAnsibleのインストールでは、すべてのAnsibleでの管理対象となるサーバは、sshの公開鍵認証によるパスワードレスSSH接続を設定する必要がある。一般ユーザの場合、数台の仮想サーバやリモートサーバのみを管理するため特に問題ないが、企業ユーザに対し業務プロセスと安全性を確保しにくくなる。\n 1台のリモートサーバを追加する毎に、パスワードレスなSSH接続の設定を手動で行うことは非常に非効率的である。企業レベルのサーバが数百台、数千台規模になると、各サーバ管理者が自分の管理者ノードで全サーバに対しパスワードレスSSH接続の設定が必要となり、作業量は膨大になる。 管理者がパスワードレスSSH接続の為のssh keyを入手したり、他人にコピーしたりすると、本番環境に対し重大なセキュリティの問題になる。  Ansible Towerでなにができるのか？ Ansible Towerは企業ユーザ向けに開発されている、集中管理や権限による制限、ジョブスケジューリング機能などを提供するソフトウェアである。このソフトウェアは管理者にWebUIやREST APIを提供し、Playbookの実行やワークフローテンプレートによる条件分岐などをサポートする。\n 管理者は、Ansible Tower上にサーバのssh keyを使用したりシェアしたりすることができるが、ssh keyの内容参照やコピーすることはできない。 Ansible Towerでは、各管理者がシェアしているplaybookスクリプトを参照するとこができるため、重複する作業を減らすことができる。 また、Ansible Towerは現在すべてのサーバでのplaybookの実行状況を集計/表示するため、状態の確認や統計もできる。  下記の図でAnsible Towerのアーキテクチャを示している。","title":"Ansible 入門 - 紹介"},{"content":"There are 2 kinds of healing functions in GlusterFS, server-side heal and client-side heal.\nServer side heal is automatically executed by self-heal daemon on all gluster server nodes. It does healing by crawling file/directory information from .glusterfs directory on brick path. So it will keep the file-data and meta-data to be consistent from server side.\nClient side heal is different, it will triggers heal for the particular file whenever client accesses files from mount path, which means a file operation on file descriptor. It will lead to some performance impact as every file access operation will go through extra set of function calls for file check and heal. Client-side heal is enable by default, and can be turned off by below command\nFile data\n1  # gluster volume set \u0026lt;volume name\u0026gt; cluster.data-self-heal off   Entry data (contents/entries of a directory)\n1  # gluster volume set \u0026lt;volume name\u0026gt; cluster.entry-self-heal off   Meta data\n1  # gluster volume set \u0026lt;volume name\u0026gt; cluster.metadata-self-heal off   Note that turn off client side healing doesn\u0026rsquo;t mean to compromise data integrity and consistency. For file read pending xattr is evaluated for replica copies and read will only served from correct copy. For file write new data will be written on both replica bricks, and self-heal daemon on gluster node will take care of these and will do healing if needed.\n","permalink":"https://wenhan.blog/post/the-different-between-server-side-heal-and-client-side-heal-in-glusterfs/","summary":"There are 2 kinds of healing functions in GlusterFS, server-side heal and client-side heal.\nServer side heal is automatically executed by self-heal daemon on all gluster server nodes. It does healing by crawling file/directory information from .glusterfs directory on brick path. So it will keep the file-data and meta-data to be consistent from server side.\nClient side heal is different, it will triggers heal for the particular file whenever client accesses files from mount path, which means a file operation on file descriptor.","title":"The difference between server-side healing and client-side healing in GlusterFS"},{"content":"I hit an error when running yum update on my centOS 7, the command failed to update my OS!\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31  # yum update Loaded plugins: fastestmirror Determining fastest mirrors * base: centos.gbeservers.com * epel: linux.mirrors.es.net * extras: linux.mirrors.es.net * ius: hkg.mirror.rackspace.com * updates: mirror.atlantic.net File \u0026#34;/usr/libexec/urlgrabber-ext-down\u0026#34;, line 28 except OSError, e: ^ SyntaxError: invalid syntax File \u0026#34;/usr/libexec/urlgrabber-ext-down\u0026#34;, line 28 except OSError, e: ^ SyntaxError: invalid syntax File \u0026#34;/usr/libexec/urlgrabber-ext-down\u0026#34;, line 28 except OSError, e: ^ SyntaxError: invalid syntax File \u0026#34;/usr/libexec/urlgrabber-ext-down\u0026#34;, line 28 except OSError, e: ^ SyntaxError: invalid syntax File \u0026#34;/usr/libexec/urlgrabber-ext-down\u0026#34;, line 28 except OSError, e: ^ SyntaxError: invalid syntax Exiting on user cancel   Looks like something is wrong with the system. After some research, I notice the root cause is the missmatch of python version. \u0026lsquo;yum\u0026rsquo; command need python 2.7 on /usr/bin/python symbol link, which I changed to python 3.6 before.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18  # head /usr/bin/yum #!/usr/bin/python import sys try: import yum except ImportError: print \u0026gt;\u0026gt; sys.stderr, \u0026#34;\u0026#34;\u0026#34;\\ There was a problem importing one of the Python modules required to run yum. The error leading to this problem was: %s # ll /usr/bin/python* lrwxrwxrwx. 1 root root 9 Jul 27 2017 python -\u0026gt; python3.6 lrwxrwxrwx. 1 root root 9 Jul 26 2017 python2 -\u0026gt; python2.7 -rwxr-xr-x. 1 root root 7136 Nov 6 2016 python2.7 -rwxr-xr-x. 2 root root 11312 Apr 7 2017 python3.6 -rwxr-xr-x. 2 root root 11312 Apr 7 2017 python3.6m   The workaround is to change the first line of /usr/bin/yum to use python2\n1  #!/usr/bin/python2   ","permalink":"https://wenhan.blog/post/failed-at-yum-update-and-how-to-fix-it/","summary":"I hit an error when running yum update on my centOS 7, the command failed to update my OS!\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31  # yum update Loaded plugins: fastestmirror Determining fastest mirrors * base: centos.gbeservers.com * epel: linux.mirrors.es.net * extras: linux.mirrors.es.net * ius: hkg.","title":"failed at yum update and how to fix it"},{"content":"Base on CentOS 7, Docker 1.12.6\nInstall Docker Some pre-requirment to install Docker to a Linux OS.\n Docker only can be installed on a 64 bit OS. Kernel version over 3.10 is recommended. recommend Need to enable cgroup and namespace.  Now it is easier to install Docker in CentOS or Fedora, it can be searched by yum command like below:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19  # yum search docker | grep ^docker docker-client.x86_64 : Client side files for Docker docker-client-latest.x86_64 : Client side files for Docker docker-common.x86_64 : Common files for docker and docker-latest docker-compose.noarch : Multi-container orchestration for Docker docker-distribution.x86_64 : Docker toolset to pack, ship, store, and deliver docker-latest-logrotate.x86_64 : cron job to run logrotate on Docker containers docker-latest-v1.10-migrator.x86_64 : Calculates SHA256 checksums for docker docker-logrotate.x86_64 : cron job to run logrotate on Docker containers docker-lvm-plugin.x86_64 : Docker volume driver for lvm volumes docker-python.x86_64 : An API client for docker written in Python docker-registry.x86_64 : Registry server for Docker docker-v1.10-migrator.x86_64 : Calculates SHA256 checksums for docker layer docker.x86_64 : Automates deployment of containerized applications docker-devel.x86_64 : A golang registry for global request variables (source docker-forward-journald.x86_64 : Forward stdin to journald docker-latest.x86_64 : Automates deployment of containerized applications docker-novolume-plugin.x86_64 : Block container starts with local volumes docker-unit-test.x86_64 : Automates deployment of containerized applications -   Running yum -y install docker to install docker to your system.\n1 2 3 4 5 6 7 8  # yum -y install docker ...snip... Installed: docker.x86_64 2:1.12.6-71.git3e8e77d.el7.centos.1 Complete!   After the installation, start docker service and run docker version to confirm your setup.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20  # systemctl start docker # docker version Client: Version: 1.12.6 API version: 1.24 Package version: docker-1.12.6-71.git3e8e77d.el7.centos.1.x86_64 Go version: go1.8.3 Git commit: 3e8e77d/1.12.6 Built: Tue Jan 30 09:17:00 2018 OS/Arch: linux/amd64 Server: Version: 1.12.6 API version: 1.24 Package version: docker-1.12.6-71.git3e8e77d.el7.centos.1.x86_64 Go version: go1.8.3 Git commit: 3e8e77d/1.12.6 Built: Tue Jan 30 09:17:00 2018 OS/Arch: linux/amd64   Basic docker command Here will introduce some basic command to use docker, for more detail please refer (Docker Documentation)[https://docs.docker.com/engine/reference/commandline/docker/].\nDocker command will help you to communicate with docker daemon. The docker daemon will listen and execute docker command. run docker or docker --help to get the command list.\n1 2 3 4 5 6 7 8 9 10 11 12  # docker --help ... Commands: attach Attach to a running container build Build an image from a Dockerfile commit Create a new image from a container\u0026#39;s changes ... version Show the Docker version information volume Manage Docker volumes wait Block until a container stops, then print its exit code Run \u0026#39;docker COMMAND --help\u0026#39; for more information on a command.   Running docker command needs root permission.\n","permalink":"https://wenhan.blog/post/docker-basic-foundation/","summary":"Base on CentOS 7, Docker 1.12.6\nInstall Docker Some pre-requirment to install Docker to a Linux OS.\n Docker only can be installed on a 64 bit OS. Kernel version over 3.10 is recommended. recommend Need to enable cgroup and namespace.  Now it is easier to install Docker in CentOS or Fedora, it can be searched by yum command like below:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19  # yum search docker | grep ^docker docker-client.","title":"Docker basic foundation"},{"content":"Although the container/pod in OpenShift transfer data by IPv4 protocol, and you do not need to worry about the setting of IPv6. But in some case people want to disable IPv6 inside the container without effecting other container/pods or host OS.\nHere is an example of the IPv6 info outputed from a container.\n1 2 3 4 5 6 7 8 9 10 11 12 13  [root@ocp37 ~]# oc exec django-ex-4-6gmsj -- ip a 1: lo: \u0026lt;LOOPBACK,UP,LOWER_UP\u0026gt; mtu 65536 qdisc noqueue state UNKNOWN qlen 1 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 inet 127.0.0.1/8 scope host lo valid_lft forever preferred_lft forever inet6 ::1/128 scope host valid_lft forever preferred_lft forever 3: eth0@if45: \u0026lt;BROADCAST,MULTICAST,UP,LOWER_UP\u0026gt; mtu 1450 qdisc noqueue state UP link/ether 0a:58:0a:80:00:24 brd ff:ff:ff:ff:ff:ff link-netnsid 0 inet 10.128.0.36/23 brd 10.128.1.255 scope global eth0 valid_lft forever preferred_lft forever inet6 fe80::a4e3:f9ff:fe55:61db/64 scope link valid_lft forever preferred_lft forever   It is not allowed to run sysctl -w to update a kernel parameter inside a container for security.\n1 2 3  [root@ocp37 ~]# oc exec django-ex-4-6gmsj -- sysctl -w net.ipv6.conf.all.disable_ipv6=1 sysctl: setting key \u0026#34;net.ipv6.conf.all.disable_ipv6\u0026#34;: Read-only file system command terminated with exit code 255   So what you need to do is to change the kubernetes settings in the DeploymentConfig. Sysctl settings are exposed via Kubernetes, allowing users to modify certain kernel parameters at runtime for namespaces within a container. Only sysctls that are namespaced can be set independently on pods; For namespaced sysctl, please refer here for detail.\nHere are the steps:\n  Add below setting to kubeletArguments field in the /etc/origin/node/node-config.yaml file. This will enable Unsafe Sysctls.\n1 2 3  kubeletArguments: experimental-allowed-unsafe-sysctls: - net.ipv6.conf.all.disable_ipv6     Restart the node service to apply the changes:\n1  # systemctl restart atomic-openshift-node     Edit DeploymentConfig of the target pod.\n1  # oc edit dc/\u0026lt;DeploymentConfig of your pod\u0026gt;     Add below settings to the metadata filed inside of template filed, then save and quit. (You may need to create annotations filed if it is not exist.)\n1 2 3 4 5 6  spec: .... template: metadata: annotations: security.alpha.kubernetes.io/unsafe-sysctls: net.ipv6.conf.all.disable_ipv6=1     Deploy a new container/pod using this updated DeploymentConfig\n1  # oc deploy dc/\u0026lt;DeploymentConfig of your pod\u0026gt; --latest     When the pod is ready, confirm ipv6 is diabled.\n1 2 3 4 5 6 7 8 9  [root@ocp37 ~]# oc exec django-ex-2-22znd -- ip a 1: lo: \u0026lt;LOOPBACK,UP,LOWER_UP\u0026gt; mtu 65536 qdisc noqueue state UNKNOWN qlen 1 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 inet 127.0.0.1/8 scope host lo valid_lft forever preferred_lft forever 3: eth0@if41: \u0026lt;BROADCAST,MULTICAST,UP,LOWER_UP\u0026gt; mtu 1450 qdisc noqueue state UP link/ether 0a:58:0a:80:00:20 brd ff:ff:ff:ff:ff:ff link-netnsid 0 inet 10.128.0.32/23 brd 10.128.1.255 scope global eth0 valid_lft forever preferred_lft forever     ","permalink":"https://wenhan.blog/post/how-to-disable-ipv6-inside-a-container-pod-in-openshift/","summary":"Although the container/pod in OpenShift transfer data by IPv4 protocol, and you do not need to worry about the setting of IPv6. But in some case people want to disable IPv6 inside the container without effecting other container/pods or host OS.\nHere is an example of the IPv6 info outputed from a container.\n1 2 3 4 5 6 7 8 9 10 11 12 13  [root@ocp37 ~]# oc exec django-ex-4-6gmsj -- ip a 1: lo: \u0026lt;LOOPBACK,UP,LOWER_UP\u0026gt; mtu 65536 qdisc noqueue state UNKNOWN qlen 1 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 inet 127.","title":"How to disable IPv6 inside a container/pod in OpenShift"},{"content":"create test file Mount gluster volume and create a file\n1 2 3 4  [root@client-1 ~]# mount -t glusterfs gluster-node-1:/vol /mnt [root@client-1 ~]# mkdir -p /mnt/hoge/hello-gluster/ [root@client-1 ~]# touch /mnt/hoge/hello-gluster/file [root@client-1 ~]# umount /mnt/   Get GFID of a file in Gluster volume From brick directory Login to gluster node and locate file in brick directory.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17  [root@gluster-node-1 ~]# gluster volume info vol Volume Name: vol Type: Replicate Volume ID: 4ac36bcc-7127-48c4-ac21-421850d8bc47 Status: Started Snapshot Count: 0 Number of Bricks: 1 x 3 = 3 Transport-type: tcp,rdma Bricks: Brick1: gluster-node-1:/gluster/brick-vol \u0026lt;-- confirm the brick path Brick2: gluster-node-2:/gluster/brick-vol Brick3: gluster-node-3:/gluster/brick-vol Options Reconfigured: performance.readdir-ahead: on nfs.disable: off cluster.server-quorum-ratio: 51%   confirm the file path in the brick, GFID can be checked by \u0026ldquo;trusted.gfid\u0026rdquo;\n1 2 3 4 5 6 7  [root@gluster-node-1 ~]# ls /gluster/brick-vol/hoge/hello-gluster/file /gluster/brick-vol/hoge/hello-gluster/file [root@gluster-node-1 ~]# getfattr -d -m . -e hex /gluster/brick-vol/hoge/hello-gluster/file getfattr: Removing leading \u0026#39;/\u0026#39; from absolute path names # file: gluster/brick-vol/hoge/hello-gluster/file security.selinux=0x73797374656d5f753a6f626a6563745f723a756e6c6162656c65645f743a733000 trusted.gfid=0xd8efc4c4d6204170ad293eda97d3d2e4   From client side Mount gluster volume with -o aux-gfid-mount option\n1  [root@gluster-node-1 ~]# mount -t glusterfs -o aux-gfid-mount gluster-node-1:/vol /mnt   confirm the file path in the mount point, GFID can be checked by glusterfs.gfid.string\n1 2 3 4  [root@gluster-node-1 ~]# getfattr -n glusterfs.gfid.string /mnt/hoge/hello-gluster/file getfattr: Removing leading \u0026#39;/\u0026#39; from absolute path names # file: mnt/hoge/hello-gluster/file glusterfs.gfid.string=\u0026#34;d8efc4c4-d620-4170-ad29-3eda97d3d2e4\u0026#34;   Convert GFID to path name By tool Use https://gist.github.com/semiosis/4392640. NOTE, GFID must be the format of XXXXXXXX-XXXX-XXXX-XXXX-XXXXXXXXXXXX\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23  bash gfid-resolver.sh -h Glusterfs GFID resolver -- turns a GFID into a real file path Usage: gfid-resolver.sh \u0026lt;brick-path\u0026gt; \u0026lt;gfid\u0026gt; [-q] \u0026lt;brick-path\u0026gt; : the path to your glusterfs brick (required) \u0026lt;gfid\u0026gt; : the gfid you wish to resolve to a real path (required) -q : quieter output (optional) with this option only the actual resolved path is printed. without this option gfid-resolver.sh will print the GFID, whether it identifies a file or directory, and the resolved path to the real file or directory. Theory: The .glusterfs directory in the brick root has files named by GFIDs If the GFID identifies a directory, then this file is a symlink to the actual directory. If the GFID identifies a file then this file is a hard link to the actual file. [root@gluster-node-1 ~]# bash gfid-resolver.sh /gluster/brick-vol d8efc4c4-d620-4170-ad29-3eda97d3d2e4 d8efc4c4-d620-4170-ad29-3eda97d3d2e4 == File: /gluster/brick-vol/hoge/hello-gluster/file   ","permalink":"https://wenhan.blog/post/gluster-filename-and-gfid/","summary":"create test file Mount gluster volume and create a file\n1 2 3 4  [root@client-1 ~]# mount -t glusterfs gluster-node-1:/vol /mnt [root@client-1 ~]# mkdir -p /mnt/hoge/hello-gluster/ [root@client-1 ~]# touch /mnt/hoge/hello-gluster/file [root@client-1 ~]# umount /mnt/   Get GFID of a file in Gluster volume From brick directory Login to gluster node and locate file in brick directory.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17  [root@gluster-node-1 ~]# gluster volume info vol Volume Name: vol Type: Replicate Volume ID: 4ac36bcc-7127-48c4-ac21-421850d8bc47 Status: Started Snapshot Count: 0 Number of Bricks: 1 x 3 = 3 Transport-type: tcp,rdma Bricks: Brick1: gluster-node-1:/gluster/brick-vol \u0026lt;-- confirm the brick path Brick2: gluster-node-2:/gluster/brick-vol Brick3: gluster-node-3:/gluster/brick-vol Options Reconfigured: performance.","title":"Gluster filename and GFID interconversion"},{"content":"Apart of date, awk also has below built-in time functions which will help you to resolve time convert problem\nsystime() This will return the current time as the number of seconds since the Epoch (1970-01-01 00:00:00).\n1 2 3 4  $ awk \u0026#39;BEGIN { \u0026gt; print \u0026#34;Number of seconds since the Epoch = \u0026#34; systime() \u0026gt; }\u0026#39; Number of seconds since the Epoch = 1511480989   mktime(YYYY MM DD HH MM SS) This will convert date string \u0026ldquo;YYYY MM DD HH MM SS\u0026rdquo; to the number of seconds since the Epoch. It is the same output with systime.\n1 2 3 4  $ awk \u0026#39;BEGIN { print \u0026#34;Number of seconds since the Epoch = \u0026#34; mktime(\u0026#34;2017 11 24 08 52 10\u0026#34;) }\u0026#39; Number of seconds since the Epoch = 1511481130   strftime() This will formats timestamps according to the specfification in format.\n1 2 3 4  $ awk \u0026#39;BEGIN { \u0026gt; print strftime(\u0026#34;Time = %m/%d/%Y %H:%M:%S\u0026#34;, systime()) \u0026gt; }\u0026#39; Time = 11/24/2017 08:54:15   Below time formats are supported by AWK:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127  %a The locale’s abbreviated weekday name. %A The locale’s full weekday name. %b The locale’s abbreviated month name. %B The locale’s full month name. %c The locale’s “appropriate” date and time representation. (This is ‘%A %B %d %T %Y’ in the \u0026#34;C\u0026#34; locale.) %C The century part of the current year. This is the year divided by 100 and truncated to the next lower integer. %d The day of the month as a decimal number (01–31). %D Equivalent to specifying ‘%m/%d/%y’. %e The day of the month, padded with a space if it is only one digit. %F Equivalent to specifying ‘%Y-%m-%d’. This is the ISO 8601 date format. %g The year modulo 100 of the ISO 8601 week number, as a decimal number (00–99). For example, January 1, 2012, is in week 53 of 2011. Thus, the year of its ISO 8601 week number is 2011, even though its year is 2012. Similarly, December 31, 2012, is in week 1 of 2013. Thus, the year of its ISO week number is 2013, even though its year is 2012. %G The full year of the ISO week number, as a decimal number. %h Equivalent to ‘%b’. %H The hour (24-hour clock) as a decimal number (00–23). %I The hour (12-hour clock) as a decimal number (01–12). %j The day of the year as a decimal number (001–366). %m The month as a decimal number (01–12). %M The minute as a decimal number (00–59). %n A newline character (ASCII LF). %p The locale’s equivalent of the AM/PM designations associated with a 12-hour clock. %r The locale’s 12-hour clock time. (This is ‘%I:%M:%S %p’ in the \u0026#34;C\u0026#34; locale.) %R Equivalent to specifying ‘%H:%M’. %S The second as a decimal number (00–60). %t A TAB character. %T Equivalent to specifying ‘%H:%M:%S’. %u The weekday as a decimal number (1–7). Monday is day one. %U The week number of the year (with the first Sunday as the first day of week one) as a decimal number (00–53). %V The week number of the year (with the first Monday as the first day of week one) as a decimal number (01–53). The method for determining the week number is as specified by ISO 8601. (To wit: if the week containing January 1 has four or more days in the new year, then it is week one; otherwise it is the last week [52 or 53] of the previous year and the next week is week one.) %w The weekday as a decimal number (0–6). Sunday is day zero. %W The week number of the year (with the first Monday as the first day of week one) as a decimal number (00–53). %x The locale’s “appropriate” date representation. (This is ‘%A %B %d %Y’ in the \u0026#34;C\u0026#34; locale.) %X The locale’s “appropriate” time representation. (This is ‘%T’ in the \u0026#34;C\u0026#34; locale.) %y The year modulo 100 as a decimal number (00–99). %Y The full year as a decimal number (e.g., 2015). %z The time zone offset in a ‘+HHMM’ format (e.g., the format necessary to produce RFC 822/RFC 1036 date headers). %Z The time zone name or abbreviation; no characters if no time zone is determinable. %Ec %EC %Ex %EX %Ey %EY %Od %Oe %OH %OI %Om %OM %OS %Ou %OU %OV %Ow %OW %Oy “Alternative representations” for the specifications that use only the second letter (‘%c’, ‘%C’, and so on).57 (These facilitate compliance with the POSIX date utility.) %% A literal ‘%’. If a conversion specifier is not one of those just listed, the behavior is undefined.58 For systems that are not yet fully standards-compliant, gawk supplies a copy of strftime() from the GNU C Library. It supports all of the just-listed format specifications. If that version is used to compile gawk (see Installation), then the following additional format specifications are available: %k The hour (24-hour clock) as a decimal number (0–23). Single-digit numbers are padded with a space. %l The hour (12-hour clock) as a decimal number (1–12). Single-digit numbers are padded with a space. %s The time as a decimal timestamp in seconds since the epoch.   refers : The GNU Awk User’s Guide: Time Functions AWK - Time Functions\n","permalink":"https://wenhan.blog/post/awk-time-functions/","summary":"Apart of date, awk also has below built-in time functions which will help you to resolve time convert problem\nsystime() This will return the current time as the number of seconds since the Epoch (1970-01-01 00:00:00).\n1 2 3 4  $ awk \u0026#39;BEGIN { \u0026gt; print \u0026#34;Number of seconds since the Epoch = \u0026#34; systime() \u0026gt; }\u0026#39; Number of seconds since the Epoch = 1511480989   mktime(YYYY MM DD HH MM SS) This will convert date string \u0026ldquo;YYYY MM DD HH MM SS\u0026rdquo; to the number of seconds since the Epoch.","title":"AWK - time functions"},{"content":"Linux will detects SSD automatically. Since kernel version 2.6.29, you can check /dev/sda with following command\n1  # cat /sys/block/sda/queue/rotational   The return number 0 shows you /dev/sda is a SSD, and 1 shows it is a HDD. Note that this command may not work when your disk is created by hardware RAID.\nAnother way is to use lsblk command, a part of the util-linux package.\n1 2 3 4  # lsblk -d -o name,rota NAME ROTA sda 0 sdb 1   ROTA means rotational device, 1 for true, 0 for false. This command report the same information as in /sys/block/.../queue/rotational\nRef : How to know if a disk is an SSD or an HDD\n","permalink":"https://wenhan.blog/post/how-to-check-if-a-disk-is-ssd-or-hdd/","summary":"Linux will detects SSD automatically. Since kernel version 2.6.29, you can check /dev/sda with following command\n1  # cat /sys/block/sda/queue/rotational   The return number 0 shows you /dev/sda is a SSD, and 1 shows it is a HDD. Note that this command may not work when your disk is created by hardware RAID.\nAnother way is to use lsblk command, a part of the util-linux package.\n1 2 3 4  # lsblk -d -o name,rota NAME ROTA sda 0 sdb 1   ROTA means rotational device, 1 for true, 0 for false.","title":"How to check if a disk is SSD or HDD"},{"content":"ps command has a --sort option which can help you to sort processes.\n1 2 3 4 5 6 7  --sort spec Specify sorting order. Sorting syntax is [+|-]key[,[+|-]key[,...]]. Choose a multi-letter key from the STANDARD FORMAT SPECIFIERS section. The \u0026#34;+\u0026#34; is optional since default direction is increasing numerical or lexicographic order. Identical to k. For example: ps jax --sort=uid,-ppid, +pid   Sort ps output by memory From high to low The highest is at the top of the command\n1  # ps aux --sort -rss   From low to high The highest is at the bottom of the command\n1  # ps aux --sort rss   Sort ps output by cpu usage From high to low The highest is at the top of the command\n1  # ps aux --sort -pcpu   From low to high The highest is at the bottom of the command\n1  # ps aux --sort rss   other sorting specifiers Check the man page of ps command.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426  STANDARD FORMAT SPECIFIERS Here are the different keywords that may be used to control the output format (e.g. with option -o) or to sort the selected processes with the GNU-style --sort option. For example: ps -eo pid,user,args --sort user This version of ps tries to recognize most of the keywords used in other implementations of ps. The following user-defined format specifiers may contain spaces: args, cmd, comm, command, fname, ucmd, ucomm, lstart, bsdstart, start. Some keywords may not be available for sorting. CODE HEADER DESCRIPTION %cpu %CPU cpu utilization of the process in \u0026#34;##.#\u0026#34; format. Currently, it is the CPU time used divided by the time the process has been running (cputime/realtime ratio), expressed as a percentage. It will not add up to 100% unless you are lucky. (alias pcpu). %mem %MEM ratio of the process\u0026#39;s resident set size to the physical memory on the machine, expressed as a percentage. (alias pmem). args COMMAND command with all its arguments as a string. Modifications to the arguments may be shown. The output in this column may contain spaces. A process marked \u0026lt;defunct\u0026gt; is partly dead, waiting to be fully destroyed by its parent. Sometimes the process args will be unavailable; when this happens, ps will instead print the executable name in brackets. (alias cmd, command). See also the comm format keyword, the -f option, and the c option. When specified last, this column will extend to the edge of the display. If ps can not determine display width, as when output is redirected (piped) into a file or another command, the output width is undefined (it may be 80, unlimited, determined by the TERM variable, and so on). The COLUMNS environment variable or --cols option may be used to exactly determine the width in this case. The w or -w option may be also be used to adjust width. blocked BLOCKED mask of the blocked signals, see signal(7). According to the width of the field, a 32 or 64-bit mask in hexadecimal format is displayed. (alias sig_block, sigmask). bsdstart START time the command started. If the process was started less than 24 hours ago, the output format is \u0026#34; HH:MM\u0026#34;, else it is \u0026#34; Mmm:SS\u0026#34; (where Mmm is the three letters of the month). See also lstart, start, start_time, and stime. bsdtime TIME accumulated cpu time, user + system. The display format is usually \u0026#34;MMM:SS\u0026#34;, but can be shifted to the right if the process used more than 999 minutes of cpu time. c C processor utilization. Currently, this is the integer value of the percent usage over the lifetime of the process. (see %cpu). caught CAUGHT mask of the caught signals, see signal(7). According to the width of the field, a 32 or 64 bits mask in hexadecimal format is displayed. (alias sig_catch, sigcatch). cgroup CGROUP display control groups to which the process belongs. class CLS scheduling class of the process. (alias policy, cls). Field\u0026#39;s possible values are: - not reported TS SCHED_OTHER FF SCHED_FIFO RR SCHED_RR B SCHED_BATCH ISO SCHED_ISO IDL SCHED_IDLE ? unknown value cls CLS scheduling class of the process. (alias policy, cls). Field\u0026#39;s possible values are: - not reported TS SCHED_OTHER FF SCHED_FIFO RR SCHED_RR B SCHED_BATCH ISO SCHED_ISO IDL SCHED_IDLE ? unknown value cmd CMD see args. (alias args, command). comm COMMAND command name (only the executable name). Modifications to the command name will not be shown. A process marked \u0026lt;defunct\u0026gt; is partly dead, waiting to be fully destroyed by its parent. The output in this column may contain spaces. (alias ucmd, ucomm). See also the args format keyword, the -f option, and the c option. When specified last, this column will extend to the edge of the display. If ps can not determine display width, as when output is redirected (piped) into a file or another command, the output width is undefined (it may be 80, unlimited, determined by the TERM variable, and so on). The COLUMNS environment variable or --cols option may be used to exactly determine the width in this case. The w or -w option may be also be used to adjust width. command COMMAND See args. (alias args, command). cp CP per-mill (tenths of a percent) CPU usage. (see %cpu). cputime TIME cumulative CPU time, \u0026#34;[DD-]hh:mm:ss\u0026#34; format. (alias time). drs DRS data resident set size, the amount of physical memory devoted to other than executable code. egid EGID effective group ID number of the process as a decimal integer. (alias gid). egroup EGROUP effective group ID of the process. This will be the textual group ID, if it can be obtained and the field width permits, or a decimal representation otherwise. (alias group). eip EIP instruction pointer. esp ESP stack pointer. etime ELAPSED elapsed time since the process was started, in the form [[DD-]hh:]mm:ss. etimes ELAPSED elapsed time since the process was started, in seconds. euid EUID effective user ID (alias uid). euser EUSER effective user name. This will be the textual user ID, if it can be obtained and the field width permits, or a decimal representation otherwise. The n option can be used to force the decimal representation. (alias uname, user). f F flags associated with the process, see the PROCESS FLAGS section. (alias flag, flags). fgid FGID filesystem access group ID. (alias fsgid). fgroup FGROUP filesystem access group ID. This will be the textual group ID, if it can be obtained and the field width permits, or a decimal representation otherwise. (alias fsgroup). flag F see f. (alias f, flags). flags F see f. (alias f, flag). fname COMMAND first 8 bytes of the base name of the process\u0026#39;s executable file. The output in this column may contain spaces. fuid FUID filesystem access user ID. (alias fsuid). fuser FUSER filesystem access user ID. This will be the textual user ID, if it can be obtained and the field width permits, or a decimal representation otherwise. gid GID see egid. (alias egid). group GROUP see egroup. (alias egroup). ignored IGNORED mask of the ignored signals, see signal(7). According to the width of the field, a 32 or 64 bits mask in hexadecimal format is displayed. (alias sig_ignore, sigignore). ipcns IPCNS Unique inode number describing the namespace the process belongs to. See namespaces(7). label LABEL security label, most commonly used for SELinux context data. This is for the Mandatory Access Control (\u0026#34;MAC\u0026#34;) found on high-security systems. lstart STARTED time the command started. See also bsdstart, start, start_time, and stime. lsession SESSION displays the login session identifier of a process, if systemd support has been included. lwp LWP light weight process (thread) ID of the dispatchable entity (alias spid, tid). See tid for additional information. machine MACHINE displays the machine name for processes assigned to VM or container, if systemd support has been included. maj_flt MAJFLT The number of major page faults that have occurred with this process. min_flt MINFLT The number of minor page faults that have occurred with this process. mntns MNTNS Unique inode number describing the namespace the process belongs to. See namespaces(7). netns NETNS Unique inode number describing the namespace the process belongs to. See namespaces(7). ni NI nice value. This ranges from 19 (nicest) to -20 (not nice to others), see nice(1). (alias nice). nice NI see ni.(alias ni). nlwp NLWP number of lwps (threads) in the process. (alias thcount). nwchan WCHAN address of the kernel function where the process is sleeping (use wchan if you want the kernel function name). Running tasks will display a dash (\u0026#39;-\u0026#39;) in this column. ouid OWNER displays the Unix user identifier of the owner of the session of a process, if systemd support has been included. pcpu %CPU see %cpu. (alias %cpu). pending PENDING mask of the pending signals. See signal(7). Signals pending on the process are distinct from signals pending on individual threads. Use the m option or the -m option to see both. According to the width of the field, a 32 or 64 bits mask in hexadecimal format is displayed. (alias sig). pgid PGID process group ID or, equivalently, the process ID of the process group leader. (alias pgrp). pgrp PGRP see pgid. (alias pgid). pid PID a number representing the process ID (alias tgid). pidns PIDNS Unique inode number describing the namespace the process belongs to. See namespaces(7). pmem %MEM see %mem. (alias %mem). policy POL scheduling class of the process. (alias class, cls). Possible values are: - not reported TS SCHED_OTHER FF SCHED_FIFO RR SCHED_RR B SCHED_BATCH ISO SCHED_ISO IDL SCHED_IDLE ? unknown value ppid PPID parent process ID. pri PRI priority of the process. Higher number means lower priority. psr PSR processor that process is currently assigned to. rgid RGID real group ID. rgroup RGROUP real group name. This will be the textual group ID, if it can be obtained and the field width permits, or a decimal representation otherwise. rss RSS resident set size, the non-swapped physical memory that a task has used (in kiloBytes). (alias rssize, rsz). rssize RSS see rss. (alias rss, rsz). rsz RSZ see rss. (alias rss, rssize). rtprio RTPRIO realtime priority. ruid RUID real user ID. ruser RUSER real user ID. This will be the textual user ID, if it can be obtained and the field width permits, or a decimal representation otherwise. s S minimal state display (one character). See section PROCESS STATE CODES for the different values. See also stat if you want additional information displayed. (alias state). sched SCH scheduling policy of the process. The policies SCHED_OTHER (SCHED_NORMAL), SCHED_FIFO, SCHED_RR, SCHED_BATCH, SCHED_ISO, and SCHED_IDLE are respectively displayed as 0, 1, 2, 3, 4, and 5. seat SEAT displays the identifier associated with all hardware devices assigned to a specific workplace, if systemd support has been included. sess SESS session ID or, equivalently, the process ID of the session leader. (alias session, sid). sgi_p P processor that the process is currently executing on. Displays \u0026#34;*\u0026#34; if the process is not currently running or runnable. sgid SGID saved group ID. (alias svgid). sgroup SGROUP saved group name. This will be the textual group ID, if it can be obtained and the field width permits, or a decimal representation otherwise. sid SID see sess. (alias sess, session). sig PENDING see pending. (alias pending, sig_pend). sigcatch CAUGHT see caught. (alias caught, sig_catch). sigignore IGNORED see ignored. (alias ignored, sig_ignore). sigmask BLOCKED see blocked. (alias blocked, sig_block). size SIZE approximate amount of swap space that would be required if the process were to dirty all writable pages and then be swapped out. This number is very rough! slice SLICE displays the slice unit which a process belongs to, if systemd support has been included. spid SPID see lwp. (alias lwp, tid). stackp STACKP address of the bottom (start) of stack for the process. start STARTED time the command started. If the process was started less than 24 hours ago, the output format is \u0026#34;HH:MM:SS\u0026#34;, else it is \u0026#34; Mmm dd\u0026#34; (where Mmm is a three-letter month name). See also lstart, bsdstart, start_time, and stime. start_time START starting time or date of the process. Only the year will be displayed if the process was not started the same year ps was invoked, or \u0026#34;MmmDD\u0026#34; if it was not started the same day, or \u0026#34;HH:MM\u0026#34; otherwise. See also bsdstart, start, lstart, and stime. stat STAT multi-character process state. See section PROCESS STATE CODES for the different values meaning. See also s and state if you just want the first character displayed. state S see s. (alias s). suid SUID saved user ID. (alias svuid). supgid SUPGID group ids of supplementary groups, if any. See getgroups(2). supgrp SUPGRP group names of supplementary groups, if any. See getgroups(2). suser SUSER saved user name. This will be the textual user ID, if it can be obtained and the field width permits, or a decimal representation otherwise. (alias svuser). svgid SVGID see sgid. (alias sgid). svuid SVUID see suid. (alias suid). sz SZ size in physical pages of the core image of the process. This includes text, data, and stack space. Device mappings are currently excluded; this is subject to change. See vsz and rss. tgid TGID a number representing the thread group to which a task belongs (alias pid). It is the process ID of the thread group leader. thcgr THCGR display control groups to which the thread belongs. thcount THCNT see nlwp. (alias nlwp). number of kernel threads owned by the process. tid TID the unique number representing a dispatchable entity (alias lwp, spid). This value may also appear as: a process ID (pid); a process group ID (pgrp); a session ID for the session leader (sid); a thread group ID for the thread group leader (tgid); and a tty process group ID for the process group leader (tpgid). time TIME cumulative CPU time, \u0026#34;[DD-]HH:MM:SS\u0026#34; format. (alias cputime). tname TTY controlling tty (terminal). (alias tt, tty). tpgid TPGID ID of the foreground process group on the tty (terminal) that the process is connected to, or -1 if the process is not connected to a tty. trs TRS text resident set size, the amount of physical memory devoted to executable code. tt TT controlling tty (terminal). (alias tname, tty). tty TT controlling tty (terminal). (alias tname, tt). ucmd CMD see comm. (alias comm, ucomm). ucomm COMMAND see comm. (alias comm, ucmd). uid UID see euid. (alias euid). uname USER see euser. (alias euser, user). unit UNIT displays unit which a process belongs to, if systemd support has been included. user USER see euser. (alias euser, uname). userns USERNS Unique inode number describing the namespace the process belongs to. See namespaces(7). utsns UTSNS Unique inode number describing the namespace the process belongs to. See namespaces(7). uunit UUNIT displays user unit which a process belongs to, if systemd support has been included. vsize VSZ see vsz. (alias vsz). vsz VSZ virtual memory size of the process in KiB (1024-byte units). Device mappings are currently excluded; this is subject to change. (alias vsize). wchan WCHAN name of the kernel function in which the process is sleeping, a \u0026#34;-\u0026#34; if the process is running, or a \u0026#34;*\u0026#34; if the process is multi-threaded and ps is not displaying threads.   ","permalink":"https://wenhan.blog/post/how-to-sort-ps-command-output/","summary":"ps command has a --sort option which can help you to sort processes.\n1 2 3 4 5 6 7  --sort spec Specify sorting order. Sorting syntax is [+|-]key[,[+|-]key[,...]]. Choose a multi-letter key from the STANDARD FORMAT SPECIFIERS section. The \u0026#34;+\u0026#34; is optional since default direction is increasing numerical or lexicographic order. Identical to k. For example: ps jax --sort=uid,-ppid, +pid   Sort ps output by memory From high to low The highest is at the top of the command","title":"How to sort ps command output"},{"content":"Introduction of systemd Before CentOS 7 and RHEL 7, System V was used to be the system controller. The system controller can manage all processes, services, and start task. System V has a performance problem as it is using script to manage the tasks. So it can only start the task serially, which will slow down the startup of the system.\nFrom CentOS 7, the systemd become the new system controller. The biggest change is it can start task parallel and will improve the startup speed. And as the its PID is 1, the systemd is controlling every process in the system!\nThis article will only shows the information of \u0026ldquo;service\u0026rdquo;, \u0026ldquo;start up task\u0026rdquo;, and \u0026ldquo;log manage\u0026rdquo; by systemd.\nCheck the Service status of the system Check all services in the system 1  systemctl list-unit-files --type=service   PageUp and PageDown to go up and down, type q to quit.\nCheck all running services in the system 1  systemctl list-units --type=service   If there is a big point ● in front of a service, it means this service has some problems.\nCheck all services which will start when the OS boot 1  systemctl list-unit-files --type=service | grep enabled   Check the detail information of a service 1  systemctl status \u0026lt;name of service\u0026gt;   The output will contain the status of activity, PID, path of the service, and the latest 10 logs.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15  systemctl status rsyslog ● rsyslog.service - System Logging Service Loaded: loaded (/usr/lib/systemd/system/rsyslog.service; enabled; vendor preset: enabled) Active: active (running) since Mon 2017-09-11 08:42:46 JST; 1h 57min ago Docs: man:rsyslogd(8) http://www.rsyslog.com/doc/ Main PID: 1417 (rsyslogd) Tasks: 3 (limit: 4915) CGroup: /system.slice/rsyslog.service └─1417 /usr/sbin/rsyslogd -n Sep 11 08:42:43 diablo systemd[1]: Starting System Logging Service... Sep 11 08:42:46 diablo rsyslogd[1417]: [origin software=\u0026#34;rsyslogd\u0026#34; swVersion=\u0026#34;8.27.0\u0026#34; x-pid=\u0026#34;1417\u0026#34; x-info=\u0026#34;http://www.rsyslog.com\u0026#34;] start Sep 11 08:42:46 diablo systemd[1]: Started System Logging Service. Sep 11 09:36:02 diablo rsyslogd[1417]: [origin software=\u0026#34;rsyslogd\u0026#34; swVersion=\u0026#34;8.27.0\u0026#34; x-pid=\u0026#34;1417\u0026#34; x-info=\u0026#34;http://www.rsyslog.com\u0026#34;] rsyslogd was HUPed   Check the services which got failed 1  systemctl list-units --type=service --state=failed   Check the time of system start up 1 2  systemd-analyze Startup finished in 1.858s (kernel) + 3.654s (initrd) + 29.878s (userspace) = 35.392s   Check the time of system start up by service 1  systemd-analyze blame | grep .service   Manage the service Start a service 1  systemctl start \u0026lt;service name\u0026gt;   Stop a service 1  systemctl stop \u0026lt;service name\u0026gt;   Restart a service 1  systemctl restart \u0026lt;service name\u0026gt;   Reload the configuration of a service 1  systemctl reload \u0026lt;service name\u0026gt;   If the service can\u0026rsquo;t be restart but need to change the settings, reload is a good choice.\nSet the service to start when OS boot 1  systemctl enable \u0026lt;service name\u0026gt;   Set the service NOT to start when OS boot 1  systemctl disable \u0026lt;service name\u0026gt;   mask a service Mask a service to make it can\u0026rsquo;t be started by other service nor start restart command.\n1  systemctl mask \u0026lt;service name\u0026gt;   unmask a service Unmask a service which has already been masked.\n1  systemctl unmask \u0026lt;service name\u0026gt;   reload all the service units Do this when you add/remove service units.\n1  systemctl daemon-reload   Create a simple service unit file Path of the unit file When can create a unit file an put it into /etc/systemd/system.\nAn example of unit file 1 2 3 4 5 6 7 8 9 10 11 12 13  [Unit] Description=\u0026lt;the description of the service\u0026gt; After=\u0026lt;start after which service(optional)\u0026gt; [Service] Type=forking ExecStart=\u0026lt;path to an executable command\u0026gt; ExecReload=\u0026lt;reload command of the configuration file(optional)\u0026gt; KillSignal=SIGTERM KillMode=mixed [Install] WantedBy=multi-user.target   Run daemon-reload after create a new service unit file.\nTarget and runlevel check the default runlevel 1  systemctl get-default   set the default runlevel 1  systemctl set-default \u0026lt;target name\u0026gt;   change the runlevel 1  systemctl isolate \u0026lt;target name\u0026gt;   Manage logs Check the logs from the latest boot 1  journalctl -e -f   Check the logs of a unit(service) 1  journalctl -e -f -u \u0026lt;unit name\u0026gt;   Check the logs between specify times 1  journalctl --since=\u0026#34;yyyy-MM-dd hh:mm:ss\u0026#34; --until=\u0026#34;yyyy-MM-dd hh:mm:ss\u0026#34;   Check the disk usage of the logs 1  journalctl --disk-usage   Change the max disk usage of the logs. Uncomment of SystemMaxUse=.. in /etc/systemd/journald.conf, and set value to this parameter.\n1  SystemMaxUse=50M   After this, restart journald service\n1  systemctl restart systemd-journald.service   refs  CentOS 7 Systemd 入门 ，作者: 余泽楠 https://zhuanlan.zhihu.com/p/29217941  ","permalink":"https://wenhan.blog/post/beginner-guide-of-systemd-on-centos7-rhel7/","summary":"Introduction of systemd Before CentOS 7 and RHEL 7, System V was used to be the system controller. The system controller can manage all processes, services, and start task. System V has a performance problem as it is using script to manage the tasks. So it can only start the task serially, which will slow down the startup of the system.\nFrom CentOS 7, the systemd become the new system controller.","title":"Beginner guide of systemd on CentOS7/RHEL7"},{"content":"To enable and start a cron job and a httpd server in a docker container, I tried systemctl command but get an error output like this\n1 2 3 4  [root@5f1f0a5cde43 app]# systemctl status crond Failed to connect to bus: No such file or directory [root@5f1f0a5cde43 app]# systemctl status httpd Failed to connect to bus: No such file or directory   Fix this issue by add --privileged parameter to docker run  command\n1 2 3 4 5 6 7  # docker images REPOSITORY TAG IMAGE ID CREATED SIZE freshcase v0.5 55acda97e409 4 minutes ago 707 MB # docker run --privileged -d freshcase # docker container list CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 81fbd5eb01cb freshcase:v0.5 \u0026#34;/usr/sbin/init\u0026#34; 3 minutes ago Up 3 minutes 80/tcp lucid_wing   Then you can login to the container and use systemctl command as well\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36  # docker exec -ti 81 bash [root@81fbd5eb01cb app]# systemctl status httpd ● httpd.service - The Apache HTTP Server Loaded: loaded (/usr/lib/systemd/system/httpd.service; enabled; vendor preset: disabled) Active: active (running) since Tue 2017-09-05 01:50:30 UTC; 4min 25s ago Docs: man:httpd.service(8) Main PID: 31 (httpd) Status: \u0026#34;Total requests: 0; Idle/Busy workers 100/0;Requests/sec: 0; Bytes served/sec: 0 B/sec\u0026#34; Tasks: 47 (limit: 4915) CGroup: /system.slice/docker-81fbd5eb01cb3f527f651b1765a6155570b7dd562b29d807b8cac12f9ecc004a.scope/system.slice/httpd.service ├─31 /usr/sbin/httpd -DFOREGROUND ├─36 /usr/sbin/httpd -DFOREGROUND ├─37 /usr/sbin/httpd -DFOREGROUND ├─38 /usr/sbin/httpd -DFOREGROUND ├─42 /usr/sbin/httpd -DFOREGROUND └─53 /usr/sbin/httpd -DFOREGROUND Sep 05 01:50:30 81fbd5eb01cb systemd[1]: Starting The Apache HTTP Server... Sep 05 01:50:30 81fbd5eb01cb httpd[31]: AH00558: httpd: Could not reliably determine the server\u0026#39;s fully qualified domain name, using 172.17.0.3. Set the \u0026#39;S erverName\u0026#39; directive globally to suppress this message Sep 05 01:50:30 81fbd5eb01cb systemd[1]: Started The Apache HTTP Server. [root@81fbd5eb01cb app]# [root@81fbd5eb01cb app]# systemctl status crond ● crond.service - Command Scheduler Loaded: loaded (/usr/lib/systemd/system/crond.service; enabled; vendor preset: enabled) Active: active (running) since Tue 2017-09-05 01:50:30 UTC; 4min 53s ago Main PID: 35 (crond) Tasks: 1 (limit: 4915) CGroup: /system.slice/docker-81fbd5eb01cb3f527f651b1765a6155570b7dd562b29d807b8cac12f9ecc004a.scope/system.slice/crond.service └─35 /usr/sbin/crond -n Sep 05 01:50:30 81fbd5eb01cb systemd[1]: Started Command Scheduler. Sep 05 01:50:30 81fbd5eb01cb crond[35]: (CRON) INFO (Syslog will be used instead of sendmail.) Sep 05 01:50:30 81fbd5eb01cb crond[35]: (CRON) INFO (RANDOM_DELAY will be scaled with factor 78% if used.) Sep 05 01:50:30 81fbd5eb01cb crond[35]: (CRON) INFO (running with inotify support) ...\u0026lt;snip\u0026gt;   ","permalink":"https://wenhan.blog/post/systemctl-command-return-failed-to-connect-to-bus-no-such-file-or-directory-in-a-docker-container/","summary":"To enable and start a cron job and a httpd server in a docker container, I tried systemctl command but get an error output like this\n1 2 3 4  [root@5f1f0a5cde43 app]# systemctl status crond Failed to connect to bus: No such file or directory [root@5f1f0a5cde43 app]# systemctl status httpd Failed to connect to bus: No such file or directory   Fix this issue by add --privileged parameter to docker run  command","title":"systemctl command return 'Failed to connect to bus: No such file or directory' in a docker container"},{"content":"check the current timezone status 1 2 3 4 5 6 7 8 9  [root@rhel7 ~]# timedatectl Local time: Thu 2017-08-10 05:19:53 UTC Universal time: Thu 2017-08-10 05:19:53 UTC RTC time: Thu 2017-08-10 05:19:52 Time zone: UTC (UTC, +0000) NTP enabled: yes NTP synchronized: yes RTC in local TZ: no DST active: n/a   list the avaliable timezones 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18  [root@rhel7 ~]# timedatectl list-timezones Asia/Aden Asia/Almaty Asia/Amman Asia/Anadyr Asia/Aqtau Asia/Aqtobe Asia/Ashgabat ... ... ... Pacific/Rarotonga Pacific/Saipan Pacific/Tahiti Pacific/Tarawa Pacific/Tongatapu Pacific/Wake Pacific/Wallis   find and set your timezone You can grep for your area.\n1 2  [root@rhel7 ~]# timedatectl list-timezones | grep Tokyo Asia/Tokyo   Now set to your timezone and check\n1 2 3 4 5 6 7 8 9 10 11 12  [root@rhel7 ~]# timedatectl set-timezone Asia/Tokyo [root@rhel7 ~]# timedatectl Local time: Thu 2017-08-10 14:22:37 JST Universal time: Thu 2017-08-10 05:22:37 UTC RTC time: Thu 2017-08-10 05:22:37 Time zone: Asia/Tokyo (JST, +0900) NTP enabled: yes NTP synchronized: yes RTC in local TZ: no DST active: n/a [root@rhel7 ~]# date Thu Aug 10 14:23:35 JST 2017   ","permalink":"https://wenhan.blog/post/how-to-change-timezone-in-centos-7-or-rhel-7/","summary":"check the current timezone status 1 2 3 4 5 6 7 8 9  [root@rhel7 ~]# timedatectl Local time: Thu 2017-08-10 05:19:53 UTC Universal time: Thu 2017-08-10 05:19:53 UTC RTC time: Thu 2017-08-10 05:19:52 Time zone: UTC (UTC, +0000) NTP enabled: yes NTP synchronized: yes RTC in local TZ: no DST active: n/a   list the avaliable timezones 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18  [root@rhel7 ~]# timedatectl list-timezones Asia/Aden Asia/Almaty Asia/Amman Asia/Anadyr Asia/Aqtau Asia/Aqtobe Asia/Ashgabat .","title":"How to change timezone in CentOS 7 or RHEL 7"},{"content":"This article show how to send desktop notice using Python\nInstall requirments we need to install notify2 by pip\n1 2 3 4 5  # pip install notify2 Collecting notify2 Downloading notify2-0.3.1-py2.py3-none-any.whl Installing collected packages: notify2 Successfully installed notify2-0.3.1   Coding First we need to import notify2\n1  import notify2   Then need to initialise the d-bus connection. D-Bus is a message bus system, a simple way for applications to talk to one another.\n1 2  # initialise the d-bus connection notify2.init(\u0026#34;hello\u0026#34;)   Next we need to create a Notification object. The simplest way is\n1  n = notify2.Notification(None)   else, you can add a icon to the notification.\n1  n = notify2.Notification(None, icon = \u0026#34;/home/wenshi/Pictures/me.jpg\u0026#34;)   next, set the urgency level for the notification.\n1  n.set_urgency(notify2.URGENCY_NORMAL)   the other available options are\n1 2 3  notify2.URGENCY_LOW notify2.URGENCY_NORMAL notify2.URGENCY_CRITICAL   next, you can decide how long will the notification will display. use set_timeout to set the time in milliseconds.\n1  n.set_timeout(5000)   next, file the title and message body of the notification\n1  n.update(\u0026#34;hello title\u0026#34;, \u0026#34;hello messages\u0026#34;)   notification will be shown on screen by show method.\n1  n.show()   Try it. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19  import notify2 # initialise the d-bus connection notify2.init(\u0026#34;hello\u0026#34;) # create Notification object n = notify2.Notification(None, icon = \u0026#34;/path/to/your/image\u0026#34;) # set urgency level n.set_urgency(notify2.URGENCY_NORMAL) # set timeout for a notification n.set_timeout(5000) # update notification data for Notification object n.update(\u0026#34;hello title\u0026#34;, \u0026#34;hello messages\u0026#34;) # show notification on screen n.show()   and it will show up in your screen ","permalink":"https://wenhan.blog/post/desktop-notifier-by-python/","summary":"This article show how to send desktop notice using Python\nInstall requirments we need to install notify2 by pip\n1 2 3 4 5  # pip install notify2 Collecting notify2 Downloading notify2-0.3.1-py2.py3-none-any.whl Installing collected packages: notify2 Successfully installed notify2-0.3.1   Coding First we need to import notify2\n1  import notify2   Then need to initialise the d-bus connection. D-Bus is a message bus system, a simple way for applications to talk to one another.","title":"Desktop Notifier by Python"},{"content":"Create a google yum repository Create a file /etc/yum.repos.d/google-chrome.repo and add the following into it.\n1 2 3 4 5 6  [google-chrome] name=google-chrome baseurl=http://dl.google.com/linux/chrome/rpm/stable/$basearch enabled=1 gpgcheck=1 gpgkey=https://dl-ssl.google.com/linux/linux_signing_key.pub   Check the google repository Run following command to check the repository is available.\n1  # yum info google-chrome-stable   From the output you should find the lastest version of the package google-chrome-stable\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19  # yum info google-chrome-stable Fedora 26 - x86_64 - Updates 6.3 MB/s | 4.1 MB 00:00 Fedora 26 - x86_64 7.5 MB/s | 53 MB 00:07 google-chrome 73 kB/s | 3.7 kB 00:00 Last metadata expiration check: 0:00:00 ago on Fri 14 Jul 2017 09:25:33 AM JST. Available Packages Name : google-chrome-stable Version : 59.0.3071.115 Release : 1 Arch : x86_64 Size : 58 M Source : google-chrome-stable-59.0.3071.115-1.src.rpm Repo : google-chrome Summary : Google Chrome URL : https://chrome.google.com/ License : Multiple, see https://chrome.google.com/ Description : The web browser from Google : : Google Chrome is a browser that combines a minimal design with sophisticated technology to make the web faster, safer, and easier.   Install Chrome via yum Let\u0026rsquo;s install chrome browser using yum command as below, which will automatically install all needed dependencies.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27  # yum install google-chrome-stable google-chrome 78 kB/s | 3.7 kB 00:00 Last metadata expiration check: 0:00:00 ago on Fri 14 Jul 2017 09:25:41 AM JST. Dependencies resolved. ============================================================================================================================================================================================== Package Arch Version Repository Size ============================================================================================================================================================================================== Installing: google-chrome-stable x86_64 59.0.3071.115-1 google-chrome 58 M Installing dependencies: at x86_64 3.1.20-3.fc26 fedora 77 k ed x86_64 1.14.1-2.fc26 fedora 79 k esmtp x86_64 1.2-6.fc26 fedora 56 k \u0026lt;snip...\u0026gt; spax x86_64 1.5.3-8.fc26 fedora 211 k systemtap-sdt-devel x86_64 3.1-5.fc26 fedora 71 k util-linux-user x86_64 2.30-1.fc26 updates 90 k Installing weak dependencies: perl-Module-Runtime noarch 0.014-9.fc26 fedora 24 k Transaction Summary ============================================================================================================================================================================================== Install 126 Packages Total download size: 71 M Installed size: 265 M Is this ok [y/N]: y   Start Chrome Start Chrome by below command, you can start it with a non-root user.\n1  $ google-chrome \u0026amp;   Enjory!\n","permalink":"https://wenhan.blog/post/install-chrome-browser-via-yum-in-centos-or-fedora-or-rhel/","summary":"Create a google yum repository Create a file /etc/yum.repos.d/google-chrome.repo and add the following into it.\n1 2 3 4 5 6  [google-chrome] name=google-chrome baseurl=http://dl.google.com/linux/chrome/rpm/stable/$basearch enabled=1 gpgcheck=1 gpgkey=https://dl-ssl.google.com/linux/linux_signing_key.pub   Check the google repository Run following command to check the repository is available.\n1  # yum info google-chrome-stable   From the output you should find the lastest version of the package google-chrome-stable\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19  # yum info google-chrome-stable Fedora 26 - x86_64 - Updates 6.","title":"Install Chrome Browser via yum in CentOS or Fedora or RHEL"},{"content":"As we Install the OpenShift by Ansible method, identity provider is set to Deny all by default, which will deny access from all users. To allow access for users, you must choose another identity provider and configure the master configuration file. By default, the master configuration file is located at /etc/origin/master/master-config.yaml. OpenShift has several identity providers which can help you to manager user authentication. I will use HTPasswd this time. You can find more information in Configuring Authentication and User Agent\nInstall package The htpasswd utility is provided in httpd-tools package.\n1  # yum install httpd-tools   configure the master configuration file 1 2 3 4 5 6 7 8 9 10 11  oauthConfig: ... identityProviders: - name: my_htpasswd_provider challenge: true login: true mappingMethod: claim provider: apiVersion: v1 kind: HTPasswdPasswordIdentityProvider file: /path/to/users.htpasswd   Need to restart atomic-openshift-master to make the change to take effect.\n1  # systemctl restart atomic-openshift-master   Setup username and password HTPasswd use a flat file to manager the username and the password, in which the password is hashed. Run following command to create the file with username, and it will ask you to input the password for username.\n1 2 3 4  $ htpasswd -c \u0026lt;/path/to/users.htpasswd\u0026gt; user1 New password: Re-type new password: Adding password for user user1   You can also include the password by adding the -b option\n1  $ htpasswd -c -b user1 \u0026lt;password\u0026gt;   After add a user, you can use it to access OpenShift Container Platform.\nadd/update an user To add or update an username, run following command\n1  $ htpasswd \u0026lt;/path/to/users.htpasswd\u0026gt; \u0026lt;user_name\u0026gt;   delete an user To delete an username, run following command\n1  $ htpasswd -D \u0026lt;/path/to/users.htpasswd\u0026gt; \u0026lt;user_name\u0026gt;   ","permalink":"https://wenhan.blog/post/openshift-configure-authetication-and-user-agent-using-htpasswd/","summary":"As we Install the OpenShift by Ansible method, identity provider is set to Deny all by default, which will deny access from all users. To allow access for users, you must choose another identity provider and configure the master configuration file. By default, the master configuration file is located at /etc/origin/master/master-config.yaml. OpenShift has several identity providers which can help you to manager user authentication. I will use HTPasswd this time. You can find more information in Configuring Authentication and User Agent","title":"OpenShift Configure authentication and User Agent using HTPasswd"},{"content":"In this article we will show you how to install OpenShift in mutliple nodes using a quick install command, atomic-openshift-installer, which is powered by ansible.\nHost preparation I use under virtual machines for OpenShift nodes to deploy\n   Type CPU Mem HDD hostname OS     Master 1 2 GB 20 GB master.example.com RHEL 7   node 1 2 GB 20 GB node1.example.com RHEL 7   node 1 2 GB 20 GB node2.example.com RHEL 7    Host Registration Note: If you are using other OS but not RHEL, Please go to [# Install necessary packages](# Install necessary packages) to install the packages. If you can not install some of them, try to add some repos or get the rpm file.\nRegister each host with RHSM(Red Hat Subscription Manager) to access the required packages.\n  Register with RHSM for each host:\n1  # subscription-manager register --username=\u0026lt;user_name\u0026gt; --password=\u0026lt;password\u0026gt;     List the available OpenShift subscriptions:\n1  # subscription-manager list --available --matches \u0026#39;*OpenShift*\u0026#39;     Find pool ID for an OpenShift Container Platform subscription and attach it.\n1  # subscription-manager attach --pool=\u0026lt;pool_id\u0026gt;     Disable all repositories and enable only the repositories required by OpenShift Container Platform 3.5\n1 2 3 4 5 6 7  # subscription-manager repos --disable=\u0026#34;*\u0026#34; # yum-config-manager --disable \\* # subscription-manager repos \\ --enable=\u0026#34;rhel-7-server-rpms\u0026#34; \\ --enable=\u0026#34;rhel-7-server-extras-rpms\u0026#34; \\ --enable=\u0026#34;rhel-7-server-ose-3.5-rpms\u0026#34; \\ --enable=\u0026#34;rhel-7-fast-datapath-rpms\u0026#34;     Install necessary packages Install the following packages.\n1 2 3 4  # yum -y install wget git net-tools bind-utils iptables-services bridge-utils bash-completion kexec sos psacct # yum update # yum -y install atomic-openshift-utils atomic-openshift-excluder atomic-openshift-docker-excluder # atomic-openshift-excluder unexclude   Install and configure docker Install docker 1  # yum -y install docker   Add parameter to docker configuration file Edit /etc/sysconfig/docker file and add --insecure-registry 172.30.0.0/16 to the OPTIONS parameter.\n1  OPTIONS=\u0026#39;--selinux-enabled --insecure-registry 172.30.0.0/16\u0026#39;   Configure Docker Storage Here we use an additional block device for docker storage. In /etc/sysconfig/docker-storage-setup , set DEVS to the path of the disk device. Set VG to the volume group name you wish to create.\n1 2 3 4  # cat \u0026lt;\u0026lt;EOF \u0026gt; /etc/sysconfig/docker-storage-setup DEVS=/dev/vdc VG=docker-vg EOF   Then run docker-storage-setup and check to make sure the docker-vg was created.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39  # docker-storage-setup [5/1868] 0 Checking that no-one is using this disk right now ... OK Disk /dev/vdc: 31207 cylinders, 16 heads, 63 sectors/track sfdisk: /dev/vdc: unrecognized partition table type Old situation: sfdisk: No partitions found New situation: Units: sectors of 512 bytes, counting from 0 Device Boot Start End #sectors Id System /dev/vdc1 2048 31457279 31455232 8e Linux LVM /dev/vdc2 0 - 0 0 Empty /dev/vdc3 0 - 0 0 Empty /dev/vdc4 0 - 0 0 Empty Warning: partition 1 does not start at a cylinder boundary Warning: partition 1 does not end at a cylinder boundary Warning: no primary partition is marked bootable (active) This does not matter for LILO, but the DOS MBR will not boot this disk. Successfully wrote the new partition table Re-reading the partition table ... If you created or changed a DOS partition, /dev/foo7, say, then use dd(1) to zero the first 512 bytes: dd if=/dev/zero of=/dev/foo7 bs=512 count=1 (See fdisk(8).) Physical volume \u0026#34;/dev/vdc1\u0026#34; successfully created Volume group \u0026#34;docker-vg\u0026#34; successfully created Rounding up size to full physical extent 16.00 MiB Logical volume \u0026#34;docker-poolmeta\u0026#34; created. Logical volume \u0026#34;docker-pool\u0026#34; created. WARNING: Converting logical volume docker-vg/docker-pool and docker-vg/docker-poolmeta to pool\u0026#39;s data and metadata volumes. THIS WILL DESTROY CONTENT OF LOGICAL VOLUME (filesystem etc.) Converted docker-vg/docker-pool to thin pool. Logical volume \u0026#34;docker-pool\u0026#34; changed.   Enable and start docker service. 1 2 3  # systemctl enable docker # systemctl start docker # systemctl is-active docker   Ensure Host Access On each hosts, generate an SSH key WITHOUT a password\n1  # ssh-keygen   Copy the id_rsa.pub to each host:\n1 2 3 4 5  # for host in master.example.com \\ node1.example.com \\ node2.example.com; \\ do ssh-copy-id -i ~/.ssh/id_rsa.pub $host; \\ done   Quick Installation Running an Interactive Installation Start the interactive installation by running under command, and follow the on-screen instructions to install a new OpenShift Continer Platform cluster.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97  $ atomic-openshift-installer install *** Installation Summary *** Hosts: - master.example.com - OpenShift master - OpenShift node (Unscheduled) - Etcd - Storage - node1.example.com - OpenShift node (Dedicated) - node2.example.com - OpenShift node (Dedicated) Total OpenShift masters: 1 Total OpenShift nodes: 3 NOTE: Add a total of 3 or more masters to perform an HA installation. Gathering information from hosts... All hosts in config are uninstalled. Proceeding with installation... Wrote atomic-openshift-installer config: /root/.config/openshift/installer.cfg.yml Wrote Ansible inventory: /root/.config/openshift/hosts Ready to run installation process. Play 1/28 (Create initial host groups for localhost) .. Play 2/28 (Create initial host groups for all hosts) . Play 3/28 (Populate config host groups) ................ Play 4/28 (Ensure that all non-node hosts are accessible) . Play 5/28 (Initialize host facts) ................ Play 6/28 (Gather and set facts for node hosts) ............... Play 7/28 (Verify compatible yum/subscription-manager combination) .. Play 8/28 (Determine openshift_version to configure on first master) ............................................................................................ Play 9/28 (Set openshift_version for all hosts) ............................................................................................ Play 10/28 (Set oo_option facts) ........ Play 11/28 (Disable excluders) .......................... Play 12/28 (Configure etcd) ................................................................................................................................................Pausing for 10 seconds (ctrl+C then \u0026#39;C\u0026#39; = continue early, ctrl+C then \u0026#39;A\u0026#39; = abort) .................................................................... Play 13/28 (Configure nfs) ............................................... Play 14/28 (Gather and set facts for master hosts) ....................... Play 15/28 (Determine if session secrets must be generated) .............. Play 16/28 (Generate master session secrets) .............. Play 17/28 (Configure masters) ............................................................................................................................................................................................................................................................................................................................................................................................................................................. Play 18/28 (Additional master configuration) ....................................................................................................................................................................................................................... Play 19/28 (Gather and set facts for node hosts) ............... Play 20/28 (Evaluate node groups) .. Play 21/28 (Configure nodes) ............................................................................................................................................................................................................................................................................................................................................. Play 22/28 (Additional node config) ..................................................................................................................... Play 23/28 (Create persistent volumes) ............................................................................................................................................................ Play 24/28 (Create Hosted Resources) .......................................................................................................................................................................................................Pausing for 30 seconds (ctrl+C then \u0026#39;C\u0026#39; = continue early, ctrl+C then \u0026#39;A\u0026#39; = abort) ................................................ Play 25/28 (Re-enable excluder if it was previously enabled) ............... localhost : ok=11 changed=0 unreachable=0 failed=0 master.example.com : ok=606 changed=151 unreachable=0 failed=0 node1.example.com : ok=202 changed=39 unreachable=0 failed=0 node2.example.com : ok=202 changed=39 unreachable=0 failed=0 Installation Complete: Note: Play count is only an estimate, some plays may have been skipped or dynamically added The installation was successful! If this is your first time installing please take a look at the Administrator Guide for advanced options related to routing, storage, authentication, and more: http://docs.openshift.com/enterprise/latest/admin_guide/overview.html   Running an unattended Installation Unattended installation allow you to run the installation with a pre-defined configuration file. The default installation configure file path is ~/.config/openshift/installer.cfg.yml. Define the configuration file and run the install command with the -u option.\n1  $ atomic-openshift-installer -u install   Here is a simple example of the install.cfg.yml file. For further information, please follow Defining an Installation Configuration File\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46  ansible_callback_facts_yaml:/root/.config/openshift/.ansible/callback_facts.yamlansible_inventory_path:/root/.config/openshift/hostsansible_log_path:/tmp/ansible.logdeployment:ansible_ssh_user:roothosts:- connect_to:master.example.comhostname:masterip:10.64.221.200public_hostname:masterpublic_ip:10.64.221.200roles:- master- etcd- node- storage- connect_to:node1.example.comhostname:node1ip:10.64.221.47node_labels:\u0026#39;{\u0026#39;\u0026#39;region\u0026#39;\u0026#39;:\u0026#39;\u0026#39;infra\u0026#39;\u0026#39;}\u0026#39;public_hostname:node1public_ip:10.64.221.47roles:- node- connect_to:node2.example.comhostname:node2ip:10.64.221.192node_labels:\u0026#39;{\u0026#39;\u0026#39;region\u0026#39;\u0026#39;:\u0026#39;\u0026#39;infra\u0026#39;\u0026#39;}\u0026#39;public_hostname:node2public_ip:10.64.221.192roles:- nodemaster_routingconfig_subdomain:\u0026#39;\u0026#39;openshift_master_cluster_hostname:Noneopenshift_master_cluster_public_hostname:Noneproxy_exclude_hosts:\u0026#39;\u0026#39;proxy_http:\u0026#39;\u0026#39;proxy_https:\u0026#39;\u0026#39;roles:etcd:{}master:{}node:{}storage:{}variant:openshift-enterprisevariant_version:\u0026#39;3.5\u0026#39;version:v2  Also you can specify a different path of the configuration file with the -c option.\n1  $ atomic-openshift-installer -u -c \u0026lt;/path/to/file\u0026gt; install   Verifying the installation After the installation is completed.\n  Verify the master and nodes are started in Ready status. On the master host, run the following as root\n1 2 3 4 5 6  # oc get nodes NAME STATUS AGE master.example.com Ready,SchedulingDisabled 165d node1.example.com Ready 165d node2.example.com Ready 165d     The web console use the master host name with a default port number 8443. In this test environment, you can find the web console at https://master.openshift.com:8443/console\n  Now that the install has been verified, run the following command on each master and node host to add the atomic-openshift packages back to the list of yum excludes on the host:\n1  # atomic-openshift-excluder exclude     Uninstallation You can uninstall OpenShift Container Platform from all hosts using the follow commands\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26  $ atomic-openshift-installer uninstall OpenShift will be uninstalled from the following hosts: * master.example.com * node1.example.com * node2.example.com Do you want to proceed? [y/N]: y Play 1/9 (OSEv3:children) .... Play 2/9 (nodes) .. Play 3/9 (masters) .. Play 4/9 (etcd) .. Play 5/9 (nodes) ............................... Play 6/9 (masters) ............ Play 7/9 (etcd) ............ master.example.com : ok=60 changed=17 unreachable=0 failed=0 node1.example.com : ok=35 changed=8 unreachable=0 failed=0 node2.example.com : ok=35 changed=8 unreachable=0 failed=0   If you are using a configuration file, specify the file path for the uninstallation:\n1  $ atomic-openshift-installer -c \u0026lt;/path/to/file\u0026gt; uninstall   ","permalink":"https://wenhan.blog/post/openshift-use-ansible-to-install-openshift-to-multi-nodes/","summary":"In this article we will show you how to install OpenShift in mutliple nodes using a quick install command, atomic-openshift-installer, which is powered by ansible.\nHost preparation I use under virtual machines for OpenShift nodes to deploy\n   Type CPU Mem HDD hostname OS     Master 1 2 GB 20 GB master.example.com RHEL 7   node 1 2 GB 20 GB node1.example.com RHEL 7   node 1 2 GB 20 GB node2.","title":"[OpenShift]Quick install OpenShift to multi nodes"},{"content":" Make sure your VIM have +clipboard enabled, Add \u0026ldquo;set clipboard=unnamedplus\u0026rdquo; to .vimrc  check +clipboard is enabled 1 2 3  # vim --version | grep clipboard +clipboard +job +path_extra +user_commands +eval +mouse_dec +statusline +xterm_clipboard   If it shows \u0026lsquo;-clipboard\u0026rsquo;, you need to compile VIM with this feature.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21  $ git clone https://github.com/vim/vim.git $ cd vim/src/ $ ./configure --with-features=huge \\ --enable-multibyte \\ --enable-rubyinterp=yes \\ --enable-pythoninterp=yes \\ --with-python-config-dir=/usr/lib64/python2.7/config \\ --enable-perlinterp=yes \\ --enable-luainterp=yes \\ --prefix=/usr/local/ \\ --enable-fail-if-missing \\ --enable-gui=no \\ --enable-tclinterp=yes \\ --enable-cscope=yes \\ --enable-gpm \\ --enable-cscope \\ --enable-fontset \\ --with-x \\ --with-compiledby=koturn $ make -j5 # after this you can find a runable vim in ./ $ sudo make install # run this if you want to install vim to system   Set clipboard to unnamedplus Add \u0026ldquo;set clipboard=unnamedplus\u0026rdquo; to your .vimrc.\nAn awesome answers is here How can I copy text to the system clipboard from Vim?\n","permalink":"https://wenhan.blog/post/how-to-copy-paste-text-to-from-the-system-clipboard-in-vim/","summary":"Make sure your VIM have +clipboard enabled, Add \u0026ldquo;set clipboard=unnamedplus\u0026rdquo; to .vimrc  check +clipboard is enabled 1 2 3  # vim --version | grep clipboard +clipboard +job +path_extra +user_commands +eval +mouse_dec +statusline +xterm_clipboard   If it shows \u0026lsquo;-clipboard\u0026rsquo;, you need to compile VIM with this feature.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21  $ git clone https://github.","title":"How to copy text to the system clipboard in VIM"},{"content":"tmux is an opensource software that can help you to do this. There are a lot of other things tmux can do, but now let me explain how to keep process running even end the ssh session. Please follow the steps:\n start a tmux session by input tmux to the shell. start your process or script inside the tmux session. leave(detach) the tmux session by input Ctrl+b and then d.  Now you can log off or terminate the ssh session safely, your process/script is still running inside the tmux session. Once you want to check the status of your process/script, you can login ang input tmux attach to re-attach the session.\nCurrently running sessions can be listed by tmux list-sessions.\ntmux can do much more things, for more details about this command please have a look at man tmux.\n","permalink":"https://wenhan.blog/post/how-to-keep-process-script-running-after-end-the-ssh-session/","summary":"tmux is an opensource software that can help you to do this. There are a lot of other things tmux can do, but now let me explain how to keep process running even end the ssh session. Please follow the steps:\n start a tmux session by input tmux to the shell. start your process or script inside the tmux session. leave(detach) the tmux session by input Ctrl+b and then d.","title":"How to keep process running after log off"},{"content":"From version 7, VIM has a built in spell check function, but disable by default.\nEnable/Disable You can use :set spell  and  :set nospell to enable and disable it. The spell check isn\u0026rsquo;t only for English, use :echo \u0026amp;spelllang to confirm the current target langurage. Use:set spelllang=en_GB.UTF-8 to change the target langurage, also you can use set spelllang=en_us,nl,medical to set it to multiple langurage.\nSpell check Use ]s to move to the next, [s to move to the previous spell mistake.\nCorrect the mistake Use z= to list the suggestions of the spell mistake, and input a number to select.\nFor some special work, use zg to add it as user work. And use zw to delete it.\nsummary    command action     :set spell enable spell check   :set nospell disable spell check   ]s move to the next mistake   [s move to the previous mistake   z= list the suggestions   zg add the word   zw delete the word    ","permalink":"https://wenhan.blog/post/vim-use-built-in-spell-check/","summary":"From version 7, VIM has a built in spell check function, but disable by default.\nEnable/Disable You can use :set spell  and  :set nospell to enable and disable it. The spell check isn\u0026rsquo;t only for English, use :echo \u0026amp;spelllang to confirm the current target langurage. Use:set spelllang=en_GB.UTF-8 to change the target langurage, also you can use set spelllang=en_us,nl,medical to set it to multiple langurage.\nSpell check Use ]s to move to the next, [s to move to the previous spell mistake.","title":"[VIM] Use built in Spell Check"},{"content":"Install Linux OS and confirm network setting. You need to create a Linux OS machine to install OpenShift. The minimum requirement is\n   CPU Memory hard disk Network     x86_64 1 core 2 GB 20 GB IPv4    First you need to install CentOS 7.3, by select minimum setup. After you finish the installation, check your IP address to make sure you have available IP to use. The following example shows the IP address at eth0 is \u0026ldquo;192.168.122.45\u0026rdquo;\n1 2 3 4 5 6 7 8 9 10 11 12 13  [root@openshift ~]# ip a 1: lo: \u0026lt;LOOPBACK,UP,LOWER_UP\u0026gt; mtu 65536 qdisc noqueue state UNKNOWN qlen 1 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 inet 127.0.0.1/8 scope host lo valid_lft forever preferred_lft forever inet6 ::1/128 scope host valid_lft forever preferred_lft forever 2: eth0: \u0026lt;BROADCAST,MULTICAST,UP,LOWER_UP\u0026gt; mtu 1500 qdisc pfifo_fast state UP qlen 1000 link/ether 52:54:00:13:fc:b5 brd ff:ff:ff:ff:ff:ff inet 192.168.122.45/24 brd 192.168.122.255 scope global dynamic eth0 valid_lft 3251sec preferred_lft 3251sec inet6 fe80::a96e:889:48df:66a3/64 scope link valid_lft forever preferred_lft forever   OpenShift need a hostname to provide the service. Use \u0026ldquo;hostnamectl set-hostname\u0026rdquo; to set a hostname. And use \u0026ldquo;hostname\u0026rdquo; to confirm your setting. The following example show the hostname is \u0026ldquo;openshift.example.com\u0026rdquo;\n1 2 3  [root@openshift ~]# hostnamectl set-hostname openshift.example.com [root@openshift ~]# hostname openshift.example.com   Install docker Openshift use docker as a container engine. Install docker by following command.\n1  [root@openshift ~]# yum install -y docker   After the installation is done, start the docker service and enable it.\n1 2  [root@openshift ~]# systemctl start docker [root@openshift ~]# systemctl enable docker   Test under command to check if docker can fetch data from internet. The following example show how to create a \u0026ldquo;hello-openshift\u0026rdquo; container. This little app was a go program and will listen 8080 and 8888 port. (Run command for the first time will take some time because no image is existed locally, docker need to pull it from the internet.)\n1 2 3 4 5 6 7 8 9 10 11  [root@openshift ~]# docker run -it openshift/hello-openshift Unable to find image \u0026#39;openshift/hello-openshift:latest\u0026#39; locally Trying to pull repository registry.access.redhat.com/openshift/hello-openshift ... Trying to pull repository docker.io/openshift/hello-openshift ... latest: Pulling from docker.io/openshift/hello-openshift a3ed95caeb02: Pull complete 318ec9b73ccb: Pull complete Digest: sha256:3fc5706d2bc0b4d804fd9a9b9f2456c811b525a222caf906b008d0a3e5aba212 serving on 8888 serving on 8080   After the test, push \u0026ldquo;Ctrl+c\u0026rdquo; to stop this image.\nInstall OpenShift There are several methods to install OpenShift. For better understand we will download the binaries of OpenShift server and deploy it manually. You can find the download link at Download OpenShift Origin. For now the latest version is v1.5.1-7b451fc-linux-64bit.tar.gz.\nDownload the file and untar it to /opt/\n1 2 3 4 5 6 7  [root@openshift ~]# wget https://github.com/openshift/origin/releases/download/v1.5.1/openshift-origin-server-v1.5.1-7b451fc-linux-64bit.tar.gz \u0026lt;snip\u0026gt; [root@openshift ~]# cd /opt/ [root@openshift opt]# ls [root@openshift opt]# tar xf ~/openshift-origin-server-v1.5.1-7b451fc-linux-64bit.tar.gz \u0026lt;snip\u0026gt; [root@openshift opt]# ln -s openshift-origin-server-v1.5.1-7b451fc-linux-64bit/ openshift   Add Openshift path to $PATH by add following text to the end of /etc/profile.\n1  PATH=$PATH:/opt/openshift   Run source command to make the configuration enable,\n1  [root@openshift ~]# source /etc/profile   After this we can check if the shell can find the command of openshift. Try following command to check the version. From the output we can see the version of Openshift is 1.5.1, Kubernetes is 1.5.2, and etcd is 3.1.0.\n1 2 3 4  [root@openshift ~]# openshift version openshift v1.5.1+7b451fc kubernetes v1.5.2+43a9be4 etcd 3.1.0   Start OpenShift Just type \u0026ldquo;openshift start\u0026rdquo; to start openshift. There will be a lot of logs output in the terminal. OpenShift will started when the logs output is stopped.\n1 2  [root@openshift ~]# openshift start \u0026lt;snip\u0026gt;   Access OpenShift Open web control UI and login Type \u0026ldquo;https://:8443\u0026rdquo; to access the web control. Ignore the warning message of security and you will see the login page. Input username and password by \u0026ldquo;dev\u0026rdquo; to login.\nCreate a new project Click \u0026ldquo;New Project\u0026rdquo; For the first test project, input the Name, Display Name and Description and click \u0026ldquo;Create\u0026rdquo;. At the next screen, click \u0026ldquo;Deploy Image\u0026rdquo; and select \u0026ldquo;Image Name\u0026rdquo;. Input the image name as \u0026ldquo;openshift/hello-openshift\u0026rdquo; and click the \u0026ldquo;find\u0026rdquo; icon at right to find images. After a while(depends on your network speed), the image will be downloaded, click \u0026ldquo;Create\u0026rdquo; at the end of the page to create a pod. OpenShift will start to deploy the pod, at first you will see a gray circle with a number \u0026ldquo;1\u0026rdquo; in it. When the deployment is done, the circle will turn to blue, it mean the pod is ready to use. Click the circle to see the detail information of your pods. You can find an \u0026ldquo;IP\u0026rdquo; is attached to the pod. Following example shows the IP address of the pod is \u0026ldquo;172.17.0.3\u0026rdquo;. Back to the OpenShift server and run under command to check the hello-openshift container. The hello-openshift service will return a string \u0026ldquo;Hello OpenShift!\u0026rdquo; to each request from the 8080 or 8888 port.\n1 2  [root@openshift ~]# curl 172.17.0.3:8080 Hello OpenShift!   Above is a simple test of how to install Openshift and create a project/pod. The IP will not be accessible outside the OpenShift server because it\u0026rsquo;s only a local IP address. To deploy a real useful pod and provide it, you need to do more settings. I will update it in the further. #TODO\n","permalink":"https://wenhan.blog/post/openshift-install-openshift-and-create-first-project-hello-openshift/","summary":"Install Linux OS and confirm network setting. You need to create a Linux OS machine to install OpenShift. The minimum requirement is\n   CPU Memory hard disk Network     x86_64 1 core 2 GB 20 GB IPv4    First you need to install CentOS 7.3, by select minimum setup. After you finish the installation, check your IP address to make sure you have available IP to use.","title":"[OpenShift]Install OpenShift and Create first project Hello-OpenShift"},{"content":"How to clear cache in Linux  Clear PageCache only  1  # sync ; echo 1 \u0026gt; /proc/sys/vm/drop_caches    Clear Dentries and inodes(metadata)  1  # sync ; echo 2 \u0026gt; /proc/sys/vm/drop_caches    Clear All cache(include PageCache and Dentries and inode)  1  # sync ; echo 3 \u0026gt; /proc/sys/vm/drop_caches   How to clear swap space in Linux 1  # swapoff -a \u0026amp;\u0026amp; swapon -a   NOTE: This may make your system unstable when you have low RAM already. The data in the swapspace will force move to your RAM and may cause a OOM when you don\u0026rsquo;t have free space in RAM.\n","permalink":"https://wenhan.blog/post/how-to-clear-memory-cache-buffer-and-swap-on-linux/","summary":"How to clear cache in Linux  Clear PageCache only  1  # sync ; echo 1 \u0026gt; /proc/sys/vm/drop_caches    Clear Dentries and inodes(metadata)  1  # sync ; echo 2 \u0026gt; /proc/sys/vm/drop_caches    Clear All cache(include PageCache and Dentries and inode)  1  # sync ; echo 3 \u0026gt; /proc/sys/vm/drop_caches   How to clear swap space in Linux 1  # swapoff -a \u0026amp;\u0026amp; swapon -a   NOTE: This may make your system unstable when you have low RAM already.","title":"How to clear memory cache, buffer and swap on linux"},{"content":"If you want to save some time for setup each gluster system, you can just setup all the necessary configuration in one vm, and then cloning it. But you may in trouble for probe a peer from another node. When you try to run the command, you could get the following error messages:\n1 2  [root@node1 ~]# gluster peer probe node2 peer probe: failed: Peer uuid (host node2) is same as local uuid   this is because when glusterfs-server package is first installed, a node UUID file will be created at /var/lib/glusterd/glusterd.info. So after you cloned the vm that has installed glusterfs, the glusterd.info with same UUID will be stored in all vms.\nHere is the solution:\n stop glusterd servie on all nodes remove /var/lib/glusterd/glusterd.info start glusterd servie on all nodes run peer probe command again.  after step 3, a new glusterd.info file will be created with a new UUID, and the issue will be resolved for good.\n","permalink":"https://wenhan.blog/post/glusterfs-failed-to-probe-a-cloned-peer/","summary":"If you want to save some time for setup each gluster system, you can just setup all the necessary configuration in one vm, and then cloning it. But you may in trouble for probe a peer from another node. When you try to run the command, you could get the following error messages:\n1 2  [root@node1 ~]# gluster peer probe node2 peer probe: failed: Peer uuid (host node2) is same as local uuid   this is because when glusterfs-server package is first installed, a node UUID file will be created at /var/lib/glusterd/glusterd.","title":"GlusterFS Failed to probe a cloned peer"},{"content":"特殊权限对文件和文件夹的影响 Linux系统中存在三个比较特殊的权限，分别是setuid，setgid，和sticky。 下面的表显示了他们之间的区别和对文件及文件夹的影响。\n   权限种类 对文件的影响 对文件夹的影响。     u+s(suid) 文件在自己的owner权限下执行，而并不是在执行这个文件的用户下执行。好处就是不管谁执行这个文件，环境变数等常量都可以统一为onwer的 无影响   g+s(sgid) 跟setuid很像，文件执行时会在自己的group下执行。 文件夹内新文件的group都会跟上层文件夹的group一致   o+t(sticky) 无影响 对文件夹有w权限的用户，仅允许更改和删除属于自己的文件。也就是owner是自己的文件。对其他用户的文件无法更改或删除。    特殊权限的设定。 用记号设定: setuid = u+s; setgid = g+s; sticky = o+t 用数字设定: setuid = 4; setgid = 2; sticky = 1\n例\n1 2  # chmod g+s file # chmod 1755 file   ","permalink":"https://wenhan.blog/post/linux-special-permissions-for-file-and-dir-cn/","summary":"特殊权限对文件和文件夹的影响 Linux系统中存在三个比较特殊的权限，分别是setuid，setgid，和sticky。 下面的表显示了他们之间的区别和对文件及文件夹的影响。\n   权限种类 对文件的影响 对文件夹的影响。     u+s(suid) 文件在自己的owner权限下执行，而并不是在执行这个文件的用户下执行。好处就是不管谁执行这个文件，环境变数等常量都可以统一为onwer的 无影响   g+s(sgid) 跟setuid很像，文件执行时会在自己的group下执行。 文件夹内新文件的group都会跟上层文件夹的group一致   o+t(sticky) 无影响 对文件夹有w权限的用户，仅允许更改和删除属于自己的文件。也就是owner是自己的文件。对其他用户的文件无法更改或删除。    特殊权限的设定。 用记号设定: setuid = u+s; setgid = g+s; sticky = o+t 用数字设定: setuid = 4; setgid = 2; sticky = 1\n例\n1 2  # chmod g+s file # chmod 1755 file   ","title":"Linux文件的特殊权限设置 setuid setgit sticky"},{"content":"特殊パーミションの説明 ファイルとフォルダに対し、setuid、setgid、sticky bitの三つの特殊パーミッションが存在する。 setuidとsetgidパーミッションは、コマンドを実行したユーザまたはグループではなく、ファイルの所有者として実行したことを意味する。 sticky bitパーミッションは、ファイルの削除に関する特殊な制限があり、ファイルの所有者及びrootのみがディレクトリ内のファイルを削除できるようになっている。\n   特殊パーミッション ファイルへの影響 フォルダへの影響     u+s(suid) ファイルを実行したユーザではなく、ファイルの所有者としてファイルが実行される。 影響なし   g+s(sgid) ファイルが所属するグループとして実行される。 フォルダ内に新規作成したファイルの所有グループは、フォルダの所有グループと同じになる。   o+t(sticky) 影響なし フォルダに書き込みパーミションを持つユーザは、自分が所有しているファイルのみ削除できる。他のユーザが所有するファイルの削除または編集はできない。    特殊パーミションの設定 記号を使用: setuid = u+s; setgid = g+s; sticky = o+t 数字を使用: setuid = 4; setgid = 2; sticky = 1\n例\n1 2  # chmod g+s file // fileに対してsetgidビットを追加 # chmod 1755 file // fileに対してstickyビットを追加   ","permalink":"https://wenhan.blog/post/linux-special-permissions-for-file-and-dir-jp/","summary":"特殊パーミションの説明 ファイルとフォルダに対し、setuid、setgid、sticky bitの三つの特殊パーミッションが存在する。 setuidとsetgidパーミッションは、コマンドを実行したユーザまたはグループではなく、ファイルの所有者として実行したことを意味する。 sticky bitパーミッションは、ファイルの削除に関する特殊な制限があり、ファイルの所有者及びrootのみがディレクトリ内のファイルを削除できるようになっている。\n   特殊パーミッション ファイルへの影響 フォルダへの影響     u+s(suid) ファイルを実行したユーザではなく、ファイルの所有者としてファイルが実行される。 影響なし   g+s(sgid) ファイルが所属するグループとして実行される。 フォルダ内に新規作成したファイルの所有グループは、フォルダの所有グループと同じになる。   o+t(sticky) 影響なし フォルダに書き込みパーミションを持つユーザは、自分が所有しているファイルのみ削除できる。他のユーザが所有するファイルの削除または編集はできない。    特殊パーミションの設定 記号を使用: setuid = u+s; setgid = g+s; sticky = o+t 数字を使用: setuid = 4; setgid = 2; sticky = 1\n例\n1 2  # chmod g+s file // fileに対してsetgidビットを追加 # chmod 1755 file // fileに対してstickyビットを追加   ","title":"ファイルとディレクトリへの特殊パーミッション"},{"content":"Linuxのユーザパスワードの有効期限(aging)の管理について説明する。\n last chage (-d) : パスワードが最後に変更した日数 min days (-m) : パスワードが変更可能の最小日数 max days (-M) : パスワードの変更が必要となる最大日数 warn days (-W) : パスワードの有効期限が近づき、変更を促す（ワーニング）の日数 password expiration date : パスワード有効期限 inactive days (-l) : パスワードの有効期限が切れた後に、アカウントが保持される日数 inactive date : アカウントが非アクティブになる期限  上記情報の変更は、chage コマンドで実現できる。\n1  # chage -m 0 -M 60 -W 7 -l 30   また、chage コマンドに対し以下の使い方もある。\n 次回ログイン時に強制パスワードを変更させる  1  # chage -d 0 username    現在の設定一覧を表示  1  # chage -l username    指定した日付でアカウントが期限切れになる  1  # chage -E YYYY-MM-DD username   ユーザアカウントに関連する内容で他にも\n ロックとアンロック  1 2  # usermod -L username # usermod -U username   ","permalink":"https://wenhan.blog/post/linux-password-aging-jp/","summary":"Linuxのユーザパスワードの有効期限(aging)の管理について説明する。\n last chage (-d) : パスワードが最後に変更した日数 min days (-m) : パスワードが変更可能の最小日数 max days (-M) : パスワードの変更が必要となる最大日数 warn days (-W) : パスワードの有効期限が近づき、変更を促す（ワーニング）の日数 password expiration date : パスワード有効期限 inactive days (-l) : パスワードの有効期限が切れた後に、アカウントが保持される日数 inactive date : アカウントが非アクティブになる期限  上記情報の変更は、chage コマンドで実現できる。\n1  # chage -m 0 -M 60 -W 7 -l 30   また、chage コマンドに対し以下の使い方もある。\n 次回ログイン時に強制パスワードを変更させる  1  # chage -d 0 username    現在の設定一覧を表示  1  # chage -l username    指定した日付でアカウントが期限切れになる  1  # chage -E YYYY-MM-DD username   ユーザアカウントに関連する内容で他にも","title":"Linux ユーザパスワードの有効期限の管理"},{"content":"ソフトリンクとハードリンクの違いについてメモする 他にも思い出したらまた追記する。\n   項目 ソフトリンク ハードリンク     size 4 byte 表示上元ファイルと同じ   inode 元ファイルと異なるinodeを持つ 元ファイルと同じinodeを持つ   制限 - 元ファイルと同じファイルシステム上にある必要がある    以下でリンクのサイズとinode番号の違いがわかるはず。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23  wenhanMBP: /tmp/link → ls -li total 1024 12807105 -rw-r--r-- 1 shiwenhan wheel 524288 4 27 22:18 file wenhanMBP: /tmp/link → ln file hardlink-to-file wenhanMBP: /tmp/link → ls -li total 2048 12807105 -rw-r--r-- 2 shiwenhan wheel 524288 4 27 22:18 file 12807105 -rw-r--r-- 2 shiwenhan wheel 524288 4 27 22:18 hardlink-to-file wenhanMBP: /tmp/link → ln -s file softlink-to-file wenhanMBP: /tmp/link → ls -lih total 2056 12807105 -rw-r--r-- 2 shiwenhan wheel 512K 4 27 22:18 file 12807105 -rw-r--r-- 2 shiwenhan wheel 512K 4 27 22:18 hardlink-to-file 12807448 lrwxr-xr-x 1 shiwenhan wheel 4B 4 27 22:27 softlink-to-file -\u0026gt; file   ","permalink":"https://wenhan.blog/post/soft-link-and-hard-link-ja/","summary":"ソフトリンクとハードリンクの違いについてメモする 他にも思い出したらまた追記する。\n   項目 ソフトリンク ハードリンク     size 4 byte 表示上元ファイルと同じ   inode 元ファイルと異なるinodeを持つ 元ファイルと同じinodeを持つ   制限 - 元ファイルと同じファイルシステム上にある必要がある    以下でリンクのサイズとinode番号の違いがわかるはず。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23  wenhanMBP: /tmp/link → ls -li total 1024 12807105 -rw-r--r-- 1 shiwenhan wheel 524288 4 27 22:18 file wenhanMBP: /tmp/link → ln file hardlink-to-file wenhanMBP: /tmp/link → ls -li total 2048 12807105 -rw-r--r-- 2 shiwenhan wheel 524288 4 27 22:18 file 12807105 -rw-r--r-- 2 shiwenhan wheel 524288 4 27 22:18 hardlink-to-file wenhanMBP: /tmp/link → ln -s file softlink-to-file wenhanMBP: /tmp/link → ls -lih total 2056 12807105 -rw-r--r-- 2 shiwenhan wheel 512K 4 27 22:18 file 12807105 -rw-r--r-- 2 shiwenhan wheel 512K 4 27 22:18 hardlink-to-file 12807448 lrwxr-xr-x 1 shiwenhan wheel 4B 4 27 22:27 softlink-to-file -\u0026gt; file   ","title":"ソフトリンクとハードリンクの違い"},{"content":"There is a noisy error message appears each time when I run hexo command\n1  Error: Cannot find module \u0026#39;./build/Release/DTraceProviderBindings\u0026#39;   It is just a trace error and doesn\u0026rsquo;t stop my work, but , it was so NOISY and I realy want to remove it.\nFrom the follow links, https://github.com/hexojs/hexo/issues/1922 https://github.com/yarnpkg/yarn/issues/1915\nI know the root cause is because the dtrace-provider package. And I don\u0026rsquo;t use is at all, so I just want to uninstall it.\n1  # npm uninstall dtrace-provider -g   but as this package is related to hexo, it will not be removed\u0026hellip; you can still see it by follow commands\n1  # npm list | grep dtrace   OK, Let clean up the enviroment, by uninstall hexo-cli and dtrace-provider both.\n NOTE: you must use sudo to run the command  1 2  # sudo npm uninstall hexo-cli -g # sudo npm uninstall dtrace-provider -g   Next, Install the hexo-cli with \u0026ndash;no-optional option, and check dtrace-provider is not installed.\n NOTE: you must use sudo to run the command  1  # sudo npm install hexo-cli --no-optional -g   Finially, the world become quiet again. :)\n","permalink":"https://wenhan.blog/post/fix-hexo-error-output-build-release-dtraceproviderbindings/","summary":"There is a noisy error message appears each time when I run hexo command\n1  Error: Cannot find module \u0026#39;./build/Release/DTraceProviderBindings\u0026#39;   It is just a trace error and doesn\u0026rsquo;t stop my work, but , it was so NOISY and I realy want to remove it.\nFrom the follow links, https://github.com/hexojs/hexo/issues/1922 https://github.com/yarnpkg/yarn/issues/1915\nI know the root cause is because the dtrace-provider package. And I don\u0026rsquo;t use is at all, so I just want to uninstall it.","title":"fix hexo error output './build/Release/DTraceProviderBindings'"},{"content":"因为没看天气预报，被大雨浇了几次之后，我打算为我这样的懒人写一个小程序。 很简单，算是Python的一个小练习，用Python实现一个邮件提醒每日天气。\n这段程序一共分两步\n 通过网站获取天气信息 将天气信息通过邮件发送到指定信箱  获取天气信息 这里我用了常用的requests和BeautifulSoup4来获取网页并抽取信息。 网站用tenki.jp搜索天气，然后通过id来抽取搜索界面的天气信息框框。 搜索界面如下，可以看到用BS4抽取id为\u0026rsquo;map_world_point_wrap\u0026rsquo;的以下的HTML就好了。 我们准备直接发送HTML代码，这样看起来更好看一些。\n代码如下 注：soup.find之后需要将内容转换为string格式，不然python2环境下无法发送邮件\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29  #!/root/.pyenv/shims/python #-*- coding: UTF-8 -*- import sys import time import requests from bs4 import BeautifulSoup #Some User Agents hds=[{\u0026#39;User-Agent\u0026#39;:\u0026#39;Mozilla/5.0 (Windows; U; Windows NT 6.1; en-US; rv:1.9.1.6) Gecko/20091201 Firefox/3.5.6\u0026#39;}] def weather_notice(): url=\u0026#39;http://www.tenki.jp/world/5/90/54511.html\u0026#39; # beijing, China try: html = requests.get(url, headers=hds[0], allow_redirects=False, timeout=3) if html.status_code == 200: soup = BeautifulSoup(html.text.encode(html.encoding), \u0026#34;html.parser\u0026#34;) town_info_block = soup.find(\u0026#39;div\u0026#39;, {\u0026#39;id\u0026#39;: \u0026#39;map_world_point_wrap\u0026#39;}) town_info_block = str(town_info_block) print(town_info_block) except Exception as e: print(url, e, str(time.ctime())) if __name__ == \u0026#34;__main__\u0026#34;: weather_notice()   运行过后如果得到了一堆HTML内容，证明第一步获取天气情报已经完成。\n发送邮件设置 接下来我们为上面的代码加入gmail送信功能。 使用MIMEMultipart,MIMEText来设置邮件内容， 使用smtplib来发送邮件。在这里我们用gmail账户来发送邮件。\n下面是代码\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56  #!/root/.pyenv/shims/python #-*- coding: UTF-8 -*- import sys import time import requests from bs4 import BeautifulSoup import smtplib from email.mime.multipart import MIMEMultipart from email.mime.text import MIMEText #Some User Agents hds=[{\u0026#39;User-Agent\u0026#39;:\u0026#39;Mozilla/5.0 (Windows; U; Windows NT 6.1; en-US; rv:1.9.1.6) Gecko/20091201 Firefox/3.5.6\u0026#39;}] def send_email(weather_info_html): # setup to list tolist= [\u0026#39;TO ADDR 1\u0026#39;, \u0026#39;TO ADDR 2\u0026#39;] # login  fromaddr = \u0026#34;YOUR GOOGLE EMAIL ADDR\u0026#34; fromaddr_pw = \u0026#34;PASSWORD\u0026#34; server = smtplib.SMTP(\u0026#39;smtp.gmail.com\u0026#39;, 587) server.starttls() server.login(fromaddr, fromaddr_pw) # make up and send the msg msg = MIMEMultipart() msg[\u0026#39;Subject\u0026#39;] = \u0026#34;Weather Mail\u0026#34; + \u0026#34;[\u0026#34; + time.strftime(\u0026#34;%a, %d%b\u0026#34;, time.gmtime()) + \u0026#34;]\u0026#34; msg[\u0026#39;From\u0026#39;] = fromaddr msg[\u0026#39;To\u0026#39;] = \u0026#34;, \u0026#34;.join(tolist) msg.attach(MIMEText(weather_info_html, \u0026#39;html\u0026#39;)) # plain will send plain text server.sendmail(fromaddr, tolist, msg.as_string()) # logout server.quit() def weather_notice(): url=\u0026#39;http://www.tenki.jp/world/5/90/54511.html\u0026#39; # beijing, China try: html = requests.get(url, headers=hds[0], allow_redirects=False, timeout=3) if html.status_code == 200: soup = BeautifulSoup(html.text.encode(html.encoding), \u0026#34;html.parser\u0026#34;) town_info_block = soup.find(\u0026#39;div\u0026#39;, {\u0026#39;id\u0026#39;: \u0026#39;map_world_point_wrap\u0026#39;}) town_info_block = str(town_info_block) except Exception as e: print(url, e, str(time.ctime())) if __name__ == \u0026#34;__main__\u0026#34;: weather_notice()   执行这个程序，看看你能否收到邮件？\n最后就是将这个程序加入crontab定时启动了。 有关crontab -e 的用法我有时间再说，今天太晚了。。。\n","permalink":"https://wenhan.blog/post/get-weather-info-and-send-by-gmail-python/","summary":"因为没看天气预报，被大雨浇了几次之后，我打算为我这样的懒人写一个小程序。 很简单，算是Python的一个小练习，用Python实现一个邮件提醒每日天气。\n这段程序一共分两步\n 通过网站获取天气信息 将天气信息通过邮件发送到指定信箱  获取天气信息 这里我用了常用的requests和BeautifulSoup4来获取网页并抽取信息。 网站用tenki.jp搜索天气，然后通过id来抽取搜索界面的天气信息框框。 搜索界面如下，可以看到用BS4抽取id为\u0026rsquo;map_world_point_wrap\u0026rsquo;的以下的HTML就好了。 我们准备直接发送HTML代码，这样看起来更好看一些。\n代码如下 注：soup.find之后需要将内容转换为string格式，不然python2环境下无法发送邮件\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29  #!/root/.pyenv/shims/python #-*- coding: UTF-8 -*- import sys import time import requests from bs4 import BeautifulSoup #Some User Agents hds=[{\u0026#39;User-Agent\u0026#39;:\u0026#39;Mozilla/5.0 (Windows; U; Windows NT 6.1; en-US; rv:1.9.1.6) Gecko/20091201 Firefox/3.","title":"Python每日获取天气情报并通过邮件通知"},{"content":"Today I configure my development envirment for my mac pro. And I find it is much easier to install VIM 8 with python support use brew. (much more easier than cent 7)\nIf you have brew, you just need to run\n1  brew install --with-python vim   If you want both python3 and python2 support, just run this\n1  brew install --with-python --with-python3 vim   Update: you may need to use below command on some latest OS for python2\n1  brew install --with-python@2   may be you need to remove the old vim bin file,\n1  rm -f /usr/local/bin/vim   and re-link the vim\n1  brew link --overwrite vim   done! enjoy your vim~\n","permalink":"https://wenhan.blog/post/install-vim-8-with-python/","summary":"Today I configure my development envirment for my mac pro. And I find it is much easier to install VIM 8 with python support use brew. (much more easier than cent 7)\nIf you have brew, you just need to run\n1  brew install --with-python vim   If you want both python3 and python2 support, just run this\n1  brew install --with-python --with-python3 vim   Update: you may need to use below command on some latest OS for python2","title":"Install vim 8 with python support on mac"},{"content":"rootパスワードを知っているのに、rootでssh接続できない問題の解決方法です。\n実はsshdの設定ファイルに、rootでのログイン許可を設定するパラメータがあります。 Linux環境では、設定ファイル/etc/ssh/sshd_configに対し\n1  PermitRootLogin no   を\n1  PermitRootLogin yes   に変更し、sshdを再起動すれば、アクセス出来るようになる。\n1  # systemctl restart sshd   また、以下のように自動ログインをOFFにするパラメータも存在する\n1  PubkeyAuthentication no   特定のIPに対しての個別設定も可能\n1 2  Match Address 172.25.0.11 PubkeyAuthentication no   ","permalink":"https://wenhan.blog/post/cannot-ssh-to-server-by-root-user/","summary":"rootパスワードを知っているのに、rootでssh接続できない問題の解決方法です。\n実はsshdの設定ファイルに、rootでのログイン許可を設定するパラメータがあります。 Linux環境では、設定ファイル/etc/ssh/sshd_configに対し\n1  PermitRootLogin no   を\n1  PermitRootLogin yes   に変更し、sshdを再起動すれば、アクセス出来るようになる。\n1  # systemctl restart sshd   また、以下のように自動ログインをOFFにするパラメータも存在する\n1  PubkeyAuthentication no   特定のIPに対しての個別設定も可能\n1 2  Match Address 172.25.0.11 PubkeyAuthentication no   ","title":"cannot ssh to server as root"},{"content":"PythonのRequestsライブラリを利用してウェブページの内容を抽出し保存する。 そして、BeautifulSoupライブラリを利用し、欲しい情報を抽出する。\nRequestsとは RequestsはPythonのHTTPライブラリで、urllib2より断然使いやすい。 公式サイトに、\n Requests is an Apache2 Licensed HTTP library, written in Python, for human beings.\n の説明の通り、人間によって読みやすくコーディングできる。\nBeautifulsoupとは BeautifulSoupはPythonで動作するHTMLとXMLのパーサーです。\nインストール 1 2  pip install requests pip install BeautifulSoup   使い方 1 2  import requests from bs4 import BeautifulSoup   requestsライブラリでは、各種のHTTPメソッドに１対１のメソッドがあります。\n1 2 3 4 5  requests.get(\u0026#39;URL\u0026#39;) requests.post(\u0026#39;URL\u0026#39;) requests.put(\u0026#39;URL\u0026#39;) requests.delete(\u0026#39;URL\u0026#39;) requests.head(\u0026#39;URL\u0026#39;)   初めてのスパイダー アマゾンの本のランキングの内容を取ってみよう。以下のことを要件とする。\n ブラウザがアクセスしているように、ヘッダ情報を編集する。 ページの取得が成功の場合のみ、ページ内容をファイルに保存する。 売れ筋ランキングの1-20位の本の順位とタイトルを出力する。  1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41  import requests import time from bs4 import BeautifulSoup def anaylise_ranking_books(html): soup = BeautifulSoup(html.text.encode(html.encoding), \u0026#34;html.parser\u0026#34;) # ランキングの本の情報を抽出する, divタグでzg_itemrowのクラスで特定できる。 books = soup.findAll(\u0026#34;div\u0026#34;, {\u0026#34;class\u0026#34; : \u0026#34;zg_itemRow\u0026#34;}) # 各本の情報をさらに細かく抽出する。 for book in books: # 順位 rank_number = book.find(\u0026#34;span\u0026#34;, {\u0026#34;class\u0026#34; : \u0026#34;zg_rankNumber\u0026#34;}).text.strip() # タイトル title = book.find(\u0026#34;div\u0026#34;, {\u0026#34;class\u0026#34; : \u0026#34;a-fixed-left-grid-col a-col-right\u0026#34;}) title = title.find(\u0026#34;a\u0026#34;, {\u0026#34;class\u0026#34; : \u0026#34;a-link-normal\u0026#34;}).text.strip() print(rank_number, title) def get_url_info(url): headers = { \u0026#34;User-Agent\u0026#34;: \u0026#34;Mozilla/5.0 (Windows NT 6.2; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/49.0.2623.87 Safari/537.36\u0026#34; } try: html = requests.get(url, headers=headers, allow_redirects=False, timeout=3) if html.status_code == 200: print(url, \u0026#39;@ok200\u0026#39;, str(time.ctime())) with open(\u0026#39;content.html\u0026#39;, \u0026#39;wb\u0026#39;) as fw: fw.write(html.content) fw.close() anaylise_ranking_books(html) else: print(url, \u0026#39;wrong\u0026#39;, str(time.ctime())) except Exception as e: print(url, e, str(time.ctime())) if __name__ == \u0026#39;__main__\u0026#39;: get_url_info(\u0026#34;https://www.amazon.co.jp/gp/bestsellers/books\u0026#34;)   実行\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22  $ python 02_anaylize_data.py https://www.amazon.co.jp/gp/bestsellers/books @ok200 Sun Dec 4 23:23:34 2016 1. 全部レンチン! やせるおかず 作りおき: 時短、手間なし、失敗なし (小学館実用シリーズ LADY BIRD) 2. ポケットモンスター サン・ムーン 公式ガイドブック 上・下セット 完全ストーリー攻略+完全アローラ図鑑 3. やせるおかず 作りおき: 著者50代、1年で26キロ減、リバウンドなし! (小学館実用シリーズ LADY BIRD) 4. SDガンダム ジージェネレーション ジェネシス ファイナルコンプリートガイド 5. 蘇える変態 6. 【Amazon.co.jp限定】 ホグワーツMAP付き　ハリー・ポッターと呪いの子 第一部、第二部 特別リハーサル 7. 夫もやせるおかず 作りおき: お肉や麺もOKなガッツリ系 (LADY BIRD 小学館実用シリーズ) 8. 【限定】“識子流\u0026#34;ごりやく参拝マナー手帖セット(福を呼ぶおみくじ帖付) 9. アイドルマスター ミリオンライブ! 5 オリジナルCD\u0026amp;画集付特別版 (特品) 10. リスアニ! Vol.27.1 「ラブライブ! 」僕らの音楽大全 11. そして生活はつづく (文春文庫) 12. 小説 君の名は。 (角川文庫) 13. 生産性―――マッキンゼーが組織と人材に求め続けるもの 14. 【Amazon co.jp限定】憎らしい彼 美しい彼2 書き下ろしショートストーリー付き (キャラ文庫) 15. 星の王子さま (新潮文庫) 16. 働く男 (文春文庫) 17. コミックマーケット 91 カタログ 18. 嫌われる勇気―――自己啓発の源流「アドラー」の教え 19. ファイナルファンタジーXV アルティマニア -シナリオSIDE- (SE-MOOK) 20. ファイナルファンタジーXV アルティマニア -バトル+マップSIDE- (SE-MOOK)   そして、content.htmlファイルが生成され、ページのソースコードが保存された。\n","permalink":"https://wenhan.blog/post/crawling-webpage-use-python-requests/","summary":"PythonのRequestsライブラリを利用してウェブページの内容を抽出し保存する。 そして、BeautifulSoupライブラリを利用し、欲しい情報を抽出する。\nRequestsとは RequestsはPythonのHTTPライブラリで、urllib2より断然使いやすい。 公式サイトに、\n Requests is an Apache2 Licensed HTTP library, written in Python, for human beings.\n の説明の通り、人間によって読みやすくコーディングできる。\nBeautifulsoupとは BeautifulSoupはPythonで動作するHTMLとXMLのパーサーです。\nインストール 1 2  pip install requests pip install BeautifulSoup   使い方 1 2  import requests from bs4 import BeautifulSoup   requestsライブラリでは、各種のHTTPメソッドに１対１のメソッドがあります。\n1 2 3 4 5  requests.get(\u0026#39;URL\u0026#39;) requests.post(\u0026#39;URL\u0026#39;) requests.put(\u0026#39;URL\u0026#39;) requests.delete(\u0026#39;URL\u0026#39;) requests.head(\u0026#39;URL\u0026#39;)   初めてのスパイダー アマゾンの本のランキングの内容を取ってみよう。以下のことを要件とする。\n ブラウザがアクセスしているように、ヘッダ情報を編集する。 ページの取得が成功の場合のみ、ページ内容をファイルに保存する。 売れ筋ランキングの1-20位の本の順位とタイトルを出力する。  1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41  import requests import time from bs4 import BeautifulSoup def anaylise_ranking_books(html): soup = BeautifulSoup(html.","title":"Pythonでスパイダーを作る"},{"content":"Vimのプラグインを管理する人気のツール、Vundleの利用メモ\n本家のサイト：[https://github.com/VundleVim/Vundle.vim]\nVundleを利用するメリット\n プラグインを.vimrcでインストール/更新/削除できる 名前だけ書けば自動で探してくれる  インストール 以下のコマンドで、ファイルをコピーするだけでインストール完了\n1  $ git clone https://github.com/VundleVim/Vundle.vim.git ~/.vim/bundle/Vundle.vim   設定 以下の設定内容を.vimrcのトップに記入する。 説明のためいくつかの行がイメージになっている。 実際利用するときにコメントアウトする必要がある。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43  set nocompatible \u0026#34; be iMproved, requiredfiletype off \u0026#34; required\u0026#34; set the runtime path to include Vundle and initializeset rtp+=~/.vim/bundle/Vundle.vimcall vundle#begin()\u0026#34; alternatively, pass a path where Vundle should install plugins\u0026#34;call vundle#begin(\u0026#39;~/some/path/here\u0026#39;)\u0026#34; let Vundle manage Vundle, requiredPlugin \u0026#39;VundleVim/Vundle.vim\u0026#39;\u0026#34; The following are examples of different formats supported.\u0026#34; Keep Plugin commands between vundle#begin/end.\u0026#34; plugin on GitHub repo\u0026#34;Plugin \u0026#39;tpope/vim-fugitive\u0026#39; \u0026lt;- ここをコメントアウト\u0026#34; plugin from http://vim-scripts.org/vim/scripts.html\u0026#34;Plugin \u0026#39;L9\u0026#39; \u0026lt;- ここをコメントアウト\u0026#34; Git plugin not hosted on GitHub\u0026#34;Plugin \u0026#39;git://git.wincent.com/command-t.git\u0026#39; \u0026lt;- ここをコメントアウト\u0026#34; git repos on your local machine (i.e. when working on your own plugin)\u0026#34;Plugin \u0026#39;file:///home/gmarik/path/to/plugin\u0026#39; \u0026lt;- ここをコメントアウト\u0026#34; The sparkup vim script is in a subdirectory of this repo called vim.\u0026#34; Pass the path to set the runtimepath properly.\u0026#34;Plugin \u0026#39;rstacruz/sparkup\u0026#39;, {\u0026#39;rtp\u0026#39;: \u0026#39;vim/\u0026#39;} \u0026lt;- ここをコメントアウト\u0026#34; Install L9 and avoid a Naming conflict if you\u0026#39;ve already installed a\u0026#34; different version somewhere else.\u0026#34;Plugin \u0026#39;ascenator/L9\u0026#39;, {\u0026#39;name\u0026#39;: \u0026#39;newL9\u0026#39;} \u0026lt;- ここをコメントアウト\u0026#34; All of your Plugins must be added before the following linecall vundle#end() \u0026#34; requiredfiletype plugin indent on \u0026#34; required\u0026#34; To ignore plugin indent changes, instead use:\u0026#34;filetype plugin on\u0026#34;\u0026#34; Brief help\u0026#34; :PluginList - lists configured plugins\u0026#34; :PluginInstall - installs plugins; append `!` to update or just :PluginUpdate\u0026#34; :PluginSearch foo - searches for foo; append `!` to refresh local cache\u0026#34; :PluginClean - confirms removal of unused plugins; append `!` to auto-approve removal\u0026#34;\u0026#34; see :h vundle for more details or wiki for FAQ\u0026#34; Put your non-Plugin stuff after this line  Vimのプラグインをインストールする 例として、ファイルエクスプローラーの機能を提供するプラグイン「NERDTree」を入れてみよう call vundle#end()の前に、NERDTreeのgitのリンクを記入\n1 2 3 4 5 6  ...省略Plugin \u0026#39;git@github.com:scrooloose/nerdtree.git\u0026#39;\u0026#34; All of your Plugins must be added before the following linecall vundle#end() \u0026#34; required...省略  Vimを起動し、:PluginInstallを実行 コマンドラインからでは、vim +PluginInstall +qallを実行\nそして画面に以下のログが出るはず\n1 2 3 4  \u0026#34; Installing plugins to /Users/shiwenhan/.vim/bundle . Plugin \u0026#39;VundleVim/Vundle.vim\u0026#39; . Plugin \u0026#39;git@github.com:scrooloose/nerdtree.git\u0026#39; * Helptags   lを押してログを確認\n1 2 3 4 5 6 7 8 9 10  [2016-11-21 00:56:51] [2016-11-21 00:56:51] Plugin git@github.com:scrooloose/nerdtree.git [2016-11-21 00:56:51] $ git clone --recursive \u0026#39;git@github.com:scrooloose/nerdtree.git\u0026#39; \u0026#39;/Users/shiwenhan/.vim/bundle/nerdtree\u0026#39; [2016-11-21 00:56:51] \u0026gt; Cloning into \u0026#39;/Users/shiwenhan/.vim/bundle/nerdtree\u0026#39;... [2016-11-21 00:56:51] \u0026gt; [2016-11-21 00:56:51] [2016-11-21 00:56:51] Helptags: [2016-11-21 00:56:51] :helptags /Users/shiwenhan/.vim/bundle/Vundle.vim/doc [2016-11-21 00:56:51] :helptags /Users/shiwenhan/.vim/bundle/nerdtree/doc [2016-11-21 00:56:51] Helptags: 2 plugins processed   これでプラグインのインストールが完了した。 Vimを再起動して、:NERDTreeToggleを実行してプラグインの確認をしましょう\nこれで確認が完成！\n","permalink":"https://wenhan.blog/post/manage-vim-plugins-with-vundle/","summary":"Vimのプラグインを管理する人気のツール、Vundleの利用メモ\n本家のサイト：[https://github.com/VundleVim/Vundle.vim]\nVundleを利用するメリット\n プラグインを.vimrcでインストール/更新/削除できる 名前だけ書けば自動で探してくれる  インストール 以下のコマンドで、ファイルをコピーするだけでインストール完了\n1  $ git clone https://github.com/VundleVim/Vundle.vim.git ~/.vim/bundle/Vundle.vim   設定 以下の設定内容を.vimrcのトップに記入する。 説明のためいくつかの行がイメージになっている。 実際利用するときにコメントアウトする必要がある。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43  set nocompatible \u0026#34; be iMproved, requiredfiletype off \u0026#34; required\u0026#34; set the runtime path to include Vundle and initializeset rtp+=~/.","title":"VundleでVimのプラグインを簡単管理"},{"content":"PythonでGoogle mapsのAPI を利用しルート検索をやってみた。\n利用するモジュールはgooglemaps/google-maps-services-python\n環境構築は以下のコマンドでOK, ついでにipythonもインストールする。\n1 2  $ pip install googlemaps $ pip install ipython   あと、GoogleのAPIキーの申請が必要なので、Google APIsで申請＆有効にする。\nここまで問題なかったら、早速使ってみよう。 戸塚駅から踊場駅まで、車のルート情報を取得\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79  $ ipython Python 3.5.1 (default, Sep 1 2016, 00:20:33) Type \u0026#34;copyright\u0026#34;, \u0026#34;credits\u0026#34; or \u0026#34;license\u0026#34; for more information. IPython 5.1.0 -- An enhanced Interactive Python. ? -\u0026gt; Introduction and overview of IPython\u0026#39;s features. %quickref -\u0026gt; Quick reference. help -\u0026gt; Python\u0026#39;s own help system. object? -\u0026gt; Details about \u0026#39;object\u0026#39;, use \u0026#39;object??\u0026#39; for extra details. In [1]: import googlemaps In [2]: gmaps = googlemaps.Client(key=\u0026#39;AIzaSyCuxTpu4wHcCz1M9S3GNLMfbCYmrc-b-dg\u0026#39;) In [3]: directions_result = gmaps.directions(\u0026#39;戸塚駅\u0026#39;,\u0026#39;踊場駅\u0026#39;,mode=\u0026#34;driving\u0026#34;,alternatives=False,language=\u0026#34;ja\u0026#34;) In [4]: directions_result Out[4]: [{\u0026#39;bounds\u0026#39;: {\u0026#39;northeast\u0026#39;: {\u0026#39;lat\u0026#39;: 35.4056304, \u0026#39;lng\u0026#39;: 139.5369436}, \u0026#39;southwest\u0026#39;: {\u0026#39;lat\u0026#39;: 35.4002057, \u0026#39;lng\u0026#39;: 139.5186068}}, \u0026#39;copyrights\u0026#39;: \u0026#39;地図データ ©2016 Google, ZENRIN\u0026#39;, \u0026#39;legs\u0026#39;: [{\u0026#39;distance\u0026#39;: {\u0026#39;text\u0026#39;: \u0026#39;2.7 km\u0026#39;, \u0026#39;value\u0026#39;: 2716}, \u0026#39;duration\u0026#39;: {\u0026#39;text\u0026#39;: \u0026#39;11分\u0026#39;, \u0026#39;value\u0026#39;: 673}, \u0026#39;end_address\u0026#39;: \u0026#39;日本, 〒245-0014 神奈川県横浜市泉区中田南１丁目１ 踊場駅\u0026#39;, \u0026#39;end_location\u0026#39;: {\u0026#39;lat\u0026#39;: 35.4056304, \u0026#39;lng\u0026#39;: 139.5186068}, \u0026#39;start_address\u0026#39;: \u0026#39;日本, 〒244-0003 神奈川県横浜市戸塚区戸塚町 戸塚駅\u0026#39;, \u0026#39;start_location\u0026#39;: {\u0026#39;lat\u0026#39;: 35.40064770000001, \u0026#39;lng\u0026#39;: 139.5346506}, \u0026#39;steps\u0026#39;: [{\u0026#39;distance\u0026#39;: {\u0026#39;text\u0026#39;: \u0026#39;91 m\u0026#39;, \u0026#39;value\u0026#39;: 91}, \u0026#39;duration\u0026#39;: {\u0026#39;text\u0026#39;: \u0026#39;1分\u0026#39;, \u0026#39;value\u0026#39;: 33}, \u0026#39;end_location\u0026#39;: {\u0026#39;lat\u0026#39;: 35.401141, \u0026#39;lng\u0026#39;: 139.5350348}, \u0026#39;html_instructions\u0026#39;: \u0026#39;\u0026lt;b\u0026gt;県道203号線\u0026lt;/b\u0026gt;に向かって\u0026lt;b\u0026gt;北\u0026lt;/b\u0026gt;に進む\u0026#39;, \u0026#39;polyline\u0026#39;: {\u0026#39;points\u0026#39;: \u0026#39;aeawEqzsrYmAVScB\u0026#39;}, \u0026#39;start_location\u0026#39;: {\u0026#39;lat\u0026#39;: 35.40064770000001, \u0026#39;lng\u0026#39;: 139.5346506}, \u0026#39;travel_mode\u0026#39;: \u0026#39;DRIVING\u0026#39;}, {\u0026#39;distance\u0026#39;: {\u0026#39;text\u0026#39;: \u0026#39;0.2 km\u0026#39;, \u0026#39;value\u0026#39;: 151}, \u0026#39;duration\u0026#39;: {\u0026#39;text\u0026#39;: \u0026#39;2分\u0026#39;, \u0026#39;value\u0026#39;: 114}, \u0026#39;end_location\u0026#39;: {\u0026#39;lat\u0026#39;: 35.402465, \u0026#39;lng\u0026#39;: 139.5353301}, \u0026#39;html_instructions\u0026#39;: \u0026#39;\u0026lt;b\u0026gt;戸塚駅東口広場出口（交差点）\u0026lt;/b\u0026gt; を\u0026lt;b\u0026gt;左折\u0026lt;/b\u0026gt;して \u0026lt;b\u0026gt;県道203号線\u0026lt;/b\u0026gt; に入る\u0026#39;, \u0026#39;maneuver\u0026#39;: \u0026#39;turn-left\u0026#39;, \u0026#39;polyline\u0026#39;: {\u0026#39;points\u0026#39;: \u0026#39;chawE}|srYW?g@Ge@OaAWQEQE{@?\u0026#39;}, \u0026#39;start_location\u0026#39;: {\u0026#39;lat\u0026#39;: 35.401141, \u0026#39;lng\u0026#39;: 139.5350348}, \u0026#39;travel_mode\u0026#39;: \u0026#39;DRIVING\u0026#39;}, {\u0026#39;distance\u0026#39;: {\u0026#39;text\u0026#39;: \u0026#39;0.4 km\u0026#39;, \u0026#39;value\u0026#39;: 387}, \u0026#39;duration\u0026#39;: {\u0026#39;text\u0026#39;: \u0026#39;2分\u0026#39;, \u0026#39;value\u0026#39;: 129}, \u0026#39;end_location\u0026#39;: {\u0026#39;lat\u0026#39;: 35.4052329, \u0026#39;lng\u0026#39;: 139.5365005}, \u0026#39;html_instructions\u0026#39;: \u0026#39;\u0026lt;b\u0026gt;戸塚駅東口入口（交差点）\u0026lt;/b\u0026gt; を\u0026lt;b\u0026gt;右折\u0026lt;/b\u0026gt;して \u0026lt;b\u0026gt;東海道\u0026lt;/b\u0026gt;/\u0026lt;b\u0026gt;国道1号線\u0026lt;/b\u0026gt; に入る\u0026#39;, \u0026#39;maneuver\u0026#39;: \u0026#39;turn-right\u0026#39;, \u0026#39;polyline\u0026#39;: {\u0026#39;points\u0026#39;: \u0026#39;kpawEy~srYU}@Yi@U_@sAgCQQe@YSCSAU?WBQF[Lc@VWHUFW@Q@_@@M?MBODKB\u0026#39;}, \u0026#39;start_location\u0026#39;: {\u0026#39;lat\u0026#39;: 35.402465, \u0026#39;lng\u0026#39;: 139.5353301}, \u0026#39;travel_mode\u0026#39;: \u0026#39;DRIVING\u0026#39;}, {\u0026#39;distance\u0026#39;: {\u0026#39;text\u0026#39;: \u0026#39;0.6 km\u0026#39;, \u0026#39;value\u0026#39;: 607}, \u0026#39;duration\u0026#39;: {\u0026#39;text\u0026#39;: \u0026#39;1分\u0026#39;, \u0026#39;value\u0026#39;: 87}, \u0026#39;end_location\u0026#39;: {\u0026#39;lat\u0026#39;: 35.401252, \u0026#39;lng\u0026#39;: 139.5320803}, \u0026#39;html_instructions\u0026#39;: \u0026#39;\u0026lt;b\u0026gt;矢部団地入口（交差点）\u0026lt;/b\u0026gt; で大きく\u0026lt;b\u0026gt;左方向\u0026lt;/b\u0026gt;に曲がる\u0026#39;, \u0026#39;maneuver\u0026#39;: \u0026#39;turn-sharp-left\u0026#39;, \u0026#39;polyline\u0026#39;: {\u0026#39;points\u0026#39;: \u0026#39;uabwEcftrYt@d@~@n@NL~AbAJJ`Ar@TTRPNVPVRXR`@vDtHJPR\\\\NXTVTVRPJHXPd@X\u0026#39;}, \u0026#39;start_location\u0026#39;: {\u0026#39;lat\u0026#39;: 35.4052329, \u0026#39;lng\u0026#39;: 139.5365005}, \u0026#39;travel_mode\u0026#39;: \u0026#39;DRIVING\u0026#39;}, {\u0026#39;distance\u0026#39;: {\u0026#39;text\u0026#39;: \u0026#39;0.1 km\u0026#39;, \u0026#39;value\u0026#39;: 130}, \u0026#39;duration\u0026#39;: {\u0026#39;text\u0026#39;: \u0026#39;1分\u0026#39;, \u0026#39;value\u0026#39;: 60}, \u0026#39;end_location\u0026#39;: {\u0026#39;lat\u0026#39;: 35.4002057, \u0026#39;lng\u0026#39;: 139.5314439}, \u0026#39;html_instructions\u0026#39;: \u0026#39;\u0026lt;b\u0026gt;清源院入口（交差点）\u0026lt;/b\u0026gt;で\u0026lt;b\u0026gt;東海道\u0026lt;/b\u0026gt;/\u0026lt;b\u0026gt;国道1号線\u0026lt;/b\u0026gt;へ進む\u0026#39;, \u0026#39;polyline\u0026#39;: {\u0026#39;points\u0026#39;: \u0026#39;yhawEojsrYt@`@jAn@lAl@\u0026#39;}, \u0026#39;start_location\u0026#39;: {\u0026#39;lat\u0026#39;: 35.401252, \u0026#39;lng\u0026#39;: 139.5320803}, \u0026#39;travel_mode\u0026#39;: \u0026#39;DRIVING\u0026#39;}, {\u0026#39;distance\u0026#39;: {\u0026#39;text\u0026#39;: \u0026#39;1.4 km\u0026#39;, \u0026#39;value\u0026#39;: 1350}, \u0026#39;duration\u0026#39;: {\u0026#39;text\u0026#39;: \u0026#39;4分\u0026#39;, \u0026#39;value\u0026#39;: 250}, \u0026#39;end_location\u0026#39;: {\u0026#39;lat\u0026#39;: 35.4056304, \u0026#39;lng\u0026#39;: 139.5186068}, \u0026#39;html_instructions\u0026#39;: \u0026#39;\u0026lt;b\u0026gt;バスセンター前（交差点）\u0026lt;/b\u0026gt; を\u0026lt;b\u0026gt;右折\u0026lt;/b\u0026gt;して \u0026lt;b\u0026gt;長後街道\u0026lt;/b\u0026gt;/\u0026lt;b\u0026gt;県道22号線\u0026lt;/b\u0026gt; に入る\u0026lt;div style=\u0026#34;font-size:0.9em\u0026#34;\u0026gt;目的地は前方右側です\u0026lt;/div\u0026gt;\u0026#39;, \u0026#39;maneuver\u0026#39;: \u0026#39;turn-right\u0026#39;, \u0026#39;polyline\u0026#39;: {\u0026#39;points\u0026#39;: \u0026#39;ibawEofsrYQ`@MT]l@c@v@KREVCTCj@QdDEhCAdA?h@AZ?`@Gv@GdASnAYhAi@hAeAbBk@h@[Zq@z@[f@EFWj@y@xBQh@e@nAERo@pBEHKXQn@]bASh@k@lB}@fCa@|@W`@g@z@g@~@k@x@ED]h@\u0026#39;}, \u0026#39;start_location\u0026#39;: {\u0026#39;lat\u0026#39;: 35.4002057, \u0026#39;lng\u0026#39;: 139.5314439}, \u0026#39;travel_mode\u0026#39;: \u0026#39;DRIVING\u0026#39;}], \u0026#39;traffic_speed_entry\u0026#39;: [], \u0026#39;via_waypoint\u0026#39;: []}], \u0026#39;overview_polyline\u0026#39;: {\u0026#39;points\u0026#39;: \u0026#39;aeawEqzsrYmAVScB_AGkCs@{@?U}@o@iAsAgCQQe@YSCi@Ai@J_Ad@m@PwAD]HKBt@d@nA|@jBnAvAhAb@h@d@p@jEvI^n@d@p@h@h@d@ZzAz@xC|AaB|CKREVG`AQdDEhCAnBA|@O|BSnAYhAi@hAeAbBgAdAmAbB]r@kAbDaB~EoAvDiBtFy@~AoAzBq@~@]h@\u0026#39;}, \u0026#39;summary\u0026#39;: \u0026#39;長後街道/県道22号線\u0026#39;, \u0026#39;warnings\u0026#39;: [], \u0026#39;waypoint_order\u0026#39;: []}]   情報の取得ができた。 次回は、ルート情報から、標高を出して表示する予定です。\n","permalink":"https://wenhan.blog/post/google-maps-api-with-python/","summary":"PythonでGoogle mapsのAPI を利用しルート検索をやってみた。\n利用するモジュールはgooglemaps/google-maps-services-python\n環境構築は以下のコマンドでOK, ついでにipythonもインストールする。\n1 2  $ pip install googlemaps $ pip install ipython   あと、GoogleのAPIキーの申請が必要なので、Google APIsで申請＆有効にする。\nここまで問題なかったら、早速使ってみよう。 戸塚駅から踊場駅まで、車のルート情報を取得\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79  $ ipython Python 3.","title":"PythonでGoogle maps のapiを利用する"},{"content":"GoogleマップをiPhone上に表示したい時にはまったことを残す。\nGoogle Maps API \u0026gt; iOS向け \u0026gt; Maps SDK for iOSを参考に、とりあえず画面上GoogleMapを表示させようの所、画面が真っ白で何も表示されない。\nそして、コンソールには以下のエラーメッセージが出ていた。\n1  ClientParametersRequest failed, 0 attempts remaining (0 vs 6). Error Domain=com.google.HTTPStatus Code=400   ネットで調べたところ、このサイトを見つけた。 そして自分の場合は、三つ目の見直しで、自分のAPIキーが「無効」になっている。。\nこのエラーの原因は、だいたいAPIキーの設定に何か間違っているようです。 以下の見直しを確認すべき。\n APIキーを生成するとき、xcodeのProjectのbundle IDを設定した方がいい。このパラメータは「任意」ってGoogleが言ったが、やはり設定した方が無難。 アプリのBundleIDを変更した場合、APIキーを再生成しましょう。 Google Developersのダッシュボードで「Google Maps SDK for iOS」が有効になっていることを確認しましょう。デフォルトでは無効になっている。(今回はここにはまった)  時間があったらAPIキー有効の画面もアップする予定\n","permalink":"https://wenhan.blog/post/googlemap-on-ios/","summary":"GoogleマップをiPhone上に表示したい時にはまったことを残す。\nGoogle Maps API \u0026gt; iOS向け \u0026gt; Maps SDK for iOSを参考に、とりあえず画面上GoogleMapを表示させようの所、画面が真っ白で何も表示されない。\nそして、コンソールには以下のエラーメッセージが出ていた。\n1  ClientParametersRequest failed, 0 attempts remaining (0 vs 6). Error Domain=com.google.HTTPStatus Code=400   ネットで調べたところ、このサイトを見つけた。 そして自分の場合は、三つ目の見直しで、自分のAPIキーが「無効」になっている。。\nこのエラーの原因は、だいたいAPIキーの設定に何か間違っているようです。 以下の見直しを確認すべき。\n APIキーを生成するとき、xcodeのProjectのbundle IDを設定した方がいい。このパラメータは「任意」ってGoogleが言ったが、やはり設定した方が無難。 アプリのBundleIDを変更した場合、APIキーを再生成しましょう。 Google Developersのダッシュボードで「Google Maps SDK for iOS」が有効になっていることを確認しましょう。デフォルトでは無効になっている。(今回はここにはまった)  時間があったらAPIキー有効の画面もアップする予定","title":"iPhoneでGoogleマップを表示する時のメモ"},{"content":"もう一題をやった。Palindrome chain length\n回文に関する計算問題です。 回文とは、文字列を真ん中で割って、左辺と右辺が反転的に対称している文字列のこと。 例えば、「あいういあ」が回文であり、「あいうえお」が回文では無い。 数字の場合も同じく、5,44,171,4884が回文であり、43,194,4773が回文ではない。\nそして、今回の問題は、引数の数字に対し、特定な計算方法で、何回計算したら回文になるかを算出する。 ここでの特定な計算方法は、「自分の数字の各桁を反転し、元の数字と足し算する」のこと。\n例えば、87が与えられ、4回の計算で回文数字になったため、戻り値を４にする。\n1 2 3 4  87 + 78 = 165; 165 + 561 = 726; 726 + 627 = 1353; 1353 + 3531 = 4884;   ここで難しいのは、数字をどうやって反転するか。 いろいろ調べた結果、意外と簡単でした。しかも一行で済む。Python で文字列反転\n1 2  str(n) # 数字を文字列に変換 str(n)[::-1] # 数字を文字列に変換し、逆順に変換する。   このような書き方は、「文字列の末尾から一つずつ遡って先頭まで要素を取り出す」の操作となる。\nそしてさっきの問題に対し自分の答えは\n1 2 3 4 5 6  def palindrome_chain_length(n): cnt=0 while str(n) != str(n)[::-1] : cnt += 1 n += int(str(n)[::-1]) return cnt   いや〜便利だな〜これで今日もよく寝れる〜\n","permalink":"https://wenhan.blog/post/python-reverse-str-to-check-palindrome/","summary":"もう一題をやった。Palindrome chain length\n回文に関する計算問題です。 回文とは、文字列を真ん中で割って、左辺と右辺が反転的に対称している文字列のこと。 例えば、「あいういあ」が回文であり、「あいうえお」が回文では無い。 数字の場合も同じく、5,44,171,4884が回文であり、43,194,4773が回文ではない。\nそして、今回の問題は、引数の数字に対し、特定な計算方法で、何回計算したら回文になるかを算出する。 ここでの特定な計算方法は、「自分の数字の各桁を反転し、元の数字と足し算する」のこと。\n例えば、87が与えられ、4回の計算で回文数字になったため、戻り値を４にする。\n1 2 3 4  87 + 78 = 165; 165 + 561 = 726; 726 + 627 = 1353; 1353 + 3531 = 4884;   ここで難しいのは、数字をどうやって反転するか。 いろいろ調べた結果、意外と簡単でした。しかも一行で済む。Python で文字列反転\n1 2  str(n) # 数字を文字列に変換 str(n)[::-1] # 数字を文字列に変換し、逆順に変換する。   このような書き方は、「文字列の末尾から一つずつ遡って先頭まで要素を取り出す」の操作となる。\nそしてさっきの問題に対し自分の答えは\n1 2 3 4 5 6  def palindrome_chain_length(n): cnt=0 while str(n) != str(n)[::-1] : cnt += 1 n += int(str(n)[::-1]) return cnt   いや〜便利だな〜これで今日もよく寝れる〜","title":"Pythonで回文チェック"},{"content":"今日もCodeWarの問題を練習した。RGB To Hex Conversion\n問題を簡略化にすると、rgbの数字を16進に変換し出力する。\n1 2 3 4  rgb(255, 255, 255) # returns FFFFFF rgb(255, 255, 300) # returns FFFFFF rgb(0,0,0) # returns 000000 rgb(148, 0, 211) # returns 9400D3   直接hexで変換すると、”0x”が余計についているし、\u0026ldquo;00\u0026quot;が\u0026quot;0\u0026quot;になる。 sprintf見たいな関数が無いかなっと思いながら、このサイトを見つけた。 Cのsprintfのような文字列フォーマット\nなんと、フォーマットを指定すれば数字が直接変換される！桁数の指定も出来る。\nそして自分の答えは\n1 2 3 4 5  def limit(a): return min(max(a, 0), 255) def rgb(r, g, b): return \u0026#34;%02X%02X%02X\u0026#34; % (limit(r), limit(g), limit(b),)   便利だな~\n","permalink":"https://wenhan.blog/post/python-print-format/","summary":"今日もCodeWarの問題を練習した。RGB To Hex Conversion\n問題を簡略化にすると、rgbの数字を16進に変換し出力する。\n1 2 3 4  rgb(255, 255, 255) # returns FFFFFF rgb(255, 255, 300) # returns FFFFFF rgb(0,0,0) # returns 000000 rgb(148, 0, 211) # returns 9400D3   直接hexで変換すると、”0x”が余計についているし、\u0026ldquo;00\u0026quot;が\u0026quot;0\u0026quot;になる。 sprintf見たいな関数が無いかなっと思いながら、このサイトを見つけた。 Cのsprintfのような文字列フォーマット\nなんと、フォーマットを指定すれば数字が直接変換される！桁数の指定も出来る。\nそして自分の答えは\n1 2 3 4 5  def limit(a): return min(max(a, 0), 255) def rgb(r, g, b): return \u0026#34;%02X%02X%02X\u0026#34; % (limit(r), limit(g), limit(b),)   便利だな~","title":"Pythonの出力フォーマット"},{"content":"ここのURLを参考しながら、HexoのBlogを設定した。 https://liginc.co.jp/web/programming/server/104594\nInstall の途中で、以下のエラーが出た。\n1 2  % hexo deploy ERROR Deployer not found: github   https://github.com/hexojs/hexo/issues/1040 を参考にして、直した。\n1  % npm install hexo-deployer-git --save   _config.ymlのtypeもgitに変更\n1 2  deploy:type:git  MarkDownの文法も勉強せねばな。。。やることが多い〜\n","permalink":"https://wenhan.blog/post/install-of-hexo/","summary":"ここのURLを参考しながら、HexoのBlogを設定した。 https://liginc.co.jp/web/programming/server/104594\nInstall の途中で、以下のエラーが出た。\n1 2  % hexo deploy ERROR Deployer not found: github   https://github.com/hexojs/hexo/issues/1040 を参考にして、直した。\n1  % npm install hexo-deployer-git --save   _config.ymlのtypeもgitに変更\n1 2  deploy:type:git  MarkDownの文法も勉強せねばな。。。やることが多い〜","title":"Hexoをインストール"}]